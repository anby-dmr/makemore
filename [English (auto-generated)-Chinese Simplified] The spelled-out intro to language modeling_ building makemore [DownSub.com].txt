hi everyone hope you're well
大家好，希望你们都好。
and next up what i'd like to do is i'd
接下来我想做的是，在
like to build out make more
make more 之前，我想先把它构建成
like micrograd before it make more is a
类似 micrograd 的版本库。make more 是
repository that i have on my github
我在 GitHub 网页上发布的一个仓库，
webpage
you can look at it
你可以去看看。
but just like with micrograd i'm going
不过，就像 micrograd 一样，我会
to build it out step by step and i'm
一步一步地构建它，我
going to spell everything out so we're
会把所有东西都解释清楚，所以我们
going to build it out slowly and
会慢慢地一起构建它。
together
now what is make more
现在，make more 是什么？
make more as the name suggests
顾名思义，make more 会根据
makes more of things that you give it
你提供的内容生成更多数据。
so here's an example
这里有一个例子，
names.txt is an example dataset to make
names.txt 是一个 make more 的示例数据集。
more
and when you look at names.txt you'll
当你查看 names.txt 时，你会
find that it's a very large data set of
发现这是一个非常大的名称数据集，里面有
names
so
here's lots of different types of names
很多不同类型的名称。
in fact i believe there are 32 000 names
事实上，我相信
that i've sort of found randomly on the
我在政府网站上随机找到了 32,000 个名称。
government website
and if you train make more on this data
如果你在这个数据集上训练 make more，
set it will learn to make more of things
它就会学会生成更多
like this
类似的名字。
and in particular in this case that will
特别是在这种情况下，这
mean more things that sound name-like
意味着会有更多听起来像名字
but are actually unique names
但实际上是独一无二的名字。
and maybe if you have a baby and you're
如果你有孩子，你
trying to assign name maybe you're
想给它取名，也许你正在
looking for a cool new sounding unique
寻找一个 听起来很酷很独特的新
name make more might help you
名字 make more 可能会对你
so here are some example generations
有所帮助，这里有一些
from the neural network
神经网络在
once we train it on our data set
数据集上训练后生成的示例，
so here's some example
这里有一些
unique names that it will generate
独特的名字示例，它会生成
dontel
dontel
irot
irot
zhendi
zhendi
and so on and so all these are sound
等等，所有这些听起来都像
name like but they're not of course
名字，但它们当然不是
names
名字，
so under the hood make more is a
所以 make more 的本质是一个
character level language model so what
字符级语言模型，这
that means is that it is treating every
意味着它将
single line here as an example and
这里的每一行都视为一个例子，
within each example it's treating them
在每个例子中，它将它们
all as sequences of individual
全部视为单个字符的序列，
characters so r e e s e is this example
所以 r e e s e 就是这个例子，这
and that's the sequence of characters
就是字符序列，这
and that's the level on which we are
就是我们
building out make more and what it means
构建 make more 的层级，
to be a character level language model
字符级语言模型的意义
then is that it's just uh sort of
在于它只是对
modeling those sequences of characters
这些字符序列进行建模，
and it knows how to predict the next
它知道如何预测
character in the sequence
序列中的下一个字符，
now we're actually going to implement a
现在我们实际上要实现
large number of character level language
大量的字符级语言
models in terms of the neural networks
模型，这些模型
that are involved in predicting the next
涉及预测
character in a sequence so very simple
序列中下一个字符的神经网络，非常简单的
bi-gram and back of work models
二元语法和反向工作模型，
multilingual perceptrons recurrent
多语言感知器循环
neural networks all the way to modern
神经网络一直到现代
transformers in fact the transformer
Transformer，实际上，
that we will build will be basically the
我们将要构建的 Transformer 基本上
equivalent transformer to gpt2 if you
相当于 GPT2，如果你
have heard of gpt uh so that's kind of a
听说过 GPT，嗯，这很
big deal it's a modern network and by
重要，它是一个现代网络，在
the end of the series you will actually
本系列结束时，你将真正
understand how that works um on the
理解它在
level of characters now to give you a
字符级别上的工作原理，现在，为了让你了解
sense of the extensions here uh after
这里的扩展，嗯，在
characters we will probably spend some
字符之后，我们可能会花一些
time on the word level so that we can
时间在单词级别上，这样我们就可以
generate documents of words not just
生成单词文档，而不仅仅是小的
little you know segments of characters
字符片段，
but we can generate entire large much
而是可以生成整个
larger documents
更大的文档，
and then we're probably going to go into
然后我们可能会进入
images and image text
图像和图像文本
networks such as dolly stable diffusion
网络，例如 Dolly 稳定扩散
and so on but for now we have to start
等等，但现在我们必须从
here character level language modeling
这里开始，字符级语言建模，我们
let's go
开始吧，就像
so like before we are starting with a
之前从一个
completely blank jupiter notebook page
完全空白的 Jupiter Notebook 页面开始之前一样，
the first thing is i would like to
首先，我想
basically load up the dataset names.txt
加载数据集 names.txt，
so we're going to open up names.txt for
所以我们要打开 names.txt 进行
reading
读取，
and we're going to read in everything
我们将把所有内容读
into a massive string
入一个巨大的字符串，
and then because it's a massive string
然后因为它是一个很大的字符串，
we'd only like the individual words and
我们只需要单个单词并将
put them in the list
它们放在列表中，
so let's call split lines
所以我们称之为分割线
on that string
字符串，将
to get all of our words as a python list
所有单词转换为 Python 字符串列表，
of strings
so basically we can look at for example
因此，基本上，我们可以查看
the first 10 words
前 10 个单词，
and we have that it's a list of emma
这是 emma、olivia、eva 等的列表。
olivia eva and so on
and if we look at
如果我们查看
the top of the page here that is indeed
页面顶部，就会发现确实是这个列表，
what we see
um
嗯，
so that's good
很好，
this list actually makes me feel that
这个列表实际上让我觉得
this is probably sorted by frequency
它可能是按频率排序的，
but okay so
但是好吧，
these are the words now we'd like to
这些就是单词，现在我们想
actually like learn a little bit more
进一步了解一下
about this data set let's look at the
这个数据集，让我们看看
total number of words we expect this to
单词总数，我们预计
be roughly 32 000
大约为 32,000 个，
and then what is the for example
然后，例如
shortest word
最短的单词是什么？
so min of
length of each word for w inwards
每个单词的长度最小为 w，
so the shortest word will be length
最短的单词长度为
two
2，
and max of one w for w in words so the
longest word will be
最长的单词长度为
15 characters
15 个字符。
so let's now think through our very
现在，让我们思考一下我们的
first language model
第一个语言模型，
as i mentioned a character level
正如我提到的，字符级
language model is predicting the next
语言模型是根据之前
character in a sequence given already
已经给出的
some concrete sequence of characters
一些具体字符序列来预测序列中的下一个字符。
before it
now we have to realize here is that
现在，我们必须意识到，
every single word here like isabella is
这里的每个单词，例如 isabella，
actually quite a few examples packed in
实际上都包含相当多的例子。
to that single word
到那个单词，
because what is an existence of a word
因为
like isabella in the data set telling us
数据集中存在像 isabella 这样的单词，
really it's saying that
实际上意味着
the character i is a very likely
字符 i 很可能
character to come first in the sequence
出现在姓名序列的第一个字符，字符 s 很可能出现在
of a name
the character s is likely to come
after i
i 之后，
the character a is likely to come after
字符 a 很可能出现在 is 之后，
is
the character b is very likely to come
字符 b 很可能出现在
after isa and so on all the way to a
isa 之后，依此类推，直到 a
following isabel
出现在 isabel 之后。
and then there's one more example
actually packed in here
这里实际上还有一个例子，
and that is that
那就是在
after there's isabella
isabella 之后，
the word is very likely to end
单词很可能结束。
so that's one more sort of explicit
所以，我们必须小心处理另一种明确的
piece of information that we have here
信息。
that we have to be careful with
and so there's a lot backed into a
因此，
single individual word in terms of the
从统计结构来看，单个单词包含很多信息，
statistical structure of what's likely
to follow in these character sequences
这些统计结构决定了这些字符序列中可能出现的单词。
and then of course we don't have just an
当然，我们不只有一个
individual word we actually have 32 000
单词，我们实际上有 32,000 个
of these and so there's a lot of
这样的单词，所以这里有很多
structure here to model
结构需要建模。
now in the beginning what i'd like to
首先，我想先
start with is i'd like to start with
building a bi-gram language model
建立一个二元语言模型。
now in the bigram language model we're
在二元语言模型中，我们
always working with just
总是
two characters at a time
每次只处理两个字符，
so we're only looking at one character
所以我们只查看给定的一个字符，
that we are given and we're trying to
并尝试
predict the next character in the
预测序列中的下一个字符，
sequence
so um what characters are likely to
所以，哪些字符可能跟在
follow are what characters are likely to
后面，哪些字符可能跟在后面
follow a and so on and we're just
等等，我们只是在对
modeling that kind of a little local
这种局部结构进行建模，
structure
and we're forgetting the fact that we
而我们忘记了我们
may have a lot more information we're
可能拥有更多信息，我们
always just looking at the previous
总是只查看前一个
character to predict the next one so
字符来预测下一个字符，所以
it's a very simple and weak language
这是一个非常简单且薄弱的语言
model but i think it's a great place to
模型，但我认为这是一个很好的
start
起点，
so now let's begin by looking at these
现在让我们先看看
bi-grams in our data set and what they
数据集中的这些二元语法，它们是什么样子
look like and these bi-grams again are
的，这些二元语法又
just two characters in a row
只是连续的两个字符，
so for w in words
所以对于单词中的 w，
each w here is an individual word a
每个 w 是一个单独的单词，一个
string
we want to iterate uh for
我们要迭代的字符串，
we're going to iterate this word
我们将
with consecutive characters so two
用连续的字符迭代这个单词，所以
characters at a time sliding it through
每次两个字符，将它在单词中滑动，顺便说一下，
the word now a interesting nice way cute
way to do this in python by the way is
在 Python 中执行此操作的一种有趣、好用、可爱的方法是
doing something like this for character
这样做，对于字符
one character two in zip off
一，字符二，在 zip off
w and w at one
w 和 w at one one column print character one character two and
one column
print
character one character two
and let's not do all the words let's
我们先不处理所有的单词，
just do the first three words and i'm
只处理前三个单词，我
going to show you in a second how this
稍后会向你展示它是如何
works
工作的，
but for now basically as an example
但现在基本上作为一个例子，
let's just do the very first word alone
我们只处理第一个单词
emma
emma，
you see how we have a emma and this will
你看我们有一个 emma，它
just print e m m m a
只会打印 e m m m a，
and the reason this works is because w
它之所以有效是因为 w
is the string emma w at one column is
是字符串 emma，其中一列是
the string mma
字符串 mma，
and zip
zip
takes two iterators and it pairs them up
需要两个迭代器，并将它们配对，
and then creates an iterator over the
然后在它们的连续条目的元组上创建一个迭代器，
tuples of their consecutive entries
and if any one of these lists is shorter
如果任何一个列表
than the other then it will just
比另一个短，那么它就会
halt and return
停止并返回，
so basically that's why we return em mmm
所以基本上这就是我们返回 em mmm ma 的原因，
ma
but then because this iterator second
但是因为这个迭代器
one here runs out of elements zip just
这里的第二个用完了元素，zip 就
ends and that's why we only get these
结束了，这就是为什么我们只得到这些
tuples so pretty cute
元组，所以非常可爱，
so these are the consecutive elements in
所以这些是
the first word now we have to be careful
第一个单词中的连续元素，现在我们必须小心，
because we actually have more
因为我们实际上
information here than just these three
这里有比这三个
examples as i mentioned we know that e
例子更多的信息，正如我提到的，我们知道 e
is the is very likely to come first and
很可能排在第一位，
we know that a in this case is coming
我们知道在这种情况下 a 排在
last
最后，
so one way to do this is basically we're
所以一种方法是 基本上，我们
going to create
要在
a special array here all
这里创建一个特殊的数组，里面包含所有
characters
字符，然后，
and um we're going to hallucinate a
嗯，我们要在这里想象一个
special start token here
特殊的起始标记，
i'm going to
我把
call it like special start
它叫做“特殊起始”，
so this is a list of one element
所以这是一个由一个元素
plus
加上
w
w
and then plus a special end character
和一个特殊结束字符组成的列表，
and the reason i'm wrapping the list of
我之所以在
w here is because w is a string emma
这里包装 w 列表，是因为 w 是一个字符串，
list of w will just have the individual
W 的列表只会包含
characters in the list
列表中的单个字符，
and then
然后
doing this again now but not iterating
再次执行此操作，但不是对
over w's but over the characters
w 进行迭代，而是对字符进行迭代，
will give us something like this
我们会得到类似这样的结果，
so e is likely so this is a bigram of
所以 e 很可能是，所以这是
the start character and e and this is a
起始字符和 e 的二元组，这是
bigram of the
a and the special end character
a 和特殊结束字符的二元组，
and now we can look at for example what
现在我们可以看看
this looks like for
olivia or eva
olivia 或 eva 的例子，
and indeed we can actually
实际上，我们可以
potentially do this for the entire data
对整个数据集执行此操作，
set but we won't print that that's going
但我们不会打印出来，因为内容
to be too much
太多了，
but these are the individual character
但这些是单个字符的
diagrams and we can print them
图表，我们现在可以打印它们，
now in order to learn the statistics
以便了解
about which characters are likely to
哪些字符可能跟在
follow other characters the simplest way
其他字符后面的统计数据，
in the bigram language models is to
二元语言模型中最简单的方法就是
simply do it by counting
通过计数来完成，
so we're basically just going to count
所以我们基本上 我们要计算
how often any one of these combinations
这些组合
occurs in the training set
在训练集中出现的频率，
in these words
so we're going to need some kind of a
所以我们需要某种
dictionary that's going to maintain some
字典来维护每个
counts for every one of these diagrams
图表的计数，
so let's use a dictionary b
所以我们使用字典 b，它将
and this will map these bi-grams so
映射这些二元语法，
bi-gram is a tuple of character one
二元语法是一个字符一
character two
字符二的元组，
and then b at bi-gram
然后二元语法中的 b 将是二元语法的 b 点，这
will be b dot get of bi-gram
which is basically the same as b at
基本上与二元语法中的 b 相同，
bigram
but in the case that bigram is not in
但是如果二元语法不在
the dictionary b we would like to by
字典 b 中，我们希望
default return to zero
默认返回零
plus one
加一，
so this will basically add up all the
所以这基本上会把所有的二元
bigrams and count how often they occur
语法加起来，并计算它们出现的频率，
let's get rid of printing
让我们去掉打印，
or rather
或者更确切地说，让
let's keep the printing and let's just
我们保留打印，让我们
inspect what b is in this case
检查一下 b 是什么，在这种情况下，
and we see that many bi-grams occur just
我们看到许多二元语法只出现
a single time this one allegedly
一次，据说这个
occurred three times
出现了三次，
so a was an ending character three times
所以 a 是结束字符三次，
and that's true for all of these words
对于所有这些词
all of emma olivia and eva and with a
都是如此，包括 emma olivia 和 eva，以及 a，
so that's why this occurred three times
所以这就是为什么它出现了三次，
now let's do it for all the words
现在让我们对所有单词都这样做
oops i should not have printed
哎呀，我不应该打印，
i'm going to erase that
我要删除它，
let's kill this
让我们终止它，
let's just run
让我们运行，
and now b will have the statistics of
现在 b 将获得
the entire data set
整个数据集的统计数据，
so these are the counts across all the
这些是各个
words of the individual pie grams
饼图的所有单词的计数，
and we could for example look at some of
例如，我们可以查看一些
the most common ones and least common
最常见和最不常见的
ones
单词，
this kind of grows in python but the way
这在 Python 中有所增长，但最
to do this the simplest way i like is we
简单的方法是
just use b dot items
使用 b 点项，
b dot items returns
b 点项返回
the tuples of
key value in this case the keys are
键值的元组，在这种情况下，键是
the character diagrams and the values
字符图，值
are the counts
是计数，
and so then what we want to do is we
因此我们想要做的是
want to do
sorted of this
对其进行排序，
but by default sort is on the first
但默认情况下，排序是按
on the first item of a tuple but we want
元组的第一个项进行，但我们希望
to sort by the values which are the
按值排序，即
second element of a tuple that is the
元组的第二个元素，即
key value
键值，
so we want to use the key
因此我们希望使用 key
equals lambda
equals lambda，
that takes the key value
它接受键值
and returns
并返回
the key value at the one not at zero but
键值 1，而不是 0，而是
at one which is the count so we want to
1，即计数，因此我们希望
sort by the count
按
of these elements
这些元素的计数进行排序，
and actually we wanted to go backwards
实际上我们想要倒序排序，
so here we have is
因此我们有
the bi-gram q and r occurs only a single
二元语法 q 和 r 只出现一次，
time
dz occurred only a single time
dz 也只出现一次。
and when we sort this the other way
当我们反过来排序时，
around
we're going to see the most likely
我们会看到最有可能的
bigrams so we see that n was
二元语法，所以我们看到 n
very often an ending character
经常是结尾字符，而且
many many times and apparently n almost
显然 n 几乎
always follows an a
总是跟在 a 后面，
and that's a very likely combination as
这也是一个非常可能的组合，
well
so
所以
this is kind of the individual counts
这是
that we achieve over the entire data set
我们在整个数据集上获得的单独计数。
now it's actually going to be
现在，将
significantly more convenient for us to
keep this information in a
这些信息保存在
two-dimensional array instead of a
二维数组中（而不是
python dictionary
Python 字典中）实际上会更加方便。
so
所以
we're going to store this information
我们将这些信息存储
in a 2d array
在一个二维数组中，
and
the rows are going to be the first
行将是二元语法的第一个字符，列将是
character of the bigram and the columns
are going to be the second character and
第二个字符，
each entry in this two-dimensional array
这个二维数组中的每个条目
will tell us how often that first
都会告诉我们第一个
character files the second character in
字符在数据集中出现第二个字符的频率。
the data set
so in particular the array
因此，具体来说，
representation that we're going to use
我们将要使用的数组表示形式
or the library is that of pytorch
或库是 PyTorch，
and pytorch is a deep
PyTorch 是一个深度
learning neural network framework but
学习神经网络框架，但
part of it is also this torch.tensor
它的一部分也是这个 torch.tensor，
which allows us to create
它允许我们创建
multi-dimensional arrays and manipulate
多维数组并进行操作。
them very efficiently
它们非常高效，
so
所以
let's import pytorch which you can do by
让我们导入 pytorch，你可以通过
import torch
import torch 来实现，
and then we can create
然后我们可以创建
arrays
数组，
so let's create a array of zeros
所以让我们创建一个零数组，
and we give it a
并给它一个
size of this array let's create a three
大小，让我们创建一个
by five array as an example
3x5 的数组作为例子，
and
this is a three by five array of zeros
这是一个 3x5 的零数组，
and by default you'll notice a.d type
默认情况下，你会注意到 a.d 类型，
which is short for data type is float32
它是数据类型的缩写，是 float32，
so these are single precision floating
所以这些是单精度浮点数，
point numbers
because we are going to represent counts
因为我们要表示计数，
let's actually use d type as torch dot
我们实际上使用 d 类型作为 torch dot
and 32
和 32，
so these are
所以这些是
32-bit integers
32 位整数，
so now you see that we have integer data
所以现在你看到我们在
inside this tensor
这个张量里面有整数数据，
now tensors allow us to really
现在张量允许我们真正
manipulate all the individual entries
操作所有单独的条目，
and do it very efficiently
并且非常高效地执行，
so for example if we want to change this
例如，如果我们想要更改这
bit
一位，
we have to index into the tensor and in
我们必须对张量进行索引，
particular here this is the first row
特别是这里，这是第一行，
and the
because it's zero indexed so this is row
因为它是零索引，所以这是行索引
index one and column index zero one two
1，列索引 0 1 2
three
3，
so a at one comma three we can set that
所以在一个逗号 3 处，我们可以将其设置
to one
为 1，
and then a we'll have a 1 over there
然后我们在那里得到一个 1，
we can of course also do things like
我们可以 当然也可以做这样的事情，
this so now a will be 2 over there
所以现在 a 在那里将是 2
or 3.
或 3。
and also we can for example say a 0 0 is
我们也可以例如说 a 0 0 是
5
5，
and then a will have a 5 over here
然后 a 在那里将是 5，
so that's how we can index into the
所以这就是我们现在可以对数组进行索引的方式，
arrays now of course the array that we
当然，我们
are interested in is much much bigger so
感兴趣的数组要大得多，所以
for our purposes we have 26 letters of
对于我们的目的来说，我们有 26 个字母，
the alphabet
and then we have two special characters
然后我们有两个特殊字符
s and e
s 和 e，
so uh we want 26 plus 2 or 28 by 28
所以我们想要 26 加 2 或 28 x 28 的
array
数组，
and let's call it the capital n because
我们称之为大写 n，因为
it's going to represent sort of the
它将代表
counts
计数，
let me erase this stuff
让我擦除这些东西，
so that's the array that starts at zeros
所以这是从零开始的
28 by 28
28 x 28 的数组，
and now let's copy paste this
现在让我们将其复制粘贴到
here
这里，
but instead of having a dictionary b
但是没有要擦除的字典 b，
which we're going to erase we now have
我们现在有
an n
一个 n，
now the problem here is that we have
现在的问题是我们有
these characters which are strings but
这些字符，它们是字符串，但
we have to now
我们现在必须
um basically index into a
基本上索引到一个
um array and we have to index using
数组中，我们必须使用
integers so we need some kind of a
整数进行索引，所以我们需要某种
lookup table from characters to integers
从字符到整数的查找表，
so let's construct such a character
所以让我们构造 这样的字符
array
数组，
and the way we're going to do this is
我们要做的就是
we're going to take all the words which
取出所有的单词，也
is a list of strings
就是一个字符串列表，然后把它们
we're going to concatenate all of it
连接
into a massive string so this is just
成一个巨大的字符串，所以这只是将
simply the entire data set as a single
整个数据集作为一个
string
字符串，
we're going to pass this to the set
我们将把它传递给集合构造函数，该构造函数
constructor which takes this massive
接受这个巨大的
string
字符串
and throws out duplicates because sets
并丢弃重复项，因为集合
do not allow duplicates
不允许重复，
so set of this will just be the set of
所以这个集合就是
all the lowercase characters
所有小写字符的集合，
and there should be a total of 26 of
总共应该有 26 个，
them
and now we actually don't want a set we
现在我们实际上不想要一个集合，我们
want a list
想要一个列表，
but we don't want a list sorted in some
但我们不希望列表以某种
weird arbitrary way we want it to be
奇怪的任意方式排序，我们希望它
sorted
from a to z
从 a 到 z 排序，
so sorted list
所以排序列表，
so those are our characters
所以这些就是我们的字符，
now what we want is this lookup table as
现在我们想要的是一个查找表，正如
i mentioned so let's create a special
我提到的，让我们创建一个特殊的
s2i i will call it
s2i，我称之为
um s is string or character and this
s，它是字符串或字符，这
will be an s2i mapping
将是
for
is in enumerate of these characters
这些字符的枚举中的 s2i 映射，
so enumerate basically gives us this
所以枚举基本上为我们提供了
iterator over the integer index and the
整数索引和
actual element of the list and then we
列表实际元素的迭代器，然后我们将
are mapping the character to the integer
字符 到整数，
so s2i
所以 s2i
is a mapping from a to 0 b to 1 etc all
是从 a 到 0、b 到 1 等等
the way from z to 25
一直到 z 到 25 的映射，
and that's going to be useful here but
这在这里很有用，但
we actually also have to specifically
实际上我们还必须特别
set that s will be 26
设置 s 为 26，
and s to i at e will be 27 right because
而 s 到 i 的 e 将是 27，因为
z was 25.
z 是 25。
so those are the lookups and now we can
所以这些是查找，现在我们可以
come here and we can map
到这里，我们可以将
both character 1 and character 2 to
字符 1 和字符 2 映射到
their integers
它们的整数，
so this will be s2i at character 1
所以这将是字符 1 处的 s2i，
and ix2 will be s2i of character 2.
而 ix2 将是字符 2 的 s2i。
and now we should be able to
现在我们应该能够
do this line but using our array so n at
执行此行，但使用我们的数组，所以 n 在
x1 ix2 this is the two-dimensional array
x1 ix2 这是
indexing i've shown you before
我之前向您展示过的二维数组索引，
and honestly just plus equals one
老实说，加号等于一，
because everything starts at
因为一切都从零开始，
zero
so this should
所以这应该可以
work
工作
and give us a large 28 by 28 array
并为我们提供一个包含所有这些计数的 28 x 28 的大数组，
of all these counts so
所以
if we print n
如果我们打印 n，
this is the array but of course it looks
这就是数组，但它当然看起来
ugly so let's erase this ugly mess and
很丑，所以让我们消除这个丑陋的混乱，
let's try to visualize it a bit more
让我们尝试将其更美观地可视化，
nicer
so for that we're going to use a library
为此 我们将使用一个
called matplotlib
名为 matplotlib 的库，
so matplotlib allows us to create
matplotlib 允许我们创建
figures so we can do things like plt
图形，这样我们就可以执行诸如
item show of the counter array
计数器数组的 plt item show 之类的操作，
so this is the 28x28 array
这是 28x28 数组，
and this is structure but even this i
这是结构，但即使这样，我
would say is still pretty ugly
想说它仍然很丑，
so we're going to try to create a much
所以我们将尝试创建更
nicer visualization of it and i wrote a
漂亮的可视化效果，我
bunch of code for that
为此编写了一堆代码，
the first thing we're going to need is
我们需要做的第一件事是
we're going to need to invert
反转
this array here this dictionary so s2i
这个数组，这个字典，s2i
is mapping from s to i
从 s 映射到 i，
and in i2s we're going to reverse this
在 i2s 中，我们将反转这个
dictionary so iterator of all the items
字典，迭代所有项，
and just reverse that array
然后反转该数组，
so i2s
i2s
maps inversely from 0 to a 1 to b etc
反向映射从 0 到 a，1 到 b 等等，
so we'll need that
所以我们需要它，
and then here's the code that i came up
然后这是我编写的代码，
with to try to make this a little bit
试图让它变得更
nicer
漂亮一些，
we create a figure
我们创建一个图形，
we plot
绘制
n
n，
and then we do and then we visualize a
然后我们这样做，然后我们将一堆东西可视化，
bunch of things later let me just run it
稍后让我运行它，
so you get a sense of what this is
这样您就可以了解它是什么，
okay
好的，
so you see here that we have
所以在这里您可以看到我们有一个
the array spaced out
间隔开的数组，
and every one of these is basically like
每个这些 基本上就像
b follows g zero times
b 跟随 g 零次，
b follows h 41 times
b 跟随 h 41 次，
um so a follows j 175 times
所以 a 跟随 j 175 次，
and so what you can see that i'm doing
所以你可以看到我在
here is first i show that entire array
这里做的是，首先我显示整个数组，
and then i iterate over all the
然后我遍历
individual little cells here
这里的所有单个小单元，
and i create a character string here
我在这里创建一个字符串，
which is the inverse mapping i2s of the
它是
integer i and the integer j so those are
整数 i 和整数 j 的逆映射 i2s，所以这些是
the bi-grams in a character
字符表示中的二元语法，
representation
and then i plot just the diagram text
然后我绘制图表文本，
and then i plot the number of times that
然后我绘制
this bigram occurs
这个二元语法出现的次数，
now the reason that there's a dot item
现在这里有一个点项的原因
here is because when you index into
是因为当你索引
these arrays these are torch tensors
这些数组时，这些是 torch 张量，
you see that we still get a tensor back
你会看到我们仍然会得到一个张量，
so the type of this thing you'd think it
所以这个东西的类型你会认为它
would be just an integer 149 but it's
只是一个整数 149，但它
actually a torch.tensor
实际上是一个 torch.tensor，
and so
所以
if you do dot item then it will pop out
如果你执行点项，它会弹出一个
that in individual integer
单独的整数，
so it will just be 149.
所以它就是 149。这
so that's what's happening there and
就是那里发生的事情，
these are just some options to make it
这些只是一些让它
look nice
看起来漂亮的选项，
so what is the structure of this array
那么这个数组的结构是什么，
we have all these counts and we see that
我们有所有这些 计数，我们发现
some of them occur often and some of
有些经常出现，有些
them do not occur often
则不经常出现。
now if you scrutinize this carefully you
现在，如果你仔细观察，你
will notice that we're not actually
会发现我们其实并不太
being very clever
聪明，
that's because when you come over here
因为当你到这里时，
you'll notice that for example we have
你会注意到，例如，我们有
an entire row of completely zeros and
一整行全是零，
that's because the end character
这是因为结束字符
is never possibly going to be the first
永远不可能成为
character of a bi-gram because we're
二元词组的第一个字符，因为我们
always placing these end tokens all at
总是把这些结束标记都放在
the end of the diagram
图的末尾。
similarly we have entire columns zeros
同样，我们这里有整列全是零，
here because the s
因为 s
character will never possibly be the
字符永远不可能成为
second element of a bigram because we
二元词组的第二个元素，因为我们
always start with s and we end with e
总是以 s 开头，以 e 结尾，
and we only have the words in between
我们只有中间的单词，
so we have an entire column of zeros an
所以我们有一整列全是零，一
entire row of zeros and in this little
整行全是零。在这个
two by two matrix here as well the only
2x2 的小矩阵中，唯一
one that can possibly happen is if s
可能发生的情况是，如果 s
directly follows e
直接跟在 e 后面，那么
that can be non-zero if we have a word
它可能不为零。如果我们有一个
that has no letters so in that case
没有字母的单词，那么在这种情况下，
there's no letters in the word it's an
单词中没有字母，它就是一个
empty word and we just have s follows e
空单词，我们只有 s 跟在 e 后面，
but the other ones are just not possible
但其他情况都不可能，
and so we're basically wasting space and
所以我们基本上
not only that but the s and the e are
不仅如此，这里的 s 和 e 也
getting very crowded here
变得非常拥挤，
i was using these brackets because
我之所以使用这些括号，是因为在
there's convention and natural language
自然语言
processing to use these kinds of
处理中，使用这种
brackets to denote special tokens
括号来表示特殊标记是惯例，
but we're going to use something else
但我们将使用其他东西，
so let's fix all this and make it
所以让我们修复所有这些问题，让它更
prettier
漂亮，
we're not actually going to have two
我们实际上不会有两个
special tokens we're only going to have
特殊标记，我们只会有
one special token
一个特殊标记，
so
所以
we're going to have n by n
我们将有一个 n x n 的
array of 27 by 27 instead
27 x 27 数组，而
instead of having two
不是两个，
we will just have one and i will call it
我们只有一个，我将其称为
a dot
点，
okay
好的，
let me swing this over here
让我把它转到这里，
now one more thing that i would like to
现在我想做的另一件事是，
do is i would actually like to make this
我实际上想将这个
special character half position zero
特殊字符的一半位置设为零，
and i would like to offset all the other
并且我想将所有其他
letters off i find that a little bit
字母偏移，我发现 稍微
more
pleasing
好看一点，
so
所以
we need a plus one here so that the
我们需要在这里加一，这样
first character which is a will start at
第一个字符 a 将从 1 开始，
one
so s2i
所以 s2i
will now be a starts at one and dot is 0
现在将从 1 开始，点为 0，
and
i2s of course we're not changing this
当然我们不会改变它，
because i2s just creates a reverse
因为 i2s 只是创建了一个反向
mapping and this will work fine so 1 is
映射，这样就可以正常工作了，所以 1 是 a，
a 2 is b
2 是 b，
0 is dot
0 是点，
so we've reversed that here
所以我们将其反转，这里
we have
我们有
a dot and a dot
一个点和一个点，
this should work fine
这应该可以正常工作，
make sure i start at zeros
确保 i 从零开始
count
计数，
and then here we don't go up to 28 we go
然后这里我们不要增加到 28，而是
up to 27
增加到 27，
and this should just work
这应该可以正常工作，
okay
so we see that dot never happened it's
所以我们看到点从未发生过，它
at zero because we don't have empty
位于零，因为我们没有空
words
词，
then this row here now is just uh very
那么现在这一行只是呃，非常
simply the um
简单，嗯，
counts for all the first letters so
所有首字母的计数，所以
uh j starts a word h starts a word i
呃，j 开始一个单词，h 开始一个单词，i
starts a word etc and then these are all
开始一个单词等等，然后这些是所有的
the ending
结束
characters
字符，
and in between we have the structure of
在它们之间，我们有一个
what characters follow each other
字符彼此跟随的结构，
so this is the counts array of our
所以这是我们
entire
整个
data set so this array actually has all
数据集的计数数组，所以这个数组实际上包含了
the information necessary for us to
我们
actually sample from this bigram
实际采样所需的所有信息 从这个二元语法
uh character level language model
字符级语言模型开始，
and um roughly speaking what we're going
嗯，粗略地说，我们
to do is we're just going to start
要做的是开始
following these probabilities and these
跟踪这些概率和
counts and we're going to start sampling
计数，然后从模型中开始采样，
from the from the model
so in the beginning of course
所以一开始
we start with the dot the start token
我们当然是从点开始，也就是起始标记
dot
点，
so to sample the first character of a
所以为了对名称的第一个字符进行采样，
name we're looking at this row here
我们查看这里的这一行，
so we see that we have the counts and
我们看到我们有计数，
those concepts terminally are telling us
这些概念最终告诉我们
how often any one of these characters is
这些字符中任何一个字符
to start a word
开始一个单词的频率，
so if we take this n
所以如果我们取这个 n
and we grab the first row
并抓取第一行，
we can do that by using just indexing as
我们可以通过使用索引
zero
为零，
and then using this notation column for
然后使用这个符号列来表示
the rest of that row
该行的其余部分，
so n zero colon
所以 n 零冒号
is indexing into the zeroth
索引到第零
row and then it's grabbing all the
行，然后它抓取所有
columns
列，
and so this will give us a
所以这将为我们提供
one-dimensional array
of the first row so zero four four ten
第一行的一维数组，所以零四四十，
you know zero four four ten one three oh
你知道零四四十一三
six one five four two etc it's just the
零六一五四二等等，这只是
first row the shape of this
第一行，它的形状
is 27 it's just the row of 27
是 27，它只是 27 行，
and the other way that you can do this
你可以用另一种方式来做到这一点
also is you just you don't need to
另外，你
actually give this
实际上不需要给出这个，
you just grab the zeroth row like this
你只需像这样抓取第零行，
this is equivalent
这相当于
now these are the counts
现在这些是计数，
and now what we'd like to do is we'd
现在我们想要做的是
like to basically um sample from this
从中抽样，
since these are the raw counts we
因为这些是原始计数，我们
actually have to convert this to
实际上必须将其转换为
probabilities
概率，
so we create a probability vector
所以我们创建一个概率向量，
so we'll take n of zero
所以我们将取 n 个零，
and we'll actually convert this to float
我们实际上会先将其转换为浮点数，
first
okay so these integers are converted to
好的，所以这些整数被转换为
float
浮点数，
floating point numbers and the reason
we're creating floats is because we're
我们创建浮点数的原因是因为我们
about to normalize these counts
要对这些计数进行归一化，
so to create a probability distribution
所以在这里创建一个概率分布，
here we want to divide
我们要除以
we basically want to do p p p divide p
我们基本上要做的就是用 p p p 除以 p 的
that sum
and now we get a vector of smaller
和，现在我们得到一个较小数字的向量，
numbers and these are now probabilities
这些现在是概率，
so of course because we divided by the
当然，因为我们除以和，
sum the sum of p now is 1.
p 的和现在是 1。
so this is a nice proper probability
所以这是一个很好的概率
distribution it sums to 1 and this is
分布，它的总和为 1，这
giving us the probability for any single
给了我们任何单个
character to be the first
字符成为
character of a word
单词第一个字符的概率，
so now we can try to sample from this
所以现在我们可以尝试从这个分布中抽样，从我们
distribution to sample from these
distributions we're going to use
将要使用的这些分布中抽样
storch.multinomial which i've pulled up
我在这里找到了 storch.multinomial，
here
so torch.multinomial returns uh
torch.multinomial 从
samples from the multinomial probability
多项式概率
distribution which is a complicated way
分布中返回样本，这是一种复杂的
of saying you give me probabilities and
说法，你给我概率，
i will give you integers which are
我会给你
sampled
according to the property distribution
根据属性分布采样的整数，
so this is the signature of the method
所以这是方法的签名，
and to make everything deterministic
为了使一切具有确定性，
we're going to use a generator object in
我们将在 pytorch 中使用生成器对象，
pytorch
so this makes everything deterministic
这使得一切具有确定性，
so when you run this on your computer
所以当你在计算机上运行它时，
you're going to the exact get the exact
你会得到与
same results that i'm getting here on my
我在我的计算机上得到的完全相同的结果，
computer
so let me show you how this works
所以让我向你展示它是如何工作的，这
here's the deterministic way of creating
是创建
a torch generator object
torch 生成器对象的确定性方法，用
seeding it with some number that we can
我们可以达成一致的数字作为种子，
agree on
so that seeds a generator gets gives us
以便生成器获得的种子会给我们
an object g
一个对象 g，
and then we can pass that g
然后我们可以将 g 传递
to a function
给一个
that creates um
创建
here random numbers twerk.rand creates
随机数的函数，twerk.rand 创建了
random numbers three of them
三个随机数，
and it's using this generator object to
它使用这个生成器对象
as a source of randomness
作为随机源，
so
所以
without normalizing it
不需要对其进行规范化，
i can just print
我可以直接打印它，
this is sort of like numbers between 0
这有点像 0
and 1 that are random according to this
到 1 之间的数字，根据这个，它们是随机的
thing and whenever i run it again
每当我再次运行它时，
i'm always going to get the same result
我总是会得到相同的结果，
because i keep using the same generator
因为我一直使用相同的生成器
object which i'm seeing here
对象，也就是我在这里看到的，
and then if i divide
然后如果我进行除法归一化，
to normalize i'm going to get a nice
我将得到一个
probability distribution of just three
只有三个元素的良好概率分布，
elements
and then we can use torsion multinomial
然后我们可以使用扭转多项式
to draw samples from it so this is what
从中抽取样本，所以这
that looks like
看起来像是
tertiary multinomial we'll take the
三次多项式，我们将采用
torch tensor
of probability distributions
概率分布的火炬张量，
then we can ask for a number of samples
然后我们可以要求一定数量的样本，
let's say 20.
比如说 20。replacement
replacement equals true means that when
等于 true 意味着当
we draw an element
我们绘制一个元素时，
we will uh we can draw it and then we
我们可以绘制它，然后我们
can put it back into the list of
可以将它放回到可
eligible indices to draw again
再次绘制的合格索引列表中，
and we have to specify replacement as
我们必须将 replacement 指定为
true because by default uh for some
true，因为默认情况下，由于某种
reason it's false
原因它是 false，
and i think
我想
you know it's just something to be
你知道这只是需要小心的事情，
careful with
and the generator is passed in here so
并且生成器在这里传递，所以
we're going to always get deterministic
我们总是会得到确定性的
results the same results so if i run
结果，相同的结果，所以如果我运行
these two
这两个，
we're going to get a bunch of samples
我们将从这个分布中得到一堆样本，
from this distribution
now you'll notice here that the
现在你会注意到
probability for the
first element in this tensor is 60
这里第一个元素的概率 张量是 60，
so in these 20 samples we'd expect 60 of
所以在这 20 个样本中，我们预计其中 60 个
them to be zero
为零，
we'd expect thirty percent of them to be
我们预计其中 30% 为
one
一，
and because the the element index two
因为元素索引 2
has only ten percent probability very
只有 10% 的概率，所以
few of these samples should be two and
这些样本中只有很少一部分是二，
indeed we only have a small number of
实际上我们只有少量的
twos
二，
and we can sample as many as we'd like
我们可以根据需要进行采样，
and the more we sample the more
采样越多，
these numbers should um roughly have the
这些数字就应该越多，大致具有
distribution here
这样的分布，
so we should have lots of zeros
所以我们应该有很多零，
half as many um
一半的嗯，一，
ones and we should have um three times
我们应该有嗯三倍
as few
少的，
oh sorry s few ones and three times as
哦，抱歉，是几个一和三倍
few uh
少的呃
twos
二，
so you see that we have very few twos we
所以你看，我们有很少的二，
have some ones and most of them are zero
有一些一，大多数是零，
so that's what torsion multinomial is
所以这就是扭转多项式在
doing
for us here
这里为我们做的事情，
we are interested in this row we've
我们对这一行感兴趣，我们在
created this
p here
这里创建了这个 p，
and now we can sample from it
现在我们可以从中采样，
so if we use the same
所以如果我们使用相同的
seed
种子，
and then we sample from this
然后从这个
distribution let's just get one sample
分布中采样，我们只得到一个样本，
then we see that the sample is say 13.
那么我们看到样本是 13。
so this will be the index
所以这将是索引，
and let's you see how it's a tensor that
让我们看看它是如何成为一个张量的
wraps 13 we again have to use that item
包裹了 13，我们再次使用该项目
to pop out that integer
弹出该整数
and now index would be just the number
，现在索引就是数字
13.
13。
and of course the um we can do
当然，我们可以做的是，
we can map the i2s of ix to figure out
我们可以映射 ix 的 i2s 来确定
exactly which character
we're sampling here we're sampling m
我们在这里采样的确切字符，我们正在采样 m，
so we're saying that the first character
所以我们说第一个字符
is
in our generation
在我们这一代，
and just looking at the road here
只要看一下这里的路线，
m was drawn and you we can see that m
m 就被绘制了，你可以看到 m
actually starts a large number of words
实际上开始了大量的单词，
uh m
嗯，m
started 2 500 words out of 32 000 words
开始了 32,000 个单词中的 2,500 个单词，
so almost
所以几乎
a bit less than 10 percent of the words
不到 10% 的单词以
start with them so this was actually a
它们开头，所以这实际上是一个
fairly likely character to draw
相当有可能绘制的字符，
um
嗯，这
so that would be the first character of
将是我们工作的第一个字符，
our work and now we can continue to
现在我们可以继续
sample more characters because now we
采样更多字符，因为现在我们
know that m started
知道 m 开始了
m is already sampled
m 已经被采样了，
so now to draw the next character we
所以现在要绘制下一个字符，我们
will come back here and we will look for
将回到这里，我们将寻找
the row
that starts with m
以 m 开头的行，
so you see m
所以你看到 m，
and we have a row here
我们在这里有一行，
so we see that m dot is
所以我们看到 m 点是
516 m a is this many and b is this many
516 m，a 是这么多，b 是这么多，
etc so these are the counts for the next
等等，所以这些是 下一
row and that's the next character that
行，也就是
we are going to now generate
我们现在要生成的下一个字符，
so i think we are ready to actually just
所以我认为我们已经准备好实际
write out the loop because i think
写出循环了，因为我认为
you're starting to get a sense of how
您开始了解
this is going to go
这将如何进行，
the um
嗯，
we always begin at
我们总是从
index 0 because that's the start token
索引 0 开始，因为那是起始标记，
and then while true
然后当为 true 时，
we're going to grab the row
我们将抓取与
corresponding to index
that we're currently on so that's p
当前所在索引对应的行，所以那是 p，所以那
so that's n array at ix
是 n 数组，在 ix 处
converted to float is rp
转换为浮点数是 rp，
then we normalize
然后我们将
this p to sum to one
这个 p 规范化为总和为 1，
i accidentally ran the infinite loop we
我不小心运行了无限循环，我们将
normalize p to something one
p 规范化为某个 1，
then we need this generator object
然后我们需要这个生成器对象，
now we're going to initialize up here
现在我们将在这里初始化，我们
and we're going to draw a single sample
将从这个分布中抽取一个样本，
from this distribution
and then this is going to tell us what
然后这将告诉我们
index is going to be next
下一个索引是什么，
if the index sampled is
如果采样的索引是
0，
then that's now the end token
那么它现在是结束标记，
so we will break
所以我们将中断，
otherwise we are going to print
否则我们将打印
s2i of ix
ix i2s 的 s2i，嗯，
i2s
and uh that's pretty much it we're just
差不多就是这样，我们只是
uh this should work okay more
嗯，这应该可以工作了，好吧，更多，
so that's that's the name that we've
所以这就是我们的名字
sampled we started with m the next step
采样我们从 m 开始，下一步
was o then r and then dot
是 o，然后是 r，然后是点，
and this dot we it here as well
这个点我们也在这里，
so
所以
let's now do this a few times
现在让我们重复几次，让我们在
so let's actually create an
out list here
这里创建一个输出列表，
and instead of printing we're going to
而不是打印，而是
append
so out that append this character
附加这个字符，
and then here let's just print it at the
然后在这里，让我们把它打印在
end so let's just join up all the outs
最后，让我们把所有的输出连接起来，然后
and we're just going to print more okay
打印更多，好的，
now we're always getting the same result
现在我们总是得到相同的结果，
because of the generator
因为有生成器，
so if we want to do this a few times we
所以如果我们想重复几次，我们
can go for i in range
可以在 10 的范围内进行采样，
10 we can sample 10 names
我们可以采样 10 个名字
and we can just do that 10 times
，我们可以重复 10 次，
and these are the names that we're
这些就是我们得到的名字，我们来
getting out
let's do 20.
采样 20 个。
i'll be honest with you this doesn't
老实说，这看起来不对劲，
look right
so i started a few minutes to convince
所以我花了几分钟来说服
myself that it actually is right
自己这是对的，
the reason these samples are so terrible
这些样本如此糟糕的原因
is that bigram language model
是二元语言模型
is actually look just like really
实际上看起来非常
terrible
糟糕，
we can generate a few more here
我们可以在这里生成更多，
and you can see that they're kind of
你可以看到它们有点
like their name like a little bit like
像它们的名字，有点像
yanu o'reilly etc but they're just like
yanu  o'reilly 等等，但他们
totally messed up um
完全搞砸了，嗯，
and i mean the reason that this is so
我的意思是，这很
bad like we're generating h as a name
糟糕的原因就像我们生成 h 作为名称，
but you have to think through
但你必须
it from the model's eyes it doesn't know
从模型的角度考虑它，它不知道
that this h is the very first h all it
这个 h 是第一个 h，它只
knows is that h was previously and now
知道 h 之前是，现在
how likely is h the last character well
h 是最后一个字符的可能性有多大，嗯，
it's somewhat
有点
likely and so it just makes it last
可能，所以它只是把它作为最后一个
character it doesn't know that there
字符，它不知道在
were other things before it or there
它之前还有其他东西，或者在
were not other things before it and so
它之前没有其他东西，所以
that's why it's generating all these
它生成所有这些
like
nonsense names
无意义的名字
another way to do this is
另一种方法是
to convince yourself that this is
让自己相信这
actually doing something reasonable even
实际上是在做一些合理的事情，
though it's so terrible is
即使它很糟糕，
these little piece here are 27 right
这里的小片段是 27，对吧，
like 27.
比如 27。
so how about if we did something like
所以如果我们做这样的事情而
this
instead of p having any structure
不是 p 有任何结构怎么样，
whatsoever
how about if p was just
如果 p 只是默认情况下 27 中的
torch dot once
一次火炬点，
of 27
by default this is a float 32 so this is
这是一个浮点数 32，所以这
fine divide 27
很好，除以 27，
so what i'm doing here is this is the
所以我在这里做的是，这是
uniform distribution which will make
均匀分布，这将使
everything equally likely
所有事情都同样可能，
and we can sample from that so let's see
我们可以从中抽样 让我们看看这样做会不会
if that does any better
更好，
okay so it's
好吗？
this is what you have from a model that
这是从一个
is completely untrained where everything
完全未经训练的模型中得到的，在这个模型中，所有事物都是
is equally likely so it's obviously
等概率的，所以它显然是
garbage and then if we have a trained
垃圾。如果我们有一个
model which is trained on just bi-grams
只用二元语法训练过的模型，
this is what we get so you can see that
这就是我们得到的结果，所以你可以看到
it is more name-like it is actually
它更像名字，它实际上是在
working it's just um
工作的，只是
my gram is so terrible and we have to do
我的语法太糟糕了，我们现在必须做得更好。
better now next i would like to fix an
接下来，我想修复
inefficiency that we have going on here
我们在这里遇到的一个效率低下的问题，
because what we're doing here is we're
因为我们在这里所做的是，我们
always fetching a row of n from the
总是从前面的计数矩阵中取出一行 n，
counts matrix up ahead
and then we're always doing the same
然后我们总是做同样的
things we're converting to float and
事情，我们将它转​​换为浮点数，然后
we're dividing and we're doing this
进行除法，我们
every single iteration of this loop and
在循环的每次迭代中都这样做，
we just keep renormalizing these rows
我们只是一遍又一遍地重新规范化这些行，
over and over again and it's extremely
这是非常
inefficient and wasteful so what i'd
低效和浪费的，所以我
like to do is i'd like to actually
想做的是，我实际上想
prepare a matrix capital p that will
准备一个大写 p 矩阵，它只
just have the probabilities in it so in
包含概率，
other words it's going to be the same as
换句话说，它将与
the capital n matrix here of counts but
这里的大写 n 矩阵包含计数，但
every single row will have the row of
每一行都会有一行
probabilities uh that is normalized to 1
概率，嗯，该概率被标准化为 1，
indicating the probability distribution
表示给定前一个字符，下一个字符的概率分布，
for the next character given the
character before it
um as defined by which row we're in
嗯，由我们所在行定义，
so basically what we'd like to do is
所以基本上我们想做的是，我们想在
we'd like to just do it up front here
这里预先做，
and then we would like to just use that
然后我们想在这里使用那一
row here so here we would like to just
行，所以在这里我们想做
do p equals p of ix instead
p 等于 ix 的 p，
okay
好的，
the other reason i want to do this is
我这样做的另一个原因
not just for efficiency but also i would
不仅仅是为了提高效率，而且我
like us to practice
希望我们能够练习
these n-dimensional tensors and i'd like
这些 n 维张量，我希望
us to practice their manipulation and
我们能够练习它们的操作，
especially something that's called
特别是所谓的
broadcasting that we'll go into in a
广播，我们稍后会讲到，
second
we're actually going to have to become
我们实际上必须
very good at these tensor manipulations
非常擅长这些张量操作，
because if we're going to build out all
因为如果我们要一路构建
the way to transformers we're going to
到 Transformer，我们将
be doing some pretty complicated um
进行一些非常复杂的
array operations for efficiency and we
数组操作以提高效率，我们
need to really understand that and be
需要真正理解这一点，并且
very good at it
非常擅长它，
so intuitively what we want to do is we
所以直观地讲，我们想要做的是，我们
first want to grab the floating point
首先要获取
copy of n
n 的浮点副本，
and i'm mimicking the line here
然后 我基本上是在模仿这条线，
basically
and then we want to divide all the rows
然后我们要除以所有行，
so that they sum to 1.
使它们的总和为 1。
so we'd like to do something like this p
所以我们想做类似 p
divide p dot sum
除 p 点和的操作，
but
但
now we have to be careful
现在我们必须小心，
because p dot sum actually
因为 p 点和实际上会
produces a sum
产生一个和，
sorry equals and that float copy
抱歉，等于，而浮点复制
p dot sum produces a um
p 点和会产生一个 um，它会将
sums up all of the counts of this entire
整个矩阵 n 的所有计数相加，
matrix n and gives us a single number of
然后给我们一个数字，只是
just the summation of everything so
所有数字的总和，所以
that's not the way we want to define
这不是我们想要定义
divide we want to simultaneously and in
除法的方式，我们想要同时
parallel divide all the rows
并行地将所有行
by their respective sums
除以它们各自的和，
so what we have to do now is we have to
所以我们现在要做的是
go into documentation for torch.sum
进入 torch.sum 的文档，
and we can scroll down here to a
我们可以向下滚动到与
definition that is relevant to us which
我们相关的定义，在这里
is where we don't only provide an input
我们不仅提供
array that we want to sum but we also
要求和的输入数组，而且还
provide the dimension along which we
提供要求和的维度，
want to sum
and in particular we want to sum up
特别是我们要对行进行求和，
over rows
right
now one more argument that i want you to
现在我希望你注意的另一个参数
pay attention to here is the keep them
是 keep them
is false
为 false，
if keep them is true then the output
如果 keep them 为 true，那么输出
tensor is of the same size as input
张量的大小与输入的大小相同，
except of course the dimension along
当然， 维度
which is summed which will become just
求和后会变成
one
1，
but if you pass in keep them as false
但是如果传入 keep 为 false，
then this dimension is squeezed out and
那么这个维度就会被挤出去，
so torch.sum not only does the sum and
所以 torch.sum 不仅会求和并将
collapses dimension to be of size one
维度折叠为 1，而且还会进行
but in addition it does what's called a
所谓的
squeeze where it squeezes out it
挤压，
squeezes out that dimension
挤出那个维度，
so
所以
basically what we want here is we
基本上我们想要的是
instead want to do p dot sum of some
对某个轴进行 p 点求和
axis
and in particular notice that p dot
，特别要注意 p 点的
shape is 27 by 27
形状是 27 x 27，
so when we sum up across axis zero then
所以当我们对零轴求和时，我们
we would be taking the zeroth dimension
会取第零维，然后对它进行
and we would be summing across it
求和，
so when keep them as true
所以当 keep 为 true 时，
then this thing will not only give us
这个东西不仅会给我们
the counts across um
沿着列的计数，
along the columns
but notice that basically the shape of
还要注意，基本上它的形状
this is 1 by 27 we just get a row vector
是 1 x 27，我们只得到一个行向量，
and the reason we get a row vector here
我们在这里再次得到行向量的原因
again is because we passed in zero
是我们传入了零
dimension so this zero dimension becomes
维度，所以这个零维度变成了
one and we've done a sum
1，我们进行了求和，
and we get a row and so basically we've
得到了一行，所以基本上我们以
done the sum
this way
这种方式
vertically and arrived at just a single
垂直求和，得到了一个
1 by 27
1 x 27 的
vector of counts
计数向量，
what happens when you take out keep them
当你取出 keep 时会发生什么 结果
is that we just get 27. so it squeezes
是 27，所以它挤出
out that dimension and we just get
了那个维度，我们得到的只是
a one-dimensional vector of size 27.
一个大小为 27 的一维向量。
now we don't actually want
现在我们实际上不需要
one by 27 row vector because that gives
1*27 的行向量，因为那样得到的是
us the counts or the sums across
计数或跨
the columns
列的和，
we actually want to sum the other way
我们实际上想要
along dimension one and you'll see that
沿着第一维反向求和，你会看到
the shape of this is 27 by one so it's a
它的形状是 27*1，所以它是一个
column vector it's a 27 by one
列向量，它是一个 27*1 的
vector of counts
计数向量，
okay
好的，
and that's because what's happened here
这是因为这里发生的情况
is that we're going horizontally and
是，我们水平移动，
this 27 by 27 matrix becomes a 27 by 1
这个 27*27 的矩阵变成了一个 27*1 的
array
数组，
now you'll notice by the way that um the
现在你会注意到，
actual numbers
of these counts are identical
这些计数的实际数字是相同的，
and that's because this special array of
这是因为这里的特殊
counts here comes from bi-gram
计数数组来自二元语法
statistics and actually it just so
统计，实际上，
happens by chance
碰巧
or because of the way this array is
或者因为这个数组的
constructed that the sums along the
构造方式，沿
columns or along the rows horizontally
列或沿水平或
or vertically is identical
垂直方向的和是相同的，
but actually what we want to do in this
但实际上，在这种情况下，我们想要的
case is we want to sum across the
是水平跨行求和，
rows
horizontally so what we want here is p
所以我们想要的是 p
that sum of one with keep in true
保持
27 by one column vector
27 乘以一列向量的和，
and now what we want to do is we want to
现在我们要做的是
divide by that
除以它，
now we have to be careful here again is
现在我们必须再次小心，是否可以
it possible to take
取
what's a um p dot shape you see here 27
一个 um p 点形状，你在这里看到的 27
by 27 is it possible to take a 27 by 27
乘 27 是否可以取一个 27 乘 27 的
array and divide it by what is a 27 by 1
数组并将其除以 27 乘 1 的
array
数组，这
is that an operation that you can do
是一个您可以执行的操作，
and whether or not you can perform this
而是否可以执行此
operation is determined by what's called
操作取决于所谓的
broadcasting rules so if you just search
广播规则，因此，如果您
broadcasting semantics in torch
在 torch 中搜索广播语义，
you'll notice that there's a special
您会注意到，对于所谓的广播有一个特殊的
definition for
定义，呃，
what's called broadcasting that uh for
whether or not um these two uh arrays
这两个呃数组是否
can be combined in a binary operation
可以在二元运算（如除法）中组合，
like division
so the first condition is each tensor
所以第一个条件是每个张量
has at least one dimension which is the
至少有一个维度，这对
case for us
我们来说就是这种情况，
and then when iterating over the
然后，当
dimension sizes starting at the trailing
从尾随维度开始迭代维度大小时，
dimension
the dimension sizes must either be equal
维度大小必须相等，
one of them is one or one of them does
其中一个是 1，或者其中一个
not exist
不存在，
okay
好的，
so let's do that we need to align the
所以让我们这样做，我们需要对齐
two arrays and their shapes which is
两个数组及其形状 这
very easy because both of these shapes
非常简单，因为这两个形状
have two elements so they're aligned
都有两个元素，所以它们是对齐的，
then we iterate over from the from the
然后我们从右向左迭代，
right and going to the left
each dimension must be either equal one
每个维度要么相等，
of them is a one or one of them does not
其中一个是 1，要么其中一个不
exist so in this case they're not equal
存在，所以在这种情况下它们不相等，
but one of them is a one so this is fine
但其中一个是 1，所以这样没问题，
and then this dimension they're both
然后在这个维度上它们都
equal
相等，所以
so uh this is fine
呃，这样没问题，
so all the dimensions are fine and
所以所有维度都没问题，
therefore the this operation is
因此这个操作是可
broadcastable so that means that this
广播的，这意味着这个
operation is allowed
操作是允许的，
and what is it that these arrays do when
当
you divide 27 by 27 by 27 by one
你用 27 除以 27 除以 27 除以 1 时，这些数组会做什么？
what it does is that it takes this
它所做的是取这个
dimension one and it stretches it out it
维度 1，然后将其拉伸，将
copies it to match
其复制以匹配
27 here in this case
27，在本
so in our case it takes this column
例中，它取这个
vector which is 27 by 1
27 x 1 的列向量，并将
and it copies it 27 times
其复制 27 次，
to make
使
these both be 27 by 27 internally you
它们在内部都是 27 x 27，你
can think of it that way and so it
可以这样想，所以它
copies those counts
复制这些计数，
and then it does an element-wise
然后进行逐元素
division
除法，
which is what we want because these
这就是我们想要的，因为
counts we want to divide by them on
我们希望在每一列上都用这些计数除以它们
every single one of these columns in
在
this matrix
这个矩阵中，
so this actually we expect will
我们实际上期望对
normalize
every single row
每一行进行归一化，
and we can check that this is true by
我们可以
taking the first row for example and
以第一行为例，计算其和，
taking its sum we expect this to be
我们期望结果是
1. because it's not normalized
1，因为它没有被归一化，
and then we expect this now because if
所以我们现在期望这个，因为如果
we actually correctly normalize all the
我们正确地对所有行进行归一化，
rows we expect to get the exact same
我们期望得到完全相同的
result here so let's run this
结果，所以让我们运行一下，
it's the exact same result
结果是完全相同的，
this is correct so now i would like to
这是正确的，所以现在我想
scare you a little bit
稍微吓吓你一下，
uh you actually have to like i basically
你实际上必须喜欢我基本上
encourage you very strongly to read
非常鼓励你阅读
through broadcasting semantics
广播语义，
and i encourage you to treat this with
我鼓励你
respect and it's not something to play
尊重它，它不是一件可以玩弄的东西，它需要
fast and loose with it's something to
really respect really understand and
真正尊重，真正理解，
look up maybe some tutorials for
查找一些
broadcasting and practice it and be
广播教程，练习它，
careful with it because you can very
小心使用它，因为你可能会
quickly run into books let me show you
很快遇到书籍，让我告诉你我的
what i mean
意思，
you see how here we have p dot sum of
你看这里我们有 p 点和
one keep them as true
1，保持它们为真，
the shape of this is 27 by one let me
这个形状是 27 乘以 1，让我
take out this line just so we have the n
取出这条线，这样我们就有了 n
and then we can see the counts
，然后我们可以看到计数，
we can see that this is a all the counts
我们可以看到这是所有计数
across all the
rows
行，
and it's a 27 by one column vector right
它是一个 27 乘 1 列的向量，
now suppose that i tried to do the
现在假设我尝试执行
following
以下操作，
but i erase keep them just true here
但我擦除了它们，只保留 true，这会有
what does that do if keep them is not
什么作用，如果 keep them 不为
true it's false then remember according
true，则为 false，然后记住根据
to documentation it gets rid of this
文档，它会摆脱这个
dimension one it squeezes it out so
维度，它会将其挤出来，所以
basically we just get all the same
基本上我们得到所有相同的
counts the same result except the shape
计数，相同的结果，除了
of it is not 27 by 1 it is just 27 the
它的形状不是 27 乘 1，它只是 27，
one disappears
一个消失了，
but all the counts are the same
但所有的计数都相同，
so you'd think that this divide that
所以你会认为这种划分会
would uh would work
起作用，
first of all can we even uh write this
首先，我们可以写下来吗，它
and will it is it even is it even
甚至可以运行吗，
expected to run is it broadcastable
它是否可以广播，
let's determine if this result is
让我们确定这个结果是否可
broadcastable
广播
p.summit one is shape
p.summit 的形状
is 27.
是 27。
this is 27 by 27. so 27 by 27
这是 27 乘 27。所以 27 乘 27
broadcasting into 27. so now
广播成 27。所以现在
rules of broadcasting number one align
广播规则一将
all the dimensions on the right done now
所有维度对齐右侧完成现在
iteration over all the dimensions
迭代所有维度
starting from the right going to the
从右到
left
左
all the dimensions must either be equal
所有维度必须相等
one of them must be one or one that does
其中一个必须为一或一个
not exist so here they are all equal
不存在，所以这里它们都是相等的，
here the dimension does not exist
这里的维度不存在，
so internally what broadcasting will do
所以内部广播
is it will create a one here
会在这里创建一个，
and then
然后
we see that one of them is a one and
我们看到其中一个是1，它
this will get copied and this will run
会被复制，然后运行，然后
this will broadcast
广播，
okay so you'd expect this
好的，所以你会期望它能
to work
工作，
because we we are
因为我们是
this broadcast and this we can divide
这个广播，我们可以现在除以
this
它，
now if i run this you'd expect it to
如果我运行它，你会期望它
work but
工作，但
it doesn't
它没有，
uh you actually get garbage you get a
呃，你实际上得到了垃圾，你得到了一个
wrong dissolve because this is actually
错误的溶解，因为这实际上是
a bug
一个错误，
this keep them equals true
保持它们相等
makes it work
使它工作，
this is a bug
这是一个错误，
in both cases we are doing
在这两种情况下，我们都在进行
the correct counts we are summing up
正确的计数，我们在行之间求和，
across the rows
but keep them is saving us and making it
但保持它们可以节省我们的时间并使其
work so in this case
工作，所以在这种情况下，
i'd like to encourage you to potentially
我想鼓励你
like pause this video at this point and
暂停这个视频，
try to think about why this is buggy and
试着思考为什么这是有缺陷的，
why the keep dim was necessary here
为什么保持暗淡是必要的，
okay
好的，
so the reason to do
这样做的原因
for this is i'm trying to hint it here
是我试图在这里提示一下，
when i was sort of giving you a bit of a
当我给你一点
hint on how this works
关于它如何工作的提示时，
this
这个
27 vector
27向量在
internally inside the broadcasting this
广播内部
becomes a 1 by 27
变成了1乘27
and 1 by 27 is a row vector right
和1乘 27 是一个行向量，
and now we are dividing 27 by 27 by 1 by
现在我们用 27 除以 27，再除以 1 除以
27
27
and torch will replicate this dimension
，torch 会复制这个维度，
so basically
所以基本上，
uh it will take
it will take this
它会取这个
row vector and it will copy it
行向量，
vertically now
然后垂直复制
27 times so the 27 by 27 lies exactly
27 次，所以 27 乘以 27 正好是
and element wise divides
逐元素除法，
and so basically what's happening here
所以基本上这里发生的
is
是，
we're actually normalizing the columns
我们实际上是在对列进行归一化，而
instead of normalizing the rows
不是对行进行归一化，
so you can check that what's happening
所以你可以检查这里发生的事情
here is that p at zero which is the
是 p 为零，也就是
first row of p dot sum
p 点和的第一行
is not one it's seven
不是 1，而是 7，
it is the first column as an example
它是第一列，作为一个例子，它的
that sums to one
总和为 1，
so
所以
to summarize where does the issue come
总结一下问题出在哪里，问题
from the issue comes from the silent
出在
adding of a dimension here because in
这里，因为在
broadcasting rules you align on the
广播规则中，你
right and go from right to left and if
向右对齐，从右到左，如果
dimension doesn't exist you create it
维度不存在，你就创建它，
so that's where the problem happens we
所以问题就出在这里，我们
still did the counts correctly we did
仍然正确地进行了计数，我们
the counts across the rows and we got
对行进行了计数，我们得到了
the the counts on the right here as a
右边的计数作为
column vector but because the keep
列向量，但是因为保持
things was true this this uh this
事物是正确的，所以这个呃这个
dimension was discarded and now we just
维度被丢弃了，现在我们
have a vector of 27. and because of
只有一个 27 的向量。由于
broadcasting the way it works this
广播的工作方式，这个
vector of 27 suddenly becomes a row
27 的向量突然变成了一个行
vector
向量，
and then this row vector gets replicated
然后这个行向量被
vertically and that every single point
垂直复制，而
we are dividing by the by the count
我们用
in the opposite direction
相反方向的计数除以每个点，所以
so uh
so this thing just uh doesn't work this
这个东西根本行不通，
needs to be keep things equal true in
在这种情况下需要保持事物相等，
this case
so then
所以
then we have that p at zero is
然后我们有 p 在零处被
normalized
归一化，
and conversely the first column you'd
相反，第一列你
expect to potentially not be normalized
期望可能不会被归一化，
and this is what makes it work
这就是它工作的原因，
so pretty subtle and uh hopefully this
非常微妙，希望这
helps to scare you that you should have
有助于吓到你，你应该
a respect for broadcasting be careful
尊重广播，小心
check your work uh and uh understand how
检查你的工作，了解
it works under the hood and make sure
它在底层是如何工作的，并确保
that it's broadcasting in the direction
它朝着
that you like otherwise you're going to
你喜欢的方向广播，否则你会
introduce very subtle bugs very hard to
引入非常微妙的错误，很难
find bugs and uh just be careful one
找到错误，只是要小心，
more note on efficiency we don't want to
关于效率还有一点要注意，我们不想在
be doing this here because this creates
这里这样做，因为这会创建
a completely new tensor that we store
一个全新的张量，我们将其存储
into p
在 p 中，
we prefer to use in place operations if
我们更喜欢在现场操作中使用 如果
possible
可能的话，
so this would be an in-place operation
这将是一个就地操作，
it has the potential to be faster it
它有可能更快，它
doesn't create new memory
不会在后台创建新的内存，
under the hood and then let's erase this
然后让我们删除它，
we don't need it
我们不需要它，我们
and let's
also
也
um just do fewer just so i'm not wasting
只做更少的操作，这样我就不会浪费
space
空间，
okay so we're actually in a pretty good
好的，所以我们现在实际上处于一个非常好的
spot now
位置，
we trained a bigram language model and
我们训练了一个二元语言模型，
we trained it really just by counting uh
我们实际上只是通过计算
how frequently any pairing occurs and
任何配对发生的频率来训练它，
then normalizing so that we get a nice
然后进行规范化，以便我们获得一个很好的
property distribution
属性分布，
so really these elements of this array p
所以这个数组 p 的这些元素
are really the parameters of our biogram
实际上是我们的传记
language model giving us and summarizing
语言模型的参数，它给我们并总结了
the statistics of these bigrams
这些二元语言的统计数据，
so we train the model and then we know
所以我们训练模型，然后我们知道
how to sample from a model we just
如何从模型中采样，我们只是
iteratively uh sample the next character
迭代地采样下一个字符
and feed it in each time and get a next
并将其输入，然后获得下一个
character
字符
now what i'd like to do is i'd like to
现在我想要做的是，我想以
somehow evaluate the quality of this
某种方式评估这个
model we'd like to somehow summarize the
模型的质量，我们想以某种方式将
quality of this model into a single
这个模型的质量总结成一个
number how good is it at predicting
数字，它在预测训练集方面有多好
the training set
and as an example so in the training set
，作为一个例子，所以在训练集中，
we can evaluate now the training loss
我们现在可以评估训练损失
and this training loss is telling us
和这个训练 损失函数就像
about
sort of the quality of this model in a
single number just like we saw in
我们在 micrograd 中看到的那样，用一个数字来告诉我们这个模型的质量，
micrograd
so let's try to think through the
所以让我们试着思考一下
quality of the model and how we would
模型的质量以及我们如何
evaluate it
评估它，
basically what we're going to do is
基本上我们要做的就是
we're going to copy paste this code
复制粘贴
that we previously used for counting
我们之前用于计数的代码，
okay
好的，
and let me just print these diagrams
让我先打印这些图表，
first we're gonna use f strings
我们将使用 f 字符串，
and i'm gonna print character one
我将打印字符 1，然后打印
followed by character two these are the
字符 2，这些是
diagrams and then i don't wanna do it
图表，然后我不想
for all the words just do the first
对所有单词都这样做，只打印前
three words so here we have emma olivia
三个单词，所以这里我们有 emma olivia
and ava bigrams
和 ava 二元语法，
now what we'd like to do is we'd like to
现在我们想要做的是，我们
basically look at the probability that
基本上想看看
the model assigns to every one of these
模型分配给每个
diagrams
图表的概率，
so in other words we can look at the
换句话说，我们可以查看
probability which is
summarized in the matrix b
在
of i x 1 x 2
i x 1 x 2 的矩阵 b 中总结的概率，
and then we can print it here
然后我们可以在这里将其打印
as probability
为概率，
and because these properties are way too
因为这些属性
large let me present
太大了，让我介绍
or call in 0.4 f
或调用 0.4 f 来
to like truncate it a bit
截断它一点，
so what do we have here right we're
所以我们在这里有什么，我们正在
looking at the probabilities that the
看
model assigns to every one of these
模型分配给
bigrams in the dataset
数据集中每个二元词组的概率，
and so we can see some of them are four
我们可以看到其中一些是 4
percent three percent etc
%、3% 等等，
just to have a measuring stick in our
只是为了在我们心中有一个衡量标准，顺便说一下，
mind by the way um we have 27 possible
我们有 27 个可能的
characters or tokens and if everything
字符或标记，如果所有字符或标记的概率都
was equally likely then you'd expect all
相同，那么你会期望所有
these probabilities
这些概率都
to be
four percent roughly
大约是 4%，
so anything above four percent means
所以任何超过 4% 的概率都意味着
that we've learned something useful from
我们从这些二元词组的统计数据中学到了一些有用的东西，
these bigram statistics and you see that
你会看到，
roughly some of these are four percent
其中一些大约是 4%，
but some of them are as high as 40
但有些高达 40
percent
%、
35 percent and so on so you see that the
35% 等等，所以你看，
model actually assigned a pretty high
模型实际上为训练集中的任何东西分配了相当高的
probability to whatever's in the
概率，
training set and so that's a good thing
所以这是一件好事，
um basically if you have a very good
基本上，如果你有一个非常好的
model you'd expect that these
模型，你会期望这些
probabilities should be near one because
概率应该接近 1，因为
that means that your model is correctly
这意味着你的模型正确地
predicting what's going to come next
预测了接下来会发生什么，
especially on the training set where you
特别是在你训练模型的训练集上，
where you trained your model
so
所以
now we'd like to think about how can we
现在我们想思考如何将
summarize these probabilities into a
这些概率总结成
single number that measures the quality
一个数字来衡量
of this model
这个模型的质量，
now when you look at the literature into
现在当你查看文献中的
maximum likelihood estimation and
最大似然估计时，
statistical modeling and so on
统计建模等等，
you'll see that what's typically used
你会发现这里通常用到的
here is something called the likelihood
是似然函数，似然
and the likelihood is the product of all
函数是所有这些概率的乘积，
of these probabilities
and so the product of all these
所以所有这些
probabilities is the likelihood and it's
概率的乘积就是似然函数，它
really telling us about the probability
实际上告诉我们的是
of the entire data set assigned uh
整个数据集的概率，这个概率是
assigned by the model that we've trained
由我们训练的模型分配的，
and that is a measure of quality
这是一个质量衡量标准，
so the product of these
所以在训练模型时，这些概率的乘积
should be as high as possible
应该尽可能高，
when you are training the model and when
当
you have a good model your pro your
你有一个好的模型时，
product of these probabilities should be
这些概率的乘积应该
very high
非常高，
um
嗯，
now because the product of these
因为这些概率的乘积
probabilities is an unwieldy thing to
很难处理，
work with you can see that all of them
你可以看到它们都在
are between zero and one so your product
0到1之间，所以
of these probabilities will be a very
这些概率的乘积会是一个非常
tiny number
小的数字，
um
嗯，
so
for convenience what people work with
为了方便起见，人们
usually is not the likelihood but they
通常使用的不是似然函数，而是
work with what's called the log
对数似然函数，
likelihood
so
所以
the product of these is the likelihood
这些概率的乘积就是似然函数。
to get the log likelihood we just have
为了得到对数似然函数，我们只需要对
to take the log of the probability
概率取对数
and so the log of the probability here i
，所以概率的对数，这里我
have the log of x from zero to one
有x的对数，从0到1，对
the log is a you see here monotonic
数是
transformation of the probability
概率的单调变换。
where if you pass in one
如果你传入 1，
you get zero
你会得到 0，
so probability one gets your log
所以概率 1 得到的对数
probability of zero
概率为 0，
and then as you go lower and lower
然后随着概率越来越低，对
probability the log will grow more and
数会越来越
more negative until all the way to
负，直到负
negative infinity at zero
无穷大为 0，
so here we have a log prob which is
所以这里我们有一个对数概率，它
really just a torch.log of probability
实际上只是一个概率的 torch.log，
let's print it out to get a sense of
让我们把它打印出来，看看它是
what that looks like
什么样子的对
log prob
数概率
also 0.4 f
也是 0.4 f，
okay
好的，
so as you can see when we plug in
所以正如你所看到的，当我们插入
numbers that are very close some of our
非常接近一些
higher numbers we get closer and closer
较大的数字时，我们会越来越接近
to zero
0，
and then if we plug in very bad
然后如果我们插入非常糟糕的
probabilities we get more and more
概率，我们会得到越来越多的
negative number that's bad
负数，这很糟糕，
so
所以
and the reason we work with this is for
我们使用这个的原因
a large extent convenience right
很大程度上是为了方便，
because we have mathematically that if
因为从数学上讲，如果
you have some product a times b times c
你有所有这些概率的乘积 a 乘以 b 乘以 c，那么
of all these probabilities right
似然
the likelihood is the product of all
就是所有
these probabilities
这些概率的乘积，
then the log
那么这些概率的对数
of these
is just log of a plus
就是 a 的对数加上
log of b
b 的对数
plus log of c if you remember your logs
加上 c 的对数，如果你还记得你
from your
high school or undergrad and so on
高中或本科的对数等等，
so we have that basically
所以我们基本上有
the likelihood of the product
乘积概率的似然，
probabilities the log likelihood is just
对数似然就是对
the sum of the logs of the individual
数的总和 单个
probabilities
概率，
so
所以
log likelihood
对数似然
starts at zero
从零开始，
and then log likelihood here we can just
然后对数似然在这里我们可以
accumulate simply
简单地累加，
and in the end we can print this
最后我们可以打印这个
print the log likelihood
打印对数似然
f strings
f字符串
maybe you're familiar with this
也许你很熟悉
so log likelihood is negative 38.
所以对数似然是-38。
okay
好的
now
现在
we actually want um
我们实际上想要嗯所以对
so how high can log likelihood get it
数似然可以达到多高它
can go to zero so when all the
可以趋近于零所以当所有
probabilities are one log likelihood
概率都是1时对数似然
will be zero and then when all the
将为零然后当所有
probabilities are lower this will grow
概率都较低时它会变得
more and more negative
越来越负
now we don't actually like this because
现在我们实际上不喜欢这个因为
what we'd like is a loss function and a
我们想要的是一个损失函数而
loss function has the semantics that low
损失函数的语义是低
is good
是好的
because we're trying to minimize the
因为我们试图最小化
loss so we actually need to invert this
损失所以我们实际上需要将其反转这
and that's what gives us something
就是我们所说的负对
called the negative log likelihood
数似然负对数似然
negative log likelihood is just negative
就是对数似然的负数
of the log likelihood
these are f strings by the way if you'd
这些是f字符串顺便说一下如果你
like to look this up
想查一下负对数似然
negative log likelihood equals
等于
so negative log likelihood now is just
所以负对数似然现在就是
negative of it and so the negative log
它的负数所以负对
block load is a very nice loss function
数块负载是一个非常好的损失函数
because um
因为嗯
the lowest it can get is zero
它能得到的最低值是零
and the higher it is the worse off the
而且它越高
predictions are that you're making
你做的预测就越差
and then one more modification to this
然后一
that sometimes people do is that for
人们有时会对此进行更多修改，为了
convenience uh they actually like to
方便起见，他们实际上喜欢将其
normalize by they like to make it an
标准化，他们喜欢将其设为
average instead of a sum
平均值而不是总和，
and so uh here
所以，这里
let's just keep some counts as well
我们也保留一些计数，
so n plus equals one
因此 n 加 1
starts at zero
从零开始，
and then here
然后这里，
um we can have sort of like a normalized
嗯，我们可以得到类似正则化对
log likelihood
数似然的数值，
um
嗯，
if we just normalize it by the count
如果我们仅通过计数对其进行正则化，
then we will sort of get the average
那么我们就会得到平均
log likelihood so this would be
对数似然，因此这
usually our loss function here is what
通常是我们的损失函数，这就是
this we would this is what we would use
我们将要使用的，
uh so our loss function for the training
因此，
set assigned by the model is 2.4 that's
模型分配给训练集的损失函数是 2.4，这是该
the quality of this model
模型的质量，
and the lower it is the better off we
它越低，效果越好，
are and the higher it is the worse off
越高，效果越差，我们
we are
and
the job of our you know training is to
训练的任务是
find the parameters that minimize the
找到最小化负对数似然损失的参数，
negative log likelihood loss
and that would be like a high quality
这就像一个高质量的
model okay so to summarize i actually
模型，好吧，总结一下，我实际上在
wrote it out here
这里写出来了，
so our goal is to maximize likelihood
所以我们的目标是最大化似然，它
which is the
是
product of all the probabilities
assigned by the model
模型分配的所有概率的乘积，
and we want to maximize this likelihood
我们希望
with respect to the model parameters and
根据模型参数最大化这个似然，
in our case the model parameters here
在我们的例子中，这里的模型参数
are defined in the table these numbers
是 表中定义的这些数字，
the probabilities
概率
are
是
the model parameters sort of in our
我们
program language models so far but you
目前为止编程语言模型中的模型参数，但你
have to keep in mind that here we are
必须记住，我们在这里
storing everything in a table format the
以表格形式存储所有
probabilities but what's coming up as a
概率，但接下来要
brief preview is that these numbers will
简要介绍的是，这些数字
not be kept explicitly but these numbers
不会明确保存，而是
will be calculated by a neural network
由神经网络计算得出的，
so that's coming up
因此我们
and we want to change and tune the
想要更改和调整
parameters of these neural networks we
这些神经网络的参数，我们希望
want to change these parameters to
更改这些参数以最大化似然值，即
maximize the likelihood the product of
the probabilities
概率的乘积，
now maximizing the likelihood is
现在最大化似然值
equivalent to maximizing the log
相当于最大化对数似然值，
likelihood because log is a monotonic
因为对数是一个单调
function
函数，
here's the graph of log
这是对数的图形，
and basically all it is doing is it's
它所做的基本上就是
just scaling your um you can look at it
缩放，嗯，你可以把它看作
as just a scaling of the loss function
是损失函数的缩放，
and so the optimization problem here and
所以这里和这里的优化问题
here are actually equivalent because
实际上是等价的，因为
this is just scaling you can look at it
这只是缩放，你可以这样看待它，
that way
and so these are two identical
所以这是两个相同的
optimization problems
优化问题，
um
嗯，
maximizing the log-likelihood is
最大化对数似然值
equivalent to minimizing the negative
相当于最小化负对
log likelihood and then in practice
数似然值，然后在实践中，
people actually minimize the average
人们实际上会最小化平均
negative log likelihood to get numbers
负对数似然值以得到
like 2.4
像 2.4 这样的数字，
and then this summarizes the quality of
然后总结一下
your model and we'd like to minimize it
你的模型的质量，我们希望最小化它，
and make it as small as possible
让它尽可能的小，
and the lowest it can get is zero
它能达到的最小值是零
and the lower it is
，它越低，
the better off your model is because
你的模型就越好，因为
it's signing it's assigning high
它在签名，它会
probabilities to your data now let's
为你的数据分配高概率，现在让我们
estimate the probability over the entire
估算一下整个
training set just to make sure that we
训练集的概率，只是为了确保我们
get something around 2.4 let's run this
得到大约2.4，让我们在
over the entire oops
整个oops上运行这个，
let's take out the print segment as well
让我们把打印部分也拿出来，
okay 2.45 or the entire training set
好的，2.45或整个训练集，
now what i'd like to show you is that
现在我想向你展示的是，
you can actually evaluate the
你可以实际评估
probability for any word that you want
任何你想要的单词的概率，
like for example
例如，
if we just test a single word andre and
如果我们只测试一个单词andre，然后
bring back the print statement
带回打印语句，
then you see that andre is actually kind
那么你会发现andre实际上有点
of like an unlikely word like on average
像一个不太可能的单词，平均而言，
we take
我们用
three
三个对
log probability to represent it and
数概率来表示它，
roughly that's because ej apparently is
粗略地说，这是因为ej显然
very uncommon as an example
非常不常见，举个例子，
now
现在仔细
think through this um
想想，
when i take andre and i append q and i
当我取andre并附加q，然后我
test the probability of it under q
在q下测试它的概率时，
we actually get
我们实际上得到了
infinity
无穷大，
and that's because jq has a zero percent
这是因为
probability according to our model so
根据我们的模型，jq的概率为零，所以 对
the log likelihood
数似然，
so the log of zero will be negative
所以零的对数将是负无穷大，
infinity we get infinite loss
我们得到无穷大的损失，
so this is kind of undesirable right
所以这有点不受欢迎，
because we plugged in a string that
因为我们插入了一个字符串，它
could be like a somewhat reasonable name
可能是一个比较合理的名称，
but basically what this is saying is
但基本上这意味着
that this model is exactly zero percent
这个模型预测这个名称的可能性是百分之零，
likely to uh to predict this
name
and our loss is infinity on this example
在这个例子中我们的损失是无穷大，
and really what the reason for that is
真正的原因是
that j
j后面
is followed by q
跟着q，呃
uh zero times
零乘以呃q，
uh where's q jq is zero and so jq is uh
其中jq为零，所以jq的
zero percent likely
可能性是百分之零，
so it's actually kind of gross and
所以这实际上有点恶心，
people don't like this too much to fix
人们不太喜欢解决
this there's a very simple fix that
这个问题，有一种非常简单的解决方法，
people like to do to sort of like smooth
人们喜欢这样做，有点像平滑
out your model a little bit and it's
你的模型，这
called model smoothing and roughly
叫做模型平滑，大致的
what's happening is that we will eight
情况是，
we will add some fake counts
我们会添加一些假计数，
so
imagine adding a count of one to
想象一下在所有东西上都加一个计数，
everything
so we add a count of one
所以我们像这样加一个计数，
like this
and then we recalculate the
然后我们重新计算
probabilities
概率，
and that's model smoothing and you can
这就是模型平滑，你可以
add as much as you like you can add five
添加任意数量的计数，比如加5，
and it will give you a smoother model
它会给你一个更平滑的模型，
and the more you add here
这里添加的越多，
the more
uniform model you're going to have and
模型就越均匀；添加的越
the less you add
少，
the more peaked model you are going to
模型就越尖锐，
have of course
当然，添加
so one is like a pretty decent count to
一个就足够了，这样就能
add
and that will ensure that there will be
确保
no zeros in our probability matrix p
概率矩阵 p 中没有零，当然，
and so this will of course change the
这会
generations a little bit in this case it
稍微改变代数，在本例中
didn't but in principle it could
没有，但原则上可以改变，
but what that's going to do now is that
但现在这样做的目的是，
nothing will be infinity unlikely
没有任何事情是无限不可能的，所以现在
so now
our model will predict some other
我们的模型会预测其他
probability and we see that jq now has a
概率，我们看到 jq 现在的
very small probability so the model
概率非常小，所以模型
still finds it very surprising that this
仍然会非常惊讶地发现这
was a word or a bigram but we don't get
是一个单词或一个二元组，但我们不会得到
negative infinity so it's kind of like a
负无穷，所以这有点像
nice fix that people like to apply
人们有时喜欢应用的一种很好的修复方法，
sometimes and it's called model
它被称为模型
smoothing okay so we've now trained a
平滑，好的，我们现在训练了一个
respectable bi-gram character level
相当不错的二元组字符级
language model and we saw that we both
语言模型，我们看到，我们都
sort of trained the model by looking at
通过查看
the counts of all the bigrams and
所有二元组的计数并对
normalizing the rows to get probability
行进行归一化来获得概率
distributions
分布，从而对模型进行了训练，
we saw that we can also then use those
我们看到，我们还可以使用
parameters of this model to perform
该模型的这些参数来
sampling of new words
对新词进行采样，
so we sample new names according to
所以我们进行采样 根据这些分布，我们可以得到新的名字，
those distributions and we also saw that
我们也看到，
we can evaluate the quality of this
我们可以评估这个模型的质量，
model and the quality of this model is
这个模型的质量可以
summarized in a single number which is
用一个数字来概括，那就是
the negative log likelihood and the
负对数似然，
lower this number is the better the
这个数字越低，模型就越好，
model is
because it is giving high probabilities
因为它为我们训练集中
to the actual next characters in all the
所有二元语法中实际的下一个字符赋予了很高的概率，
bi-grams in our training set
so that's all well and good but we've
所以这一切都很好，但是我们
arrived at this model explicitly by
通过做一些感觉合理的事情明确地得到了这个模型，
doing something that felt sensible we
我们
were just performing counts and then we
只是进行了计数，然后对
were normalizing those counts
这些计数进行了归一化，
now what i would like to do is i would
现在我想做的是，我
like to take an alternative approach we
想采用另一种方法，我们
will end up in a very very similar
最终会得到一个非常相似的结果，
position but the approach will look very
但方法看起来会非常
different because i would like to cast
不同，因为我想把
the problem of bi-gram character level
二元语法字符级
language modeling into the neural
语言建模的问题放到神经
network framework
网络框架中，
in the neural network framework we're
在神经网络框架中，我们
going to approach things slightly
将采用略有
differently but again end up in a very
不同的方法，但最终还是会得到一个非常
similar spot i'll go into that later now
相似的结果，我稍后会详细介绍，现在
our neural network is going to be a
我们的神经网络
still a background character level
仍然是一个背景字符级
language model so it receives a single
语言模型，因此它接收一个
character as an input
字符作为输入，
then there's neural network with some
然后有一个带有一些
weights or some parameters w
权重或参数 w 的神经网络，
and it's going to output the probability
它将输出概率
distribution over the next character in
a sequence it's going to make guesses as
序列中下一个字符的分布，它会猜测
to what is likely to follow this
character that was input to the model
输入到模型中的这个字符
and then in addition to that we're going
之后可能出现的字符是什么，除此之外，我们还
to be able to evaluate any setting of
可以评估
the parameters of the neural net because
神经网络参数的任何设置，因为
we have the loss function
我们有损失函数，也就是
the negative log likelihood so we're
负对数似然函数，所以我们
going to take a look at its probability
将查看它的概率
distributions and we're going to use the
分布，并使用
labels
标签，这些标签
which are basically just the identity of
基本上就是
the next character in that diagram the
图中下一个字符的身份，也就是
second character
第二个字符，
so knowing what second character
所以知道
actually comes next in the bigram allows
二元组中下一个字符是什么，
us to then look at what how high of
我们就可以看看
probability the model assigns to that
模型分配给该字符的概率有多高，
character
and then we of course want the
我们当然希望
probability to be very high
概率非常高，也
and that is another way of saying that
就是
the loss is low
损失很低，
so we're going to use gradient-based
所以我们将使用基于梯度的
optimization then to tune the parameters
优化来调整
of this network because we have the loss
这个网络的参数，因为我们有损失
function and we're going to minimize it
函数，我们将最小化它，
so we're going to tune the weights so
所以我们将调整权重，
that the neural net is correctly
以便神经网络能够正确
predicting the probabilities for the
预测下
next character
一个字符的概率，
so let's get started the first thing i
所以让我们开始吧，我要做的第一件事
want to do is i want to compile the
是编译
training set of this neural network
这个的训练集 神经网络，
right so
对吧，所以要
create
创建
the training set
of all the bigrams
所有二元语法的训练集，
okay
好的，在
and
here
这里
i'm going to copy paste this code
我要复制粘贴这段代码，
because this code iterates over all the
因为这段代码会遍历所有
programs
程序，
so here we start with the words we
所以在这里我们从单词开始，遍历
iterate over all the bygrams and
所有二元语法，
previously as you recall we did the
之前你还记得我们进行了
counts but now we're not going to do
计数，但现在我们不进行
counts we're just creating a training
计数，我们只是创建一个训练
set
集，
now this training set will be made up of
现在这个训练集将由
two lists
两个列表组成，
we have the
我们有
inputs
输入
and the targets
和目标，
the the labels
标签，
and these bi-grams will denote x y those
这些二元语法将表示 x y，这些
are the characters right
是字符，对吧，
and so we're given the first character
所以我们给出了二元语法的第一个字符，
of the bi-gram and then we're trying to
然后我们尝试
predict the next one
预测下一个字符，
both of these are going to be integers
这两个都是整数，
so here we'll take x's that append is
所以在这里我们取 x，附加的
just
只是
x1 ystat append ix2
x1 ystat 附加 ix2，
and then here
然后这里
we actually don't want lists of integers
我们实际上不需要整数列表，
we will create tensors out of these so
我们将用这些创建张量，所以
axis is torch.tensor of axis and wise a
轴是 axis 的 torch.tensor，明智的是
storage.tensor of ys
ys 的 storage.tensor，
and then
然后
we don't actually want to take all the
我们实际上还不想取出所有的
words just yet because i want everything
单词，因为我希望一切都易于
to be manageable
管理，
so let's just do the first word which is
所以 我们先来处理第一个单词
emma
emma，
and then it's clear what these x's and
然后 x 和 y 就一目了然了。
y's would be
here let me print
我先打印
character 1 character 2 just so you see
字符 1 和字符 2，这样你就能明白
what's going on here
这里发生了什么。
so the bigrams of these characters is
这些字符的二元组是
dot e e m m m a a dot so this single
dot e e m m m a a dot。正如
word as i mentioned has one two three
我之前提到的，这个单词有 one two three
four five examples for our neural
four five 的例子。对于我们的神经
network
网络来说，
there are five separate examples in emma
emma 中有五个独立的例子。
and those examples are summarized here
这些例子在这里进行了总结。
when the input to the neural network is
当神经网络的输入是
integer 0
整数 0 时，
the desired label is integer 5 which
所需的标签是整数 5，它
corresponds to e when the input to the
对应于 e。当
neural network is 5 we want its weights
神经网络的输入是 5 时，我们希望它的权重能够
to be arranged so that 13 gets a very
得到非常
high probability
高的概率。
when 13 is put in we want 13 to have a
当输入 13 时，我们希望 13 具有
high probability
较高的概率；
when 13 is put in we also want 1 to have
当输入 13 时，我们也希望 1 具有
a high probability
较高的概率；
when one is input we want zero to have a
当输入 1 时，我们希望 0 具有
very high probability so there are five
非常高的概率。因此，在这个数据集中，
separate input examples to a neural nut
神经网络有五个单独的输入示例。
in this data set
i wanted to add a tangent of a node of
我想添加一个节点的切线。需要注意的是，在使用
caution to be careful with a lot of the
apis of some of these frameworks
一些框架的 API 时要小心。
you saw me silently use torch.tensor
你看到我默默地使用 torch.tensor
with a lowercase t
和小写的 t
and the output looked right
，输出看起来正确。
but you should be aware that there's
但你应该知道，
actually two ways of constructing a
实际上有两种构造
tensor there's a torch.lowercase tensor
张量的方法。一个是 torch.lowercase 张量，
and there's also a torch.capital tensor
另一个是 torch.capital 张量
class which you can also construct
类，你也可以构造它们。
so you can actually call both you can
所以你实际上可以同时调用它们。你
also do torch.capital tensor
也可以使用 torch.capital 张量，
and you get a nexus and wise as well
这样你就可以得到一个 nexus 和 wise，
so that's not confusing at all
所以这并不令人困惑。
um
嗯，
there are threads on what is the
有很多关于
difference between these two
这两者之间有什么区别的帖子，
and um
嗯，
unfortunately the docs are just like not
不幸的是，文档并没有
clear on the difference and when you
清楚地说明它们的区别。当你
look at the the docs of lower case
查看 小写张
tensor construct tensor with no autograd
量的文档通过复制数据来构造没有自动求导历史的张量，
history by copying data
it's just like it doesn't
这就像没有一样，
it doesn't make sense so the actual
没有意义，所以
difference as far as i can tell is
据我所知，实际的区别
explained eventually in this random
最终在这个随机线程中得到了解释，
thread that you can google
你可以谷歌一下
and really it comes down to
，实际上它归结为
i believe
我相信
that um
嗯
what is this
这个 torch.tensor 是什么，
torch.tensor in first d-type the data
首先 d 类型
type automatically while torch.tensor
自动输入数据类型，而 torch.tensor
just returns a float tensor
只返回一个浮点张量，
i would recommend stick to
我建议坚持使用
torch.lowercase tensor
torch.lowercase 张量，
so um
所以嗯，
indeed we see that when i
确实我们看到当我
construct this with a capital t the data
用大写 t 构造它时，
type here of xs is float32
这里的 xs 的数据类型是 float32，
but towards that lowercase tensor
但是对于那个小写张量，
you see how it's now x dot d type is now
你会看到它现在是 x 点 d 类型现在是
integer
整数，
so um
所以嗯，
it's advised that you use lowercase t
建议你使用小写 t，
and you can read more about it if you
如果你愿意，你可以
like in some of these threads but
在这些线程中阅读更多相关内容，但
basically
基本上
um
嗯，
i'm pointing out some of these things
我指出其中的一些事情是
because i want to caution you and i want
因为我想提醒你，我希望
you to re get used to reading a lot of
你重新习惯阅读大量的
documentation and reading through a lot
文档和阅读大量的
of
q and a's and threads like this
q 和 像这样的东西，
and
you know some of the stuff is
你知道有些东西很
unfortunately not easy and not very well
不幸并不容易，也没有很好的文档记录，
documented and you have to be careful
你必须小心，
out there what we want here is integers
我们这里想要的是整数，
because that's what makes uh sense
因为这是有意义的，
um
嗯，
and so
所以
lowercase tensor is what we are using
我们正在使用小写张量，
okay now we want to think through how
好的，现在我们想思考如何将
we're going to feed in these examples
这些示例输入
into a neural network
到神经网络中，
now it's not quite as straightforward as
现在它不像
plugging it in because these examples
插入它那么简单，因为这些示例
right now are integers so there's like a
现在都是整数，所以有
0 5 or 13 it gives us the index of the
0、5 或 13，它给我们字符的索引，
character and you can't just plug an
你不能直接将
integer index into a neural net
整数索引插入神经网络，
these neural nets right are sort of made
这些神经网络是由
up of these neurons
这些神经元组成的，
and
these neurons have weights and as you
这些神经元有权重，正如你
saw in micrograd these weights act
在 micrograd 中看到的，这些权重对
multiplicatively on the inputs w x plus
输入 w x 加
b there's 10 h's and so on and so it
b 进行乘法作用，有 10 h 等等，所以让输入
doesn't really make sense to make an
input neuron take on integer values that
神经元接受
you feed in and then multiply on with
你输入的整数值，然后乘以
weights
权重是没有意义的，
so instead
所以
a common way of encoding integers is
一种常见的整数编码方式是
what's called one hot encoding
所谓的独热编码
in one hot encoding
独热编码
we take an integer like 13 and we create
我们取一个像 13 这样的整数，然后创建
a vector that is all zeros except for
一个除了第 13 维之外全为零的向量，
the 13th dimension which we turn to a
我们将第 13 维变为
one and then that vector can feed into a
1，然后该向量可以方便地输入到
neural net
神经网络中，
now conveniently
uh pi torch actually has something
嗯，pi torch 实际上在
called the one hot
function inside torching and functional
torching 和 functional 中有一个叫做独热函数的东西，
it takes a tensor made up of integers
它需要一个由整数组成的张量，
um
嗯，
long is a is a as an integer
长是一个整数，
um
嗯，
and it also takes a number of classes um
它还需要一些类，嗯，也
which is how large you want your uh
就是你想要的
tensor uh your vector to be
张量，嗯，你的向量有多大，
so here let's import
所以在这里让我们导入
torch.n.functional sf this is a common
torch.n.functionsf，这是一种常见的
way of importing it
导入方式，
and then let's do f.1 hot
然后让我们做 f.1 hot，
and we feed in the integers that we want
我们输入我们想要
to encode so we can actually feed in the
编码的整数，这样我们实际上可以输入
entire array of x's
整个 x 数组，
and we can tell it that num classes is
我们可以告诉它类数是
27.
27。
so it doesn't have to try to guess it it
所以它不必尝试猜测，它
may have guessed that it's only 13 and
可能已经猜到只有 13，并且
would give us an incorrect result
会给我们一个错误的结果，
so this is the one hot let's call this x
所以这是一个独热函数，我们称之为 x
inc for x encoded
inc，用于 x 编码
and then we see that x encoded that
然后我们看到 x 编码后的
shape is 5 by 27
形状是 5 x 27，
and uh we can also visualize it plt.i am
嗯，我们也可以将其可视化 plt。我
show of x inc
展示了 x inc，以便
to make it a little bit more clear
更清晰地显示，
because this is a little messy
因为这有点混乱，
so we see that we've encoded all the
所以我们看到我们已将所有
five examples uh into vectors we have
五个示例编码为向量，我们有
five examples so we have five rows and
五个示例，所以我们有五行，
each row here is now an example into a
这里的每一行现在都是一个
neural nut
神经网络螺母中的示例，
and we see that the appropriate bit is
我们看到相应的位被
turned on as a one and everything else
设置为 1，其他所有位都设置为
is zero
0，
so um
所以嗯，
here for example the zeroth bit is
例如，这里第 0 位
turned on the fifth bit is turned on
设置为 1，第 5 位设置为 1，
13th bits are turned on for both of
对于这两个示例，第 13 位都设置为 1，
these examples and then the first bit
然后这里的第一位设置为
here is turned on
1，这
so that's how we can encode
就是我们将
integers into vectors and then these
整数编码为向量的方式，然后这些
vectors can feed in to neural nets one
向量可以输入到神经网络中。
more issue to be careful with here by
顺便说一下，这里要注意的另一个问题
the way is
是，
let's look at the data type of encoding
让我们看一下编码的数据类型，
we always want to be careful with data
我们总是要小心处理数据
types
类型，
what would you expect x encoding's data
type to be
when we're plugging numbers into neural
当我们将数字输入神经网络
nuts we don't want them to be integers
螺母时，您期望 x 编码的数据类型是什么？我们不希望它们是整数，
we want them to be floating point
我们希望它们是浮点数
numbers that can take on various values
数字可以采用各种值，
but the d type here is actually 64-bit
但这里的 d 类型实际上是 64 位
integer
整数，
and the reason for that i suspect is
我怀疑这是
that one hot received a 64-bit integer
因为 one hot 在这里接收了一个 64 位整数，
here and it returned the same data type
它返回了相同的数据类型，
and when you look at the signature of
当你查看 one hot 的签名时，
one hot it doesn't even take a d type a
它甚至不接受
desired data type of the output tensor
输出张量的所需数据类型，
and so we can't in a lot of functions in
所以我们在 torch 中的很多函数中无法做到这一点，
torch we'd be able to do something like
我们可以执行类似
d type equal storage.float32
d 类型等于 storage.float32 的操作，
which is what we want but one heart does
这是我们想要的，但 One Heart
not support that
不支持它，
so instead we're going to want to cast
所以我们要将
this to float like this
其转换为浮点数，
so that these
这样
everything is the same
everything looks the same but the d-type
所有东西看起来都一样，但 d 类型
is float32 and floats can feed into
是 float32，浮点数可以输入
neural nets so now let's construct our
神经网络，所以现在让我们构建
first neuron
第一个神经元，
this neuron will look at these input
这个神经元将查看这些输入向量，
vectors
and as you remember from micrograd these
正如你在 micrograd 中记得的那样，这些
neurons basically perform a very simple
神经元基本上执行一个非常简单的
function w x plus b where w x is a dot
函数 w x 加 b，其中 w x 是点
product
积，
right
对吧，
so we can achieve the same thing here
所以我们可以在这里实现同样的事情，
let's first define the weights of this
让我们首先定义这个
neuron basically what are the initial
神经元的权重，基本上初始
weights at initialization for this
初始化此神经元的权重，
neuron
let's initialize them with torch.rendin
让我们用 torch.rendin 来初始化它们。torch.rendin
torch.rendin
is um
是用
fills a tensor with random numbers
drawn from a normal distribution
从正态分布中抽取的随机数填充张量，
and a normal distribution
而正态分布
has
具有
a probability density function like this
这样的概率密度函数，
and so most of the numbers drawn from
因此从该分布中抽取的大多数数字
this distribution will be around 0
将在 0 左右，
but some of them will be as high as
但其中一些数字将
almost three and so on and very few
高达 3 左右，以此类推，很少有
numbers will be above three in magnitude
数字的大小会超过 3，
so we need to take a size as an input
因此我们需要在此处将大小作为输入，
here
and i'm going to use size as to be 27 by
我将使用 27 乘以 1，
one
so
因此
27 by one and then let's visualize w so
27 乘以 1，然后让我们将 w 可视化，因此
w is a column vector of 27 numbers
w 是 27 个数字的列向量，
and
these weights are then multiplied by the
然后将这些权重乘以输入，
inputs
so now to perform this multiplication we
因此现在要执行此乘法，我们
can take x encoding and we can multiply
可以采用 x 编码，然后将
it with w
其与 w 相乘，
this is a matrix multiplication operator
这是 pi torch 中的矩阵乘法运算符，
in pi torch
and the output of this operation is five
此运算的输出是 5
by one
乘以 1，
the reason is five by five is the
原因是 5 乘以 5，
following
因此
we took x encoding which is five by
我们采用 x 编码，即 5 乘以
twenty seven and we multiplied it by
27，然后将其乘以
twenty seven by one
27 乘以 1，
and
in matrix multiplication
在矩阵乘法中，
you see that the output will become five
您会看到输出将变为 5
by one because these 27
乘以 1 因为这 27 个神经元
will multiply and add
会相乘和相加，
so basically what we're seeing here outs
所以基本上我们在这里看到的，
out of this operation
从这个操作中得到的结果
is we are seeing the five
是，我们看到
activations
of this neuron
这个神经元
on these five inputs
在五个输入上的五次激活，
and we've evaluated all of them in
并且我们并行地评估了它们，
parallel we didn't feed in just a single
我们没有只给
input to the single neuron we fed in
单个神经元输入一个输入，而是
simultaneously all the five inputs into
同时将所有五个输入输入到
the same neuron
同一个神经元中，
and in parallel patrol has evaluated
并行地，patrol 评估了
the wx plus b but here is just the wx
wx 加 b，但这里只是 wx，
there's no bias
没有偏差，
it has value w times x for all of them
它对所有输入都独立地具有 w 乘以 x 的值，而
independently now instead of a single
不是单个
neuron though i would like to have 27
神经元，虽然我想要 27 个
neurons and i'll show you in a second
神经元，我稍后会向你们展示
why i want 27 neurons
为什么我想要 27 个神经元，
so instead of having just a 1 here which
所以这里不是只有一个 1 来
is indicating this presence of one
表示
single neuron
单个神经元的存在，
we can use 27
我们可以使用 27，
and then when w is 27 by 27
然后当 w 为 27 乘以 27 时，
this will in parallel evaluate all the
这将并行评估所有
27 neurons on all the 5 inputs
5 个输入上的所有 27 个神经元，从而
giving us a much better much much bigger
得到一个更好更大的
result so now what we've done is 5 by 27
结果，所以现在我们所做的是 5 乘以 27 乘以
multiplied 27 by 27
27 乘以 27，
and the output of this is now 5 by 27
现在的输出是 5 x 27，
so we can see that the shape of this
所以我们可以看到它的形状
is 5 by 27.
是 5 x 27。
so what is every element here telling us
那么这里的每个元素告诉我们什么呢
right
？
it's telling us for every one of 27
它告诉我们，对于
neurons that we created
我们创建的 27 个神经元中的每一个，
what is the firing rate of those neurons
这些神经元在这五个示例中的发放率是多少。
on every one of those five examples
so
the element for example 3 comma 13
例如，元素 3 逗号 13
is giving us the firing rate of the 13th
给出了第 13 个
neuron
神经元
looking at the third input
在第三个输入时的发放率，
and the way this was achieved is by a
而实现这一目标的方法是通过对
dot product
between the third
第三个
input
输入
and the 13th column
和
of this w matrix here
这里 w 矩阵的第 13 列进行点积计算，
okay
好的，
so
因此
using matrix multiplication we can very
使用矩阵乘法，我们可以非常
efficiently evaluate
有效地评估
the dot product between lots of input
examples in a batch
一批中的大量输入示例和
and lots of neurons where all those
大量神经元之间的点积，所有这些
neurons have weights in the columns of
神经元在这些 w 的列中都有权重，
those w's
and in matrix multiplication we're just
在矩阵乘法中，我们只是
doing those dot products and
in parallel just to show you that this
并行地执行这些点积，只是为了向你展示这种
is the case we can take x and we can
情况，我们可以取 x，我们可以
take the third
取第三
row
行，
and we can take the w and take its 13th
我们可以取 w 并取其第 13
column
列，
and then we can do
然后我们可以对
x and get three
x 进行三次
elementwise multiply with w at 13.
元素乘法，将 w 乘以 13。
and sum that up that's wx plus b
然后将其相加 这是 wx 加 b，
well there's no plus b it's just wx dot
嗯，没有加 b，只是 wx 点积，
product
and that's
这就是
this number
这个数字，
so you see that this is just being done
所以你看，这只是通过
efficiently by the matrix multiplication
operation
for all the input examples and for all
对所有输入示例和
the output neurons of this first layer
第一层的所有输出神经元进行矩阵乘法运算而高效完成的，
okay so we fed our 27-dimensional inputs
好，所以我们将 27 维输入馈送
into a first layer of a neural net that
到
has 27 neurons right so we have 27
具有 27 个神经元的神经网络的第一层，所以我们有 27 个
inputs and now we have 27 neurons these
输入，现在我们有 27 个神经元，这些
neurons perform w times x they don't
神经元执行 w 乘以 x，它们
have a bias and they don't have a
没有偏差，也没有
non-linearity like 10 h we're going to
像 10 h 这样的非线性，我们将将
leave them to be a linear layer
它们保留为线性层，
in addition to that we're not going to
除此之外，我们不会再
have any other layers this is going to
有任何其他层，这将
be it it's just going to be
是最
the dumbest smallest simplest neural net
愚蠢的最小最简单的神经网络，它
which is just a single linear layer
只是一个线性层，
and now i'd like to explain what i want
现在我想解释一下我希望
those 27 outputs to be
这 27 个输出
intuitively what we're trying to produce
直观地显示我们试图
here for every single input example is
为每个输入示例生成的结果是，
we're trying to produce some kind of a
我们试图为序列中的下一个字符生成某种
probability distribution for the next
概率分布，
character in a sequence
and there's 27 of them
并且 27 个，
but we have to come up with like precise
但我们必须想出精确的
semantics for exactly how we're going to
语义，来
interpret these 27 numbers that these
解释这些
neurons take on
神经元
now intuitively
现在所接受的这 27 个数字。直观地
you see here that these numbers are
看，你可以看到这些数字有
negative and some of them are positive
负数，有正数
etc
等等，
and that's because these are coming out
这是因为它们
of a neural net layer initialized with
来自一个用正态分布参数初始化的神经网络层。
these
normal distribution
parameters
but what we want is we want something
但我们想要的是
like we had here
like each row here
像这里的每一行都
told us the counts and then we
告诉我们计数，然后我们对
normalized the counts to get
计数进行归一化以获得
probabilities and we want something
概率，我们希望
similar to come out of the neural net
从神经网络中得到类似的结果。
but what we just have right now is just
我们现在得到的只是
some negative and positive numbers
一些负数和正数，
now we want those numbers to somehow
现在我们希望这些数字以某种方式
represent the probabilities for the next
表示下一个字符的概率，
character
but you see that probabilities they they
但你会看到，概率
have a special structure they um
具有特殊的结构，它们是
they're positive numbers and they sum to
正数，它们的总和为
one
1，
and so that doesn't just come out of a
所以它们不能直接来自
neural net
神经网络，也
and then they can't be counts
不能是计数，
because these counts are positive and
因为这些计数是正数，而
counts are integers
计数是整数，
so counts are also not really a good
所以计数
thing to output from a neural net
对于神经网络的输出来说也不是什么好事。
so instead what the neural net is going
因此，神经网络将
to output and how we are going to
输出什么，以及我们将如何
interpret the um
解释这些
the 27 numbers is that these 27 numbers
这 27 个数字
are giving us log counts
basically
基本上就是对数计数，
um
so instead of giving us counts directly
所以它们不是像表格中那样直接提供计数，而是对数计数，
like in this table they're giving us log
counts
and to get the counts we're going to
为了得到计数，我们要
take the log counts and we're going to
取对数计数，然后对它们进行幂运算，幂
exponentiate them
now
exponentiation
运算的
takes the following form
形式如下，
it takes numbers
取
that are negative or they are positive
正数或负数，
it takes the entire real line
取整条实数轴，
and then if you plug in negative numbers
然后如果代入负数，就会
you're going to get e to the x
得到 e 到 x 的幂，这个数
which is uh always below one
总是小于 1，
so you're getting numbers lower than one
所以你会得到小于 1 的数，
and if you plug in numbers greater than
如果代入大于 0 的数，就会得到大于 1 的数，
zero you're getting numbers greater than
one all the way growing to the infinity
一直增长到无穷大，
and this here grows to zero
这里会增长到 0，
so basically we're going to
所以基本上我们要
take these numbers
取这些数字，它们不是
here
and
instead of them being positive and
正数或
negative and all over the place we're
负数，而是到处都是，我们
going to interpret them as log counts
要把它们解释为对数计数，
and then we're going to element wise
然后我们要对
exponentiate these numbers
这些数字进行元素幂运算，对
exponentiating them now gives us
它们进行幂运算，现在我们得到
something like this
这样的结果，
and you see that these numbers now
你会看到，这些数字现在
because they went through an exponent
因为它们经过了幂运算，
all the negative numbers turned into
所有的负数都变成了
numbers below 1 like 0.338 and all the
小于 1 的数字，比如 0.338，而所有
positive numbers originally turned into
正数最初都会变成
even more positive numbers sort of
大于 1 的更正数，
greater than one
so like for example
例如
seven
7
is some positive number over here
是大于 0 的正数，
that is greater than zero
but exponentiated outputs here
但这里的指数输出
basically give us something that we can
基本上给了我们一些可以
use and interpret as the equivalent of
使用和解释为
counts originally so you see these
最初计数等价的东西，所以你看到这里的
counts here 112 7 51 1 etc
计数是 112 7 51 1 等等，
the neural net is kind of now predicting
神经网络现在正在预测
uh
counts
计数，
and these counts are positive numbers
这些计数是正数，
they can never be below zero so that
它们永远不会低于零，所以这是
makes sense
有道理的，
and uh they can now take on various
它们现在可以根据 w 的设置取各种
values
值，
depending on the settings of w
so let me break this down
所以让我分解一下，
we're going to interpret these to be the
我们将这些解释为对
log counts
数计数，
in other words for this that is often
换句话说，通常
used is so-called logits
使用的是所谓的 logits，
these are logits log counts
这些是 logits 对数计数，
then these will be sort of the counts
那么这些将是
largest exponentiated
计数的最大指数，
and this is equivalent to the n matrix
这相当于我们之前使用的 n 矩阵
sort of the n
排序的 n
array that we used previously remember
数组，记住
this was the n
这是 n，这
this is the the array of counts
是计数数组，
and each row here are the counts for the
这里的每一行都是
for the um
next character sort of
下一个字符的计数，
so those are the counts and now the
所以这些是 计数，现在
probabilities are just the counts um
概率只是经过
normalized
归一化的计数，嗯，
and so um
所以
i'm not going to find the same but
我不会找到相同的，但
basically i'm not going to scroll all
基本上我不会到处滚动，
over the place
we've already done this we want to
我们已经完成了，我们希望
counts that sum
计数
along the first dimension and we want to
沿着第一个维度求和，我们希望
keep them as true
它们保持真实，
we've went over this and this is how we
我们已经讨论过了，这就是我们如何对
normalize the rows of our counts matrix
计数矩阵的行进行归一化以
to get our probabilities
获得概率
props
props，
so now these are the probabilities
所以现在这些是概率，
and
these are the counts that we ask
这些是我们当前要求的计数，
currently and now when i show the
现在当我显示概率时，
probabilities
you see that um
您会看到，嗯，
every row here
这里的每一行
of course
当然
will sum to 1
都会加起来为 1，
because they're normalized
因为它们是归一化的，
and the shape of this
并且形状
is 5 by 27
是 5 乘 27，
and so really what we've achieved is for
所以我们实际上实现的是，对于
every one of our five examples
我们的五个示例中的每一个，
we now have a row that came out of a
我们现在都有一行来自
neural net
神经网络
and because of the transformations here
，由于这里的转换，
we made sure that this output of this
我们确保这个
neural net now are probabilities or we
神经网络的输出现在是概率，或者我们
can interpret to be probabilities
可以解释为概率，
so
所以
our wx here gave us logits
我们这里的 wx 给了我们对数，
and then we interpret those to be log
然后我们将它们解释为对数计数，
counts
we exponentiate to get something that
我们对其进行指数运算以得到
looks like counts
如下所示的结果 计数，
and then we normalize those counts to
然后我们对这些计数进行归一化以
get a probability distribution
获得概率分布，
and all of these are differentiable
所有这些都是可微分
operations
运算，
so what we've done now is we're taking
所以我们现在所做的就是获取
inputs we have differentiable operations
输入，我们可以通过可微分运算进行
that we can back propagate through
反向传播，
and we're getting out probability
然后我们得到概率
distributions
分布，
so
for example for the zeroth example that
例如，对于输入的第零个示例，
fed in
right which was um
嗯，
the zeroth example here was a one-half
这里的第零个示例是一个零的二分之一
vector of zero
向量，
and um
嗯，
it basically corresponded to feeding in
它基本上对应于输入
this example here so we're feeding in a
这个示例，所以我们将一个
dot into a neural net and the way we fed
点输入到神经网络中，我们将
the dot into a neural net is that we
点输入神经网络的方式是，我们
first got its index
首先获得它的索引，
then we one hot encoded it
然后我们对它进行独热编码，
then it went into the neural net and out
然后它进入神经网络，得到
came
this distribution of probabilities
这个概率分布，
and its shape
它的形状
is 27 there's 27 numbers and we're going
是 27，有 27 个数字，我们将把
to interpret this as the neural nets
它解释为神经网络对
assignment for how likely every one of
these characters um
这些字符中的每一个字符的分配，嗯，这
the 27 characters are to come next
27 个字符接下来出现的可能性
and as we tune the weights w
，当我们调整权重时，我们
we're going to be of course getting
当然会得到
different probabilities out for any
character that you input
你输入的任何字符的不同概率，
and so now the question is just can we
所以现在的问题是，我们能否
optimize and find a good w
优化并找到 一个好的 w，
such that the probabilities coming out
这样得到的概率就
are pretty good and the way we measure
相当不错了，我们衡量“
pretty good is by the loss function okay
相当好”的方法是通过损失函数，好的，
so i organized everything into a single
所以我把所有内容组织成一个
summary so that hopefully it's a bit
摘要，希望它
more clear so it starts here
更清晰一些，它
with an input data set
从输入数据集开始，
we have some inputs to the neural net
我们有一些输入到神经网络的输入，
and we have some labels for the correct
我们有一些标签，用于表示
next character in a sequence these are
序列中正确的下一个字符，这些是
integers
整数，
here i'm using uh torch generators now
我现在使用 torch 生成器，
so that you see the same numbers that i
这样你看到的数字
see
and i'm generating um
和我看到的一样，我生成了
27 neurons weights
27 个神经元的权重，
and each neuron here receives 27 inputs
每个神经元接收 27 个输入，
then here we're going to plug in all the
然后我们将所有
input examples x's into a neural net so
输入示例 x 代入神经网络，所以
here this is a forward pass
这是一个前向传播，
first we have to encode all of the
首先我们必须将所有
inputs into one hot representations
输入编码成独热表示，
so we have 27 classes we pass in these
所以我们有 27 个类，我们传入这些
integers and
整数，
x inc becomes a array that is 5 by 27
x inc 变成一个 5 乘以 27 个
zeros except for a few ones
零的数组，除了少数 1，
we then multiply this in the first layer
然后我们在神经网络的第一层将其相乘以获得对数，对对数取
of a neural net to get logits
exponentiate the logits to get fake
幂以获得假
counts sort of
计数，
and normalize these counts to get
并对这些计数进行归一化以获得
probabilities
概率，
so we lock these last two lines by the
所以我们锁定最后两行，
way here are called the softmax
顺便说一下，这两行叫做softmax，
which i pulled up here soft max is a
我在这里提到softmax是
very often used layer in a neural net
神经网络中非常常用的层，它
that takes these z's which are logics
接受这些z，也就是逻辑值，对
exponentiates them
它们进行指数运算、除法和
and divides and normalizes it's a way of
归一化，这是一种
taking
获取
outputs of a neural net layer and these
神经网络层输出的方法，
these outputs can be positive or
这些输出可以是正数或
negative
负数，
and it outputs probability distributions
它输出概率分布，
it outputs something that is always
它输出一个总是和
sums to one and are positive numbers
1相加的数值，并且是正数，
just like probabilities
就像概率一样，
um so it's kind of like a normalization
嗯，所以它有点像一个归一化
function if you want to think of it that
函数，如果你想这样想的话，
way and you can put it on top of any
你可以把它放在
other linear layer inside a neural net
神经网络中任何其他线性层的顶部，
and it basically makes a neural net
它基本上就是一个神经网络
output probabilities that's very often
输出概率，这是非常
used and we used it as well here
常用的，我们在这里也用到了它，
so this is the forward pass and that's
所以这是正向传递，这就是
how we made a neural net output
我们制作神经网络输出概率的方法，
probability
now
现在
you'll notice that
你会注意到，
um
嗯，
all of these
this entire forward pass is made up of
整个正向传递都是由
differentiable
可微
layers everything here we can back
分层组成的，这里的所有东西我们都可以反向
propagate through and we saw some of the
传播，我们
back propagation in micrograd
在微梯度中看到了一些反向传播，
this is just
这只是
multiplication and addition all that's
乘法和加法，
happening here is just multiply and then
这里发生的一切只是 先乘后
add and we know how to backpropagate
加，我们知道如何反向传播
through them
它们。
exponentiation we know how to
指数运算我们知道如何
backpropagate through
反向传播。
and then here we are summing
然后我们在这里求和，求和也很
and sum is is easily backpropagable as
容易反向传播，
well
and division as well so everything here
除法也很容易反向传播。所以这里的一切都是可
is differentiable operation
微分运算，
and we can back propagate through
我们可以反向传播。
now we achieve these probabilities which
现在我们得到了这些概率，
are 5 by 27
for every single example we have a
对于每个例子，它们是 5 乘以 27。我们有一个
vector of probabilities that's into one
概率向量，它们合二为一。
and then here i wrote a bunch of stuff
然后我在这里写了一堆东西来分解
to sort of like break down uh the
examples
例子。
so we have five examples making up emma
我们有五个例子组成了 emma，emma
right
and there are five bigrams inside emma
里面有五个二元模型。
so bigram example a bigram example1 is
二元模型的例子是，
that e is the beginning character right
e 是点后面的起始字符，
after dot
and the indexes for these are zero and
它们的索引是 0 和
five
5。
so then we feed in a zero
所以我们输入一个 0，这
that's the input of the neural net
是神经网络的输入。
we get probabilities from the neural net
我们从神经网络中得到
that are 27 numbers
27 个数字的概率，
and then the label is 5 because e
然后标签是 5，因为 e
actually comes after dot
实际上在点后面，
so that's the label
所以这是标签。
and then
然后
we use this label 5 to index into the
我们使用这个标签 5 来索引
probability distribution here
这里的概率分布，
so
所以
this
index 5 here is 0 1 2 3 4 5. it's this
这里的索引 5 是 0 1 2 3 4 5。这个
number here
数字在这里，
which is here
so that's basically the probability
所以这基本上是
assigned by the neural net to the actual
神经网络分配给实际
correct character
正确字符的概率。
you see that the network currently
你可以看到，网络目前
thinks that this next character that e
认为下一个字符 e 跟在
following dot is only one percent likely
点后面的可能性只有 1%，
which is of course not very good right
这当然不是很好，
because this actually is a training
因为这实际上是一个训练
example and the network thinks this is
示例，网络认为这
currently very very unlikely but that's
目前非常不可能，但这
just because we didn't get very lucky in
只是因为我们在
generating a good setting of w so right
生成 w 的良好设置时运气不佳，所以
now this network things it says unlikely
现在这个网络认为不太可能，
and 0.01 is not a good outcome
0.01 不是一个好结果，
so the log likelihood then is very
所以对数似然非常负，负对数似然非常正，
negative
and the negative log likelihood is very
positive
and so four is a very high negative log
所以 4 是一个非常高的负对数似然，
likelihood and that means we're going to
这意味着我们将
have a high loss
有很高的损失，
because what is the loss the loss is
因为损失
just the average negative log likelihood
只是平均负对数似然，
so the second character is em
所以第二个字符是 em，
and you see here that also the network
你在这里看到，网络也
thought that m following e is very
认为 m 跟在 e 后面的可能性非常小，
unlikely one percent
1%，
the for m following m i thought it was
对于 m 跟在 m 后面，我认为是
two percent
2%，
and for a following m it actually
而对于 a 跟在 m 后面，它实际上
thought it was seven percent likely so
认为是 7 概率百分比，所以
just by chance this one actually has a
偶然情况下，这个
pretty good probability and therefore
概率实际上相当高，因此
pretty low negative log likelihood
负对数似然相当低，
and finally here it thought this was one
最后这里它认为这个概率是 1
percent likely
%，
so overall our average negative log
所以总体而言，我们的平均负对数似然，也
likelihood which is the loss the total
就是损失，总
loss that summarizes
损失，基本上总结了
basically the how well this network
这个网络
currently works at least on this one
目前至少在这个
word not on the full data suggested one
单词上（而不是在完整数据上）的运行情况。建议一个
word is 3.76 which is actually very
单词的损失是 3.76，这实际上是
fairly high loss this is not a very good
相当高的损失，这不是一个非常好的
setting of w's
w 设置。
now here's what we can do
现在我们可以这样做，
we're currently getting 3.76
我们目前得到 3.76，
we can actually come here and we can
我们实际上可以来这里，我们可以
change our w we can resample it so let
改变我们的 w，我们可以重新采样，所以
me just add one to have a different seed
我只需添加一个种子，
and then we get a different w
然后我们得到一个不同的 w，
and then we can rerun this
然后我们可以重新运行这个程序，
and with this different c with this
使用这个不同的 c，使用这个
different setting of w's we now get 3.37
不同的 w 设置，我们现在得到 3.37，
so this is a much better w right and
所以这是一个更好的 w，对
that and it's better because the
吧，这样更好，因为对于
probabilities just happen to come out
higher for the for the characters that
actually are next
接下来的字符来说，概率恰好更高，
and so you can imagine actually just
所以你可以想象实际上只是重新
resampling this you know we can try two
采样，我们知道我们可以尝试两次，
so
所以
okay this was not very good
好吧，这不是很好，
let's try one more
让我们再试一次，
we can try three
我们可以尝试三次，
okay this was terrible setting because
好的 设定很糟糕，因为
we have a very high loss
损失非常高，
so anyway i'm going to erase this
所以无论如何我都要删除它，
what i'm doing here which is just guess
我在这里做的只是猜测
and check of randomly assigning
和检查随机分配的
parameters and seeing if the network is
参数，看看网络是否
good that is uh amateur hour that's not
良好，这只是业余时间，这不是
how you optimize a neural net the way
优化神经网络的方法，
you optimize your neural net is you
优化神经网络的方法是
start with some random guess and we're
从一些随机猜测开始，我们
going to commit to this one even though
将致力于这个，即使
it's not very good
它不是很好，
but now the big deal is we have a loss
但现在最重要的是我们有一个损失
function
函数，
so this loss
所以这个损失
is made up only of differentiable
仅由可微分
operations and we can minimize the loss
运算组成，我们可以通过
by tuning
ws
by computing the gradients of the loss
计算损失
with respect to
关于
these w matrices
这些 w 矩阵的梯度来调整 w 来最小化损失，
and so then we can tune w to minimize
然后我们可以调整 w 以最小化
the loss and find a good setting of w
损失，并使用基于梯度的优化找到 w 的良好设置，
using gradient based optimization so
let's see how that will work now things
让我们看看它将如何工作，现在
are actually going to look almost
看起来几乎与
identical to what we had with micrograd
我们使用 micrograd 时相同，
so here
所以
i pulled up the lecture from micrograd
我从 micrograd 的笔记本中取出了讲座，
the notebook it's from this repository
它来自这个存储库，
and when i scroll all the way to the end
当我一直滚动到我们
where we left off with micrograd we had
离开 micrograd 的末尾时，我们得到了
something very very similar
非常非常相似的东西，
we had
我们有
a number of input examples in this case
许多输入示例，在这种情况下，
we had four input examples inside axis
我们有四个输入示例 在轴内，
and we had their targets these are
我们有它们的目标，这些是
targets
目标，
just like here we have our axes now but
就像这里一样，我们现在有轴，但
we have five of them and they're now
我们有五个，它们现在是
integers instead of vectors
整数而不是向量，
but we're going to convert our integers
但我们将把整数转换
to vectors except our vectors will be 27
为向量，除了我们的向量将是 27 个
large instead of three large
大向量，而不是 3 个大向量，
and then here what we did is first we
然后在这里我们所做的是，首先我们
did a forward pass where we ran a neural
进行了前向传递，我们
net on all of the inputs
在所有输入上运行神经网络来
to get predictions
获得预测，
our neural net at the time this nfx was
当时
a multi-layer perceptron
our neural net is going to look
我们的神经网络是多层感知器，我们的
different because our neural net is just
神经网络看起来会有所不同，因为我们的神经网络只是
a single layer
单层
single linear layer followed by a soft
单线性层，后跟一个软
max
最大值，
so that's our neural net
所以这就是我们的神经网络，
and the loss here was the mean squared
这里的损失是均方
error so we simply subtracted the
误差，所以我们只需从
prediction from the ground truth and
基本事实中减去预测，然后求
squared it and summed it all up and that
平方，再将其相加，这
was the loss and loss was the single
就是损失，损失是
number that summarized the quality of
总结
the neural net and when loss is low like
神经网络质量的单个数字，当损失低到
almost zero that means the neural net is
几乎为零时，这意味着神经网络
predicting correctly
预测正确，
so we had a single number that uh that
所以我们有一个
summarized the uh the performance of the
总结
neural net and everything here was
神经网络性能的单个数字，这里的所有内容都是可
differentiable and was stored in massive
微的，并存储在海量
compute graph
计算中 图形，
and then we iterated over all the
然后我们迭代所有
parameters we made sure that the
参数，确保
gradients are set to zero and we called
梯度设置为零，我们调用
lost up backward
loss向后传播，并
and lasted backward initiated back
持续向后传播，
propagation at the final output node of
在loss的最终输出节点启动反向传播，
loss
right so
对吧，所以
yeah remember these expressions we had
记住这些表达式，我们一直都有
loss all the way at the end we start
损失，最后我们开始
back propagation and we went all the way
反向传播，然后一路
back
向后，
and we made sure that we populated all
确保填充了所有
the parameters dot grad
参数点梯度，
so that graph started at zero but back
这样图形从零开始，但反向
propagation filled it in
传播会将其填充，
and then in the update we iterated over
然后在更新中，我们迭代
all the parameters and we simply did a
所有参数，我们只是进行了
parameter update where every single
参数更新，其中
element of our parameters was nudged in
参数的每个元素都朝着
the opposite direction of the gradient
梯度的相反方向移动，
and so we're going to do the exact same
所以我们将在这里做完全相同的
thing here
事情，
so i'm going to pull this up
所以我将把它拉到一边，
on the side here
so that we have it available and we're
以便我们可以使用它，我们
actually going to do the exact same
实际上将做完全相同的
thing so this was the forward pass so
事情，所以这是前向传播，所以
where we did this
我们在哪里这样做，
and probs is our wipe red so now we have
probs是我们的擦除红色，所以现在我们必须
to evaluate the loss but we're not using
评估损失，但我们不使用
the mean squared error we're using the
均方误差，而是使用
negative log likelihood because we are
负对数似然，因为我们
doing classification we're not doing
在进行分类，而不是进行
regression as it's called
回归，因为它在
so here we want to calculate loss
这里 我们现在要计算损失，
now the way we calculate it is it's just
计算方法是
this average negative log likelihood
平均负对数似然，
now this probs here
这里的概率
has a shape of 5 by 27
形状是 5 乘以 27，
and so to get all the we basically want
因此，为了得到所有这些，我们基本上想要在
to pluck out the probabilities at the
correct indices here
这里正确的索引处提取概率，
so in particular because the labels are
特别是因为标签是以
stored here in array wise
数组形式存储的，
basically what we're after is for the
基本上，我们想要的是，对于
first example we're looking at
第一个例子，我们查看
probability of five right at index five
索引 5 处 5 的概率，
for the second example
对于第二个例子，
at the the second row or row index one
在第二行或行索引 1 处，我们
we are interested in the probability
感兴趣的是
assigned to index 13.
分配给索引 13 的概率。
at the second example we also have 13.
在第二个例子中，我们也有 13。
at the third row we want one
在第三行，我们想要 1，
and then the last row which is four we
然后在最后一行，也就是 4，我们
want zero so these are the probabilities
想要 0，所以这些就是
we're interested in right
我们感兴趣的概率，对吧，
and you can see that they're not amazing
你可以看到它们并不像
as we saw above
我们上面看到的那么神奇，
so these are the probabilities we want
所以这些是我们想要的概率，
but we want like a more efficient way to
但我们希望有一种更有效的方法来
access these probabilities
访问这些概率，而
not just listing them out in a tuple
不仅仅是像这样将它们列在一个元组中，
like this so it turns out that the way
所以事实证明，
to do this in pytorch uh one of the ways
在 PyTorch 中执行此操作的方法，嗯，至少其中一种方法
at least is we can basically pass in
是，我们基本上可以传入
all of these
所有 这些，很
sorry about that all of these um
抱歉，所有这些 um
integers in the vectors
整数都在向量中，
so
所以
the
these ones you see how they're just 0 1
这些，你看，它们只是 0 1
2 3 4
2 3 4，
we can actually create that using mp
我们实际上可以使用 mp 而
not mp sorry torch dot range of 5
不是 mp 来创建，抱歉，torch 点范围为 5
0 1 2 3 4.
0 1 2 3 4。
so we can index here with torch.range of
所以我们可以在这里用 torch.range 5 进行索引，
5
and here we index with ys
这里我们用 ys 进行索引，
and you see that that gives us
你看，这给了我们
exactly these numbers
这些数字，
so that plucks out the probabilities of
这样就提取了
that the neural network assigns to the
神经网络分配给
correct next character
正确的下一个字符的概率，
now we take those probabilities and we
现在我们取这些概率，
don't we actually look at the log
我们实际上不看对数
probability so we want to dot log
概率，所以我们想要点对数，
and then we want to just
然后我们想要取
average that up so take the mean of all
平均值，所以取所有这些的平均值，
of that
and then it's the negative
然后它的负平均对数似然
average log likelihood that is the loss
就是损失，
so the loss here is 3.7 something and
所以这里的损失是 3.7 左右，
you see that this loss 3.76 3.76 is
你看这个损失 3.76 3.76 和
exactly as we've obtained before but
我们之前得到的完全一样，但这
this is a vectorized form of that
是该表达式的矢量化形式，
expression
so
所以
we get the same loss
我们得到相同的损失
and the same loss we can consider
，相同的损失我们可以考虑
service part of this forward pass
服务于这个前向传递的一部分，
and we've achieved here now loss
现在我们已经实现了损失函数，
okay so we made our way all the way to
好的，所以我们一路走到了
loss we've defined the forward pass
损失函数，我们已经定义了前向传播，
we forwarded the network and the loss
我们转发了网络和损失函数，
now we're ready to do the backward pass
现在我们准备进行反向传播，
so backward pass
所以在反向传播中，
we want to first make sure that all the
我们首先要确保所有
gradients are reset so they're at zero
梯度都已重置，因此它们
now in pytorch you can set the gradients
现在为零，在 pytorch 中，您可以将梯度设置
to be zero but you can also just set it
为零，但您也可以将其设置
to none and setting it to none is more
为 none，将其设置为 none
efficient and pi torch will interpret
效率更高，并且 pi torch 会将
none as like a lack of a gradient and is
none 解释为缺少梯度，并且
the same as zeros
与零相同，
so this is a way to set to zero the
因此这是一种将梯度设置为零的方法，
gradient
and now we do lost it backward
现在我们在向后丢失了它，在
before we do lost that backward we need
向后丢失之前，我们还需要
one more thing if you remember from
一件事，如果您还记得
micrograd
micrograd
pytorch actually requires
pytorch 实际上要求
that we pass in requires grad is true
我们传入 require grad 为 true，
so that when we tell
以便当我们告诉
pythorge that we are interested in
pythorge 我们有兴趣
calculating gradients for this leaf
计算这个叶张
tensor by default this is false
量的梯度时，默认情况下这是 false，
so let me recalculate with that
所以让我用它重新计算，
and then set to none and lost that
然后将其设置为 none，然后向后丢失了它，
backward
now something magical happened when
现在当持续向后运行时发生了一些神奇的事情，
lasted backward was run
because pytorch just like micrograd when
因为 pytorch 就像 micrograd 一样，当
we did the forward pass here
我们在这里进行前向传播时
it keeps track of all the operations
它会跟踪 所有
under the hood it builds a full
底层操作都会构建一个完整的
computational graph just like the graphs
计算图，就像
we've
我们
produced in micrograd those graphs exist
在 micrograd 中生成的图一样，这些图存在
inside pi torch
于 pi torch 中，
and so it knows all the dependencies and
因此它知道所有依赖关系和
all the mathematical operations of
所有数学运算，然后当
everything
and when you then calculate the loss
您计算损失时，
we can call a dot backward on it
我们可以对它进行点向后移动，
and that backward then fills in the
然后向后移动会将
gradients of
all the intermediates
所有中间值的梯度填充
all the way back to w's which are the
回 w，w 是
parameters of our neural net so now we
我们神经网络的参数，所以现在我们
can do w grad and we see that it has
可以进行 w grad，我们看到它有
structure there's stuff inside it
结构，里面有内容，
and these gradients
这些梯度
every single element here
这里的每个元素，
so w dot shape is 27 by 27
所以 w 点形状是 27 x 27，
w grad shape is the same 27 by 27
w grad 形状也是相同的 27 x 27，
and every element of w that grad
w grad 的每个元素都
is telling us
告诉我们
the influence of that weight on the loss
权重对损失函数的影响，
function
so for example this number all the way
例如这里的这个数字，
here
if this element the zero zero element of
如果这个元素是 w 的零元素，
w
because the gradient is positive is
因为梯度为正，这
telling us that this has a positive
告诉我们这
influence in the loss slightly nudging
对损失有正向影响，稍微调整
w
w，
slightly taking w 0 0
取 w 0 0
and
并
adding a small h to it
添加一个小的 h
would increase the loss
会略微增加损失，
mildly because this gradient is positive
因为这个梯度是正的，
some of these gradients are also
其中一些梯度是 也是
negative
负数，
so that's telling us about the gradient
所以这告诉我们梯度
information and we can use this gradient
信息，我们可以用这个梯度
information to update the weights of
信息来更新
this neural network so let's now do the
这个神经网络的权重，所以现在让我们进行
update it's going to be very similar to
更新，它会和
what we had in micrograd we need no loop
我们在 micrograd 中做的非常相似，我们不需要循环
over all the parameters because we only
遍历所有参数，因为我们
have one parameter uh tensor and that is
只有一个参数，那就是
w
w，
so we simply do w dot data plus equals
所以我们只需做 w 点数据加等于，
uh the
we can actually copy this almost exactly
我们实际上可以复制这个几乎正好是
negative 0.1 times w dot grad
负 0.1 倍的 w 点梯度，
and that would be the update to the
这将是对张
tensor
量的更新，这样就可以更新张量，
so that updates
the tensor
and
because the tensor is updated we would
因为张量已经更新，我们
expect that now the loss should decrease
预计现在损失应该会减少，
so
所以在
here if i print loss
这里如果我打印损失
that item
那一项
it was 3.76 right
它是 3.76 对，
so we've updated the w here so if i
所以我们更新了这里的 w，所以如果我
recalculate forward pass
重新计算前向传递损失
loss now should be slightly lower so
现在应该会稍微低一些，所以
3.76 goes to
3.76 变成
3.74
3.74，
and then
然后
we can again set to set grad to none and
我们可以再次将梯度设置为无并进行
backward
后向
update
更新，
and now the parameters changed again
现在参数再次改变，
so if we recalculate the forward pass we
所以如果我们重新计算前向传递，我们
expect a lower loss again 3.72
预计损失会再次降低 3.72，
okay and this is again doing the we're
好的，这又在做 我们
now doing gradient descent
现在进行梯度下降，
and when we achieve a low loss that will
当我们实现低损失时，这
mean that the network is assigning high
意味着网络会为正确字符分配高
probabilities to the correctness
概率，
characters okay so i rearranged
好的，所以我重新安排了
everything and i put it all together
所有内容，并从头开始将它们放在一起，
from scratch
so here is where we construct our data
所以这里我们构建了
set of bigrams
二元模型数据集，
you see that we are still iterating only
您会看到我们仍然只在
on the first word emma
第一个单词 emma 上进行迭代，
i'm going to change that in a second i
我稍后会进行更改，我
added a number that counts the number of
添加了一个数字，用于计算
elements in x's so that we explicitly
x 中元素的数量，以便我们明确
see that number of examples is five
看到示例数量为 5，
because currently we're just working
因为目前我们只处理
with emma and there's five backgrounds
emma，那里有 5 个背景，在
there
and here i added a loop of exactly what
这里我添加了一个与之前完全相同的循环，
we had before so we had 10 iterations of
因此我们有 10 次
grainy descent of forward pass backward
颗粒状下降迭代，包括前向传递、后向
pass and an update
传递和更新，
and so running these two cells
因此运行这两个单元
initialization and gradient descent
初始化和梯度下降
gives us some improvement
会使我们的损失函数有所改进，
on
the loss function
but now i want to use all the words
但现在我想使用所有单词，现在
and there's not 5 but 228 000 bigrams
不是 5 个而是 228,000 个二元模型，
now
however this should require no
但是这应该不需要
modification whatsoever everything
任何修改，一切都
should just run because all the code we
应该运行，因为我们编写的所有代码
wrote doesn't care if there's five
都不关心是否有 5 个
migrants or 228 000 bigrams and with
移民或 228,000 个二元模型，
everything we should just work so
所有一切都应该正常工作，所以
you see that this will just run
你可以看到它会运行，
but now we are optimizing over the
但现在我们正在优化
entire training set of all the bigrams
所有二元模型的整个训练集，
and you see now that we are decreasing
你现在看到我们正在
very slightly so actually we can
稍微减少，所以实际上我们
probably afford a larger learning rate
可能可以承受更大的学习率
and probably for even larger learning
，甚至更大的学习
rate
率，
even 50 seems to work on this very very
即使 50 似乎也适用于这个非常
simple example right so let me
简单的例子，对吧，所以让我
re-initialize and let's run 100
重新初始化，让我们运行 100 次
iterations
迭代
see what happens
看看会发生什么，
okay
好的，
we seem to be
我们似乎在
coming up to some pretty good losses
here 2.47
这里得到了一些相当不错的损失 2.47，
let me run 100 more
让我再运行 100 次，
what is the number that we expect by the
我们期望的数字是多少，顺便问一下，
way in the loss we expect to get
在损失方面，我们期望得到与
something around what we had originally
我们最初实际结果差不多的数字，所以
actually
so all the way back if you remember in
一直追溯到如果你还记得在
the beginning of this video when we
这个视频的开头，当我们
optimized uh just by counting
通过计算进行优化时，
our loss was roughly 2.47
我们的损失在平滑后大约是 2.47，
after we had it smoothing
but before smoothing we had roughly 2.45
但在平滑之前，我们的似然损失大约是 2.45，
likelihood
sorry loss
抱歉，
and so that's actually roughly the
这实际上大致是
vicinity of what we expect to achieve
我们期望达到的目标，
but before we achieved it by counting
但在我们通过计算实现它之前，
and here we are achieving the roughly
我们实现了大致
the same result but with gradient based
结果相同，但采用了基于梯度的
optimization
优化，
so we come to about 2.4
所以我们得到的结果大约是 2.4
6 2.45 etc
6 2.45 等等，这是
and that makes sense because
有道理的，因为从
fundamentally we're not taking any
根本上讲，我们没有采用任何
additional information we're still just
其他信息，我们仍然只是
taking in the previous character and
采用前一个字符并
trying to predict the next one but
尝试预测下一个字符，但
instead of doing it explicitly by
counting and normalizing
we are doing it with gradient-based
我们不是通过计数和规范化来明确地做到这一点，而是采用基于梯度的
learning and it just so happens that the
学习来实现的，而且恰好，
explicit approach happens to very well
显式方法恰好可以很好地
optimize the loss function without any
优化损失函数，而
need for a gradient based optimization
无需基于梯度的优化，
because the setup for bigram language
因为二元语言模型的设置
models are is so straightforward that's
so simple we can just afford to estimate
非常简单，我们可以直接估计
those probabilities directly and
这些概率并将
maintain them
它们保存
in a table
在表中，
but the gradient-based approach is
但基于梯度的方法要
significantly more flexible
灵活得多，
so we've actually gained a lot
所以我们实际上收获了很多，
because
因为
what we can do now is
我们现在可以做的是
we can expand this approach and
扩展这种方法并使
complexify the neural net so currently
神经网络复杂化，所以目前
we're just taking a single character and
我们只是将一个字符
feeding into a neural net and the neural
输入到神经网络中，
that's extremely simple but we're about
这非常简单，但我们即将
to iterate on this substantially we're
对此进行大量迭代，我们
going to be taking multiple previous
将采用多个先前的
characters and we're going to be feeding
字符，并将
feeding them into increasingly more
它们输入到越来越
complex neural nets but fundamentally
复杂的神经网络中，但从根本
out the output of the neural net will
上讲，神经网络的输出将
always just be logics
始终 只是逻辑，
and those logits will go through the
这些对数函数会经历
exact same transformation we are going
完全相同的变换，我们将对
to take them through a soft max
它们进行软最大值
calculate the loss function and the
计算损失函数和
negative log likelihood and do gradient
负对数似然，并进行
based optimization and so actually
基于梯度的优化，因此实际上，
as we complexify the neural nets and
当我们使神经网络复杂化并一直
work all the way up to transformers
工作到Transformer时，
none of this will really fundamentally
这些都不会从根本上
change none of this will fundamentally
改变，这些都不会从根本上
change the only thing that will change
改变，唯一会改变的
is
是
the way we do the forward pass where we
我们进行前向传递的方式，即
take in some previous characters and
接收一些先前的字符并
calculate logits for the next character
计算序列中下一个字符的对数函数，
in the sequence that will become more
这将变得更加
complex
复杂，
and uh but we'll use the same machinery
但是我们将使用相同的机制
to optimize it
来优化它，
and um
嗯，我们
it's not obvious how we would have
不清楚如何将
extended
this bigram approach
这种二元模型方法扩展
into the case where there are many more
到
characters at the input because
输入中有更多字符的情况，因为
eventually these tables would get way
最终这些表会变得
too large because there's way too many
太大，因为
combinations of what previous characters
先前字符的组合太多了，
could be
if you only have one previous character
如果只有一个先前字符，
we can just keep everything in a table
我们可以将所有内容保存在一个重要的表中，
that counts but if you have the last 10
但是如果你有最后10个
characters that are input we can't
输入字符，我们
actually keep everything in the table
实际上无法再将所有内容保存在表中，
anymore so this is fundamentally an
所以这从根本上来说是一种
unscalable approach and the neural
不可扩展的方法，而神经
network approach is significantly more
网络方法则具有更高的可
scalable and it's something that
扩展性，这
actually we can improve on over time so
实际上 我们可以随着时间的推移不断改进，所以
that's where we will be digging next i
这就是我们接下来要深入研究的地方。我
wanted to point out two more things
想指出另外两点，
number one
第一，
i want you to notice that
我希望你们注意到，
this
这里的
x ink here
x 向量
this is made up of one hot vectors and
是由一个独热向量组成的，
then those one hot vectors are
然后这些独热向量
multiplied by this w matrix
乘以这个 w 矩阵，
and we think of this as multiple neurons
我们将其视为
being forwarded in a fully connected
以完全连接的方式转发的多个神经元，
manner
but actually what's happening here is
但实际上这里发生的事情是，
that for example
例如，
if you have a one hot vector here that
如果你有一个独热向量，它在
has a one at say the fifth dimension
第五维有一个 1，
then because of the way the matrix
那么由于矩阵
multiplication works
乘法的工作原理，
multiplying that one-half vector with w
将这个二分之一向量与 w 相乘
actually ends up plucking out the fifth
实际上会取出
row of w
w 的第五行，对数对数向量将
log logits would become just the fifth
变成 w 的第五
row of w
行，
and that's because of the way the matrix
这是由于矩阵
multiplication works
乘法的工作原理，
um
嗯，这
so
that's actually what ends up happening
就是最终发生的事情，
so but that's actually exactly what
但这实际上正是
happened before
之前发生的事情，
because remember all the way up here
因为记住，这里我们
we have a bigram we took the first
有一个二元组，我们取第一个
character and then that first character
字符，然后将第一个字符
indexed into a row of this array here
索引到这个数组的一行中，
and that row gave us the probability
这一行给出了
distribution for the next character so
下一个字符的概率分布，所以
the first character was used as a lookup
第一个字符被用作查找
into a
matrix here to get the probability
矩阵，以获得 概率
distribution
分布，
well that's actually exactly what's
嗯，实际上这就是
happening here because we're taking the
这里发生的事情，因为我们取
index we're encoding it as one hot and
索引，将其编码为独热函数，然后乘以
multiplying it by w
w，
so logics literally becomes
所以逻辑实际上变成了
the
the appropriate row of w
w 的相应行，
and that gets just as before
并且像之前一样对其
exponentiated to create the counts
进行指数运算以创建计数，
and then normalized and becomes
然后进行归一化并成为
probability
概率，
so this w here
所以这里的 w
is literally
实际上
the same as this array here
与这里的数组相同，
but w remember is the log counts not the
但记住 w 是对数计数而不是
counts so it's more precise to say that
计数，所以更准确地说，
w exponentiated
w 对 w 进行指数运算，
w dot x is this array
点 x 是这个数组，
but this array was filled in by counting
但这个数组是通过计数
and by
和填充
basically
populating the counts of bi-grams
二元语法计数来填充的，
whereas in the gradient-based framework
而在基于梯度的框架中，
we initialize it randomly and then we
我们随机初始化它，然后让
let the loss
损失
guide us
引导我们
to arrive at the exact same array
到达完全相同的数组，
so this array exactly here
所以这个数组
is
basically the array w at the end of
基本上就是优化结束时的数组 w，
optimization except we arrived at it
只不过我们
piece by piece by following the loss
通过跟踪损失一点一点地到达它，
and that's why we also obtain the same
这就是为什么我们最后也获得了相同的
loss function at the end and the second
损失函数，第二个
note is if i come here
注意事项是，如果我来到这里还
remember the smoothing where we added
记得平滑，我们在计数中添加了
fake counts to our counts
假计数，
in order to
以便
smooth out and make more uniform the
平滑并使
distributions of these probabilities
这些概率的分布更加均匀，
and that prevented us from assigning
并且 阻止我们
zero probability to
为
to any one bigram
任何一个二元组分配零概率，
now if i increase the count here
现在如果我增加这里的计数，
what's happening to the probability
概率会发生什么变化？
as i increase the count probability
随着计数的增加，概率会
becomes more and more uniform
变得越来越均匀，对吧？
right because these counts go only up to
因为这些计数最多只能达到
like 900 or whatever so if i'm adding
900 左右。所以，如果我给
plus a million to every single number
here you can see how
这里的每个数字加上一百万，你可以看到
the row and its probability then when we
行和它的概率，然后当我们
divide is just going to become more and
除以它时，它会
more close to exactly even probability
越来越接近
uniform distribution
均匀概率分布。
it turns out that the gradient based
事实证明，基于梯度的
framework has an equivalent to smoothing
框架具有与平滑相当的作用，
in particular
特别是
think through these w's here
思考一下
which we initialized randomly
我们随机初始化的这些 w，
we could also think about initializing
我们也可以考虑将
w's to be zero
w 初始化为零，
if all the entries of w are zero
如果 w 的所有条目都为零，
then you'll see that logits will become
那么你会看到 logits 将
all zero
全为零，
and then exponentiating those logics
然后对这些逻辑取幂
becomes all one
变为 1，
and then the probabilities turned out to
然后概率就会变得
be exactly uniform
完全均匀。
so basically when w's are all equal to
所以基本上，当 w 都
each other or say especially zero
彼此相等，或者说特别是为零时，
then the probabilities come out
概率就会
completely uniform
完全均匀，
so
所以
trying to incentivize w to be near zero
试图激励 w 接近零
is basically equivalent to
基本上相当于
label smoothing and the more you
标签平滑，你
incentivize that in the loss function
在损失函数中激励的越多，
the more smooth distribution you're
分布就越平滑。
going to achieve
so this brings us to something that's
这就引出了
called
所谓的
regularization where we can actually
正则化，我们可以在
augment the loss function to have a
损失函数中增加一个
small component that we call a
小的分量，我们称之为正
regularization loss
则化损失。
in particular what we're going to do is
具体来说，我们要做的是
we can take w and we can for example
取 w，例如，我们可以对其
square all of its entries
所有项求平方，
and then we can um whoops
然后，嗯，
sorry about that
抱歉，
we can take all the entries of w and we
我们可以取 w 的所有项，然后对它们
can sum them
求和。
and because we're squaring uh there will
因为我们求平方，所以
be no signs anymore um
不再有符号，
negatives and positives all get squashed
负数和正数都会被压缩
to be positive numbers
为正数。
and then the way this works is you
其工作原理是，
achieve zero loss if w is exactly or
如果 w 正好为零，则损失为零。
zero but if w has non-zero numbers you
但如果 w 有非零数，则
accumulate loss
损失会累积。
and so we can actually take this and we
因此，我们可以取这个值，并将其
can add it on here
添加到这里。
so we can do something like loss plus
我们可以执行类似损失加
w square
w 平方
dot sum
点和的操作，
or let's actually instead of sum let's
或者实际上，我们不求和，而是
take a mean because otherwise the sum
取平均值，因为否则和会
gets too large
变得太大，
so mean is like a little bit more
所以平均值更容易
manageable
控制。
and then we have a regularization loss
然后，我们在这里得到正则化损失，
here say 0.01 times
比如 0.01 倍
or something like that you can choose
或类似的值。您可以选择
the regularization strength
正则化强度，
and then we can just optimize this and
然后我们可以对其进行优化。
now this optimization actually has two
现在，这种优化实际上包含两个
components not only is it trying to make
部分 它不仅试图使
all the probabilities work out but in
所有概率都成立，而且
addition to that there's an additional
除此之外，还有一个额外的
component that simultaneously tries to
组件同时试图
make all w's be zero because if w's are
使所有 w 都为零，因为如果 w
non-zero you feel a loss and so
非零，你会感到损失，因此
minimizing this the only way to achieve
最小化它，唯一的方法
that is for w to be zero
是让 w 为零，
and so you can think of this as adding
所以你可以把它想象成添加
like a spring force or like a gravity
一个弹簧力或重力，
force that that pushes w to be zero so w
推动 w 为零，所以 w
wants to be zero and the probabilities
希望为零，概率希望
want to be uniform but they also
均匀，但它们
simultaneously want to match up your
同时也希望
your probabilities as indicated by the
与数据所示的概率相匹配，
data
and so the strength of this
因此，这种
regularization is exactly controlling
正则化的强度恰好控制着
the amount of counts
that you add here
你在这里添加的计数数量 在
adding a lot more counts
here
这里添加更多的计数
corresponds to
相当于
increasing this number
增加这个数字，
because the more you increase it the
因为增加的越多，
more this part of the loss function
损失函数的这一部分就越占
dominates this part and the more these
主导地位，
these weights will un be unable to grow
这些权重就越无法增长，
because as they grow
因为随着它们的增长，
they uh accumulate way too much loss
它们会累积太多的损失，
and so if this is strong enough
所以如果这个值足够大，
then we are not able to overcome the
那么我们就无法克服
force of this loss and we will never
这种损失的力量，我们永远无法，
and basically everything will be uniform
基本上所有的东西都会是统一的
predictions
预测，
so i thought that's kind of cool okay
所以我认为这很酷，好吗？
and lastly before we wrap up
最后，在我们结束之前，
i wanted to show you how you would
我想向你们展示如何
sample from this neural net model
从这个神经网络模型中采样，
and i copy-pasted the sampling code from
我从之前复制粘贴了采样代码，
before
where remember that we sampled five
记得我们采样了五次，
times
and all we did we start at zero we
我们所做的就是从零开始，
grabbed the current ix row of p and that
抓取p的当前ix行，这
was our probability row
是我们的概率行，
from which we sampled the next index and
我们从中采样下一个索引，然后将其
just accumulated that and
累积起来，
break when zero
当为零时中断，
and running this
然后运行它，
gave us these
我们得到了这些
results still have the
结果，仍然
p in memory so this is fine
在内存中还有p，所以
now
现在
the speed doesn't come from the row of b
速度不是来自b的行，
instead it comes from this neural net
而是来自这个神经网络，
first we take ix
首先我们取ix，
and we encode it into a one hot row of x
然后将其编码成一个x inc的one-hot行，
inc
this x inc multiplies rw
这个x inc乘以rw，
which really just plucks out the row of
实际上只是取出
w corresponding to ix really that's
与 ix 对应的 w 行，这就是正在
what's happening
发生的事情，
and that gets our logits and then we
这样就得到了对数，然后我们对
normalize those low jets
这些低速喷流进行指数化
exponentiate to get counts and then
以获得计数，然后对其
normalize to get uh the distribution and
进行归一化以获得分布，
then we can sample from the distribution
然后我们可以从分布中采样，
so if i run this
所以如果我运行这种
kind of anticlimactic or climatic
虎头蛇尾或虎头蛇尾（
depending how you look at it but we get
取决于你怎么看），但我们得到的
the exact same result
结果完全相同，
um
嗯，
and that's because this is in the
那是因为这是在
identical model not only does it achieve
相同的模型中，它不仅实现了
the same loss
相同的损失，而且
but
as i mentioned these are identical
正如我提到的，它们是相同的
models and this w is the log counts of
模型，这个 w 是
what we've estimated before but we came
我们之前估计的对数计数，但我们
to this answer in a very different way
以非常不同的方式得出这个答案，
and it's got a very different
它有一个非常不同的
interpretation but fundamentally this is
解释，但从根本上来说，这
basically the same model and gives the
基本上是相同的模型，并且在
same samples here and so
这里给出了相同的样本，所以
that's kind of cool okay so we've
这很酷，好吧，我们
actually covered a lot of ground we
实际上已经涵盖了很多内容，我们
introduced the bigram character level
介绍了二元字符级
language model
语言模型，
we saw how we can train the model how we
我们看到了如何训练模型，如何
can sample from the model and how we can
从模型中采样，以及如何
evaluate the quality of the model using
使用负对数似然损失来评估模型的质量，
the negative log likelihood loss
and then we actually trained the model
然后我们实际上用
in two completely different ways that
两种完全不同的方式训练了模型，
actually get the same result and the
实际上得到了 相同的结果和
same model
相同的模型，第
in the first way we just counted up the
一种方法我们只是计算
frequency of all the bigrams and
所有二元组的频率并
normalized
进行归一化，
in a second way we used the
第二种方法我们使用
negative log likelihood loss as a guide
负对数似然损失作为指导
to optimizing the counts matrix
来优化计数矩阵
or the counts array so that the loss is
或计数数组，以便在
minimized in the in a gradient-based
基于梯度的框架中最小化损失，
framework and we saw that both of them
我们看到它们都
give the same result
给出了相同的结果，
and
that's it
就是这样，
now the second one of these the
现在第二种方法，
gradient-based framework is much more
基于梯度的框架更加
flexible and right now our neural
灵活，现在我们的神经
network is super simple we're taking a
网络非常简单，我们取一个
single previous character and we're
先前的字符，然后
taking it through a single linear layer
通过一个线性层来
to calculate the logits
计算对数，
this is about to complexify so in the
这将变得复杂，所以在
follow-up videos we're going to be
后续视频中，我们将
taking more and more of these characters
取越来越多的这些字符，并将
and we're going to be feeding them into
它们输入到
a neural net but this neural net will
神经网络中，但这个神经网络仍然会
still output the exact same thing the
输出完全相同的东西，
neural net will output logits
神经网络将输出对数，
and these logits will still be
这些对数仍然会
normalized in the exact same way and all
以完全相同的方式进行归一化，所有的
the loss and everything else and the
损失和其他一切，以及
gradient gradient-based framework
基于梯度的框架，
everything stays identical it's just
一切都保持不变，只是
that this neural net will now complexify
这个神经网络现在会一直复杂化
all the way to transformers
到Transformer，
so that's gonna be pretty awesome and
所以这将是 非常棒，
i'm looking forward to it for now bye
我现在很期待，再见