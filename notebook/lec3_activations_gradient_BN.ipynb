{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafadec6",
   "metadata": {},
   "source": [
    "# TODO: Go deeeper\n",
    "## Key Points\n",
    "1. **Initialization matters**: wants to logits to be close to 0 at the very beginning to avoid large losses (avoid confidently wrong of softmax)\n",
    "2. **Saturation of the tanh function**: break the gradient flow. plot tanh intermediate values to see how it saturates.\n",
    "   1. Gradient always deceases as we go back through tanh.\n",
    "   2. Dead neurons: tanh (or sigmoid or relu) satrurates and cause the gradient to be 0 for all instances.\n",
    "   3. Initialization: motivation is to keep the distribution's variance of the input and output of each layer the same (and reasonable large). If no activation, can multiply by sqrt of (1 / num_of_inputs).\n",
    "   4. Kaiming's initialization: for relu, multiply by sqrt of (2 / number of inputs), where 2 is called \"gain\", used to compensate for the fact that relu is not symmetric around 0. \"gain\" has different values for different activation functions (see pytorch docs aboud weight initialization).\n",
    "   5. \"Gain\" is important: if no activation, then using gain=1 is enough to keep the variance the same, but with squashing activation functions like tanh, the variance will be smaller and smaller, so we need to multiply by a larger value to keep the variance the same.\n",
    "   6. Modern techniques like Adam, BN, residual connections, make initialization less important, but still useful.\n",
    "3. **BN motivation**: make the outputs before the activation function subject to a Gaussian (if too small, tanh has no effect; if too large, tanh saturates)\n",
    "   1. It's a direct idea: just normalize it to have mean 0 and variance 1.\n",
    "   2. Then scale and shift using learnable parameters.\n",
    "   3. BN's side effect: the activation value of an instance is dependent on the other instances in the batch (which is randmomly sampled), thus cause jittering in the training process. But this turns out to be a good thing, as it acts as a regularizer/data augmentation.\n",
    "   4. BN in inference: use the mean and variance of the entire training set or maintain a running mean and variance during training.\n",
    "   5. Bias is no more needed in the linear layer, as BN has its own bias (mathematically, the bias term will be substrated out by the mean of BN, so it never learns).\n",
    "   6. Momentum in BN: in running mean and variance, use momentum to update the mean and variance. (small batch size can cause large variance in estimation, so use a smaller momentum value to smooth it out)\n",
    "## Implementation detail\n",
    "1. Torchify the code of Linear, BatchNorm1d and Tanh, which should have `__call__()` and `parameters()` methods.\n",
    "2. Write: build MLP, enable the grad tracking, initialize the weights, training process.\n",
    "3. No BN, run for only 1 epoch, visualize (can try some other \"gain\" values to see the effect):\n",
    "   1. Tanh Value distribution of activation. (use Tanh because its value is between -1 and 1, so can see the distribution better)\n",
    "   2. Tanh Grad distribution of activation.\n",
    "   3. Linear layer weight distribution.\n",
    "   4. Update:data ratio over time (to see the speed of training of each layer) (use the ratio of the .std() to calculate)\n",
    "4. Add BN and check the robustness (to \"gain\" values)\n",
    "5. Other details:\n",
    "   1. The position of the BN.\n",
    "   2. Make the last layer less confident (avoid large loss at the beginning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f5d96",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f3df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"../names.txt\", \"r\").read().splitlines()\n",
    "chars = sorted(set('.'.join(words)))\n",
    "stoi = {char:i for i, char in enumerate(chars)}\n",
    "itos = {i:char for i, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67331ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio1 = 0.8\n",
    "ratio2 = 0.9\n",
    "chunk = 3\n",
    "\n",
    "def create_data(ws, chunk=3):\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for word in ws:\n",
    "        Xt = [0] * chunk # Cool\n",
    "        for ch in word + '.': # Dont forget to add ending token.\n",
    "            X.append(Xt)\n",
    "            Y.append(stoi[ch])   \n",
    "            Xt = Xt[1:] + [stoi[ch]]\n",
    "\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "random.shuffle(words) # No obvious affect (original words already shuffled)\n",
    "n1 = int(ratio1 * len(words))\n",
    "n2 = int(ratio2 * len(words))\n",
    "Xt, Yt = create_data(words[:n1], chunk=chunk)\n",
    "Xdev, Ydev = create_data(words[n1:n2], chunk=chunk)\n",
    "Xte, Yte = create_data(words[n2:], chunk=chunk)\n",
    "print(Xt.shape)\n",
    "print(Xdev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d1d0e",
   "metadata": {},
   "source": [
    "# Construct the component classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e58180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, n_in, n_out, bias=True):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.W = torch.randn((n_in, n_out)) * (n_in)**(-0.5)\n",
    "        self.b = torch.zeros(n_out) if bias else None # zeros\n",
    "        self.out = None # For visualization\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.W\n",
    "        if self.b is not None:\n",
    "            self.out += self.b\n",
    "        return self.out   \n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b] if self.b is not None else [self.W]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Linear({self.n_in}, {self.n_out})\"\n",
    "\n",
    "test_linear = Linear(10, 50)\n",
    "x_test = torch.randn((32, 10))\n",
    "print(test_linear(x_test).shape)\n",
    "print(test_linear.parameters()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01366099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BN1d:\n",
    "    def __init__(self, num_feat, eps=1e-5, momentum=0.1):\n",
    "        self.n_feat = num_feat\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.gamma = torch.ones(num_feat) # ones\n",
    "        self.beta = torch.zeros(num_feat) # zeros\n",
    "\n",
    "        self.training = True\n",
    "        self.rmean = torch.ones(num_feat) \n",
    "        self.rvar = torch.zeros(num_feat)\n",
    "\n",
    "        self.out = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            meani = torch.mean(x, dim=0, keepdim=True) \n",
    "            vari = torch.var(x, dim=0, keepdim=True)\n",
    "        else:\n",
    "            meani = self.rmean\n",
    "            vari = self.rvar\n",
    "        \n",
    "        if not self.training:\n",
    "            self.rmean = (1- self.momentum) * self.rmean + self.momentum * meani \n",
    "            self.rvar = (1 - self.momentum) * self.rvar + self.momentum * vari\n",
    "        \n",
    "        self.out = self.gamma * (x - meani) / torch.sqrt(vari + self.eps) + self.beta\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"BN1d({self.n_feat})\"\n",
    "\n",
    "BN_test = BN1d(4)\n",
    "x_test = torch.randn((5, 4))\n",
    "print(BN_test(x_test))\n",
    "print((x_test - x_test.mean(0, keepdim=True))/(torch.sqrt(x_test.var(0, keepdim=True) + 1e-5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d8431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)       \n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Tanh()\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484aca2e",
   "metadata": {},
   "source": [
    "# Build the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce12c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = F.one_hot(Xt, num_classes=27).float()\n",
    "print(X_.shape)\n",
    "W = torch.randn(27, 3)\n",
    "print((X_ @ W).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7221d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(embedding_size):\n",
    "    return torch.randn(27, embedding_size, requires_grad=True)\n",
    "\n",
    "test_embed = get_embedding(10)\n",
    "print(test_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87cc520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MLP(chunk, embedding_size, num_neurons, BN=True, eps=1e-5, momentum=0.1):\n",
    "    layers = []\n",
    "    n_in = chunk * embedding_size\n",
    "    for num in num_neurons:\n",
    "        layer = [Linear(n_in, num, False), BN1d(num, eps, momentum), Tanh()] if BN else [Linear(n_in, num, True), Tanh()]\n",
    "        layers.extend(layer)\n",
    "        n_in = num\n",
    "    \n",
    "    layer = [Linear(num_neurons[-1], 27, False), BN1d(27, eps, momentum)] if BN else [Linear(num_neurons[-1], 27, True)]\n",
    "    layers.extend(layer)\n",
    "    return layers\n",
    "\n",
    "layers = get_MLP(3, 10, [300])\n",
    "print(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89844759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(layers, gain=5/3):\n",
    "    if isinstance(layers[-1], BN1d): # Less confident\n",
    "        layers[-2].W *= 0.1\n",
    "    else:\n",
    "        layers[-1].W *= 0.1\n",
    "    for layer in layers:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.W *= gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d225fdb0",
   "metadata": {},
   "source": [
    "# Train & val scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3889ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_through(xx, embed, layers):\n",
    "    # Through embedding\n",
    "    xx = F.one_hot(xx, num_classes=27).float()\n",
    "    xx = (xx @ embed).view(xx.shape[0], -1)\n",
    "    # xx = F.tanh(xx) # Do we really need this?\n",
    "\n",
    "    # Through MLP\n",
    "    for layer in layers:\n",
    "        xx = layer(xx)\n",
    "    return xx\n",
    "\n",
    "xx_test = Xt[:4].clone()\n",
    "xx_test = go_through(xx_test, test_embed, layers)\n",
    "print(xx_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34a6ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(layers):\n",
    "    param_list = []\n",
    "    for layer in layers:\n",
    "        params = layer.parameters()\n",
    "        if params is not None:\n",
    "            param_list.extend(params)\n",
    "    return param_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f663cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLP(X, Y, embed, layers, batch_size, epochs, lr):\n",
    "    param_list = [embed]\n",
    "    param_list.extend(get_params(layers))\n",
    "    for param in param_list:\n",
    "        param.requires_grad = True\n",
    "\n",
    "    loss_list = []\n",
    "    update_ratio = []\n",
    "    for epoch in range(epochs):\n",
    "        # Zero out the grad\n",
    "        for param in param_list:\n",
    "            param.grad = None\n",
    "\n",
    "        # Select batch\n",
    "        idx = torch.randint(0, len(X), (batch_size, ))\n",
    "        xx = X[idx]\n",
    "        yy = Y[idx]\n",
    "\n",
    "        # Forward pass\n",
    "        xx = go_through(xx, embed, layers)\n",
    "\n",
    "        # Retain grads for visualization\n",
    "        for layer in layers:\n",
    "            layer.out.retain_grad()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(xx, yy)\n",
    "\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Record ratio to visualize\n",
    "        with torch.no_grad(): # no_grad()\n",
    "            update_ratio.append([((lr * param.grad).std() / param.data.std()).log10().item() for param in param_list]) # log10.item()\n",
    "\n",
    "        # Update params (Use tensor.data, because leaf tensor cannot do in-place operations)\n",
    "        for param in param_list:\n",
    "            param.data += -lr * param.grad\n",
    "        \n",
    "    return loss_list, update_ratio \n",
    "\n",
    "# chunk = 3\n",
    "# embedding_size = 10\n",
    "# embed = get_embedding(embedding_size)\n",
    "# layers = get_MLP(chunk, embedding_size, [100, 100, 100, 100, 100], BN=False)\n",
    "# param_list = [embed]\n",
    "# param_list.extend(get_params(layers))\n",
    "# weight_init(layers, gain=5/3)\n",
    "# loss_list, update_ratio = train_MLP(Xt, Yt, embed, layers, 64, 5000, 0.1)\n",
    "# plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020d72f1",
   "metadata": {},
   "source": [
    "# Visualize intermediate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e602714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_activation_value(layers):\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    legends = []\n",
    "    for i, layer in enumerate(layers):\n",
    "        if isinstance(layer, Tanh):\n",
    "            xx = layer.out\n",
    "            print(\"Layer%d %s -- mean: %.2f | std: %.2f | saturation: %.2f%%\" %\n",
    "                  (i, layer, xx.mean(), xx.std(), (xx.abs() > 0.97).float().mean()))\n",
    "            hy, hx = torch.histogram(xx, density=True)\n",
    "            plt.plot(hx[:-1].detach(), hy.detach())\n",
    "            legends.append(f\"Layer{i} {layer}\")\n",
    "    plt.legend(legends)\n",
    "    plt.title(\"activation value distribution\")\n",
    "    \n",
    "visualize_activation_value(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f6cb1",
   "metadata": {},
   "source": [
    "## activation values\n",
    "- No BN, gain=5/3: Saturated at the first layer, but getting better latter. And the std tends not to be converge to 0 but roughly 0.5 - 0.6, which is good.\n",
    "![activation value distribution](../images/No_BN_5_3_Gain.png)\n",
    "- No BN, gain=1, less saturated, but std tends to be smaller and smaller, which is not good. Pure Linear preserve the variance, but tanh, as a squashing function, will break this unless we use a larger gain.\n",
    "![activation value distribution](../images/No_BN_1_Gain.png)\n",
    "- No BN, gain>=3, this case the saturation is quite severe.\n",
    "- Add BN, the result will be a lot better and robust to the gain value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad617b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_activation_grad(layers):\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    legends = []\n",
    "    for i, layer in enumerate(layers):\n",
    "        if isinstance(layer, Tanh):\n",
    "            xx = layer.out.grad\n",
    "            print(\"Layer %d %s -- mean: %+f | std: %e\" % (i, layer, xx.mean(), xx.std()))\n",
    "            hy, hx = torch.histogram(xx, density=True)\n",
    "            plt.plot(hx[:-1], hy)\n",
    "            legends.append(f\"Layer{i} {layer}\")\n",
    "    plt.legend(legends)\n",
    "    plt.title(\"activation grad distribution\")\n",
    "\n",
    "visualize_activation_grad(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a920a8d2",
   "metadata": {},
   "source": [
    "## activation gradients\n",
    "- No BN, gain=5/3: The gradient stays almost the same, neither shrinks nor explodes, which is good.\n",
    "![activation gradient distribution](../images/Grad_No_BN_5_3_Gain.png)\n",
    "- No BN, gain=0.5: The gradient's variance is getting larger.\n",
    "![activation gradient distribution](../images/Grad_No_BN_0.5_Gain.png)\n",
    "- No BN, larger gain, the gradient will tend to explode.\n",
    "- Again, add BN, the result will be a lot better and robust to the gain value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weights(param_list):\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    legends = []\n",
    "    for param in param_list:\n",
    "        if param.ndim > 1:\n",
    "            w = param\n",
    "            ww = w.grad\n",
    "            print(\"weight(%d, %d) -- mean: %+f | std: %e | grad:data scale ratio: %e\" % (w.shape[0], w.shape[1], ww.mean(), ww.std(), ww.std() / w.std()))\n",
    "            hy, hx = torch.histogram(ww, density=True)\n",
    "            plt.plot(hx[:-1].detach(), hy.detach())\n",
    "            legends.append(f\"weight({w.shape[0]}, {w.shape[1]})\")\n",
    "    plt.legend(legends)\n",
    "    plt.title(\"weights grad distribution\")\n",
    "\n",
    "visualize_weights(param_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5918349c",
   "metadata": {},
   "source": [
    "## Linear layer weights\n",
    "- No BN, gain=5/3. The grad:data scale ratio of the last layer is several times larger than other layers, which indicates that the last layer may be updated faster. Use modern optimizer may ease this.\n",
    "\n",
    "weight(27, 10) -- mean: +0.002877 | std: 1.685361e-02 | grad:data scale ratio: 1.746943e-02\n",
    "\n",
    "weight(30, 100) -- mean: +0.000466 | std: 1.569925e-02 | grad:data scale ratio: 5.231872e-02\n",
    "\n",
    "weight(100, 100) -- mean: +0.000128 | std: 1.358390e-02 | grad:data scale ratio: 8.207078e-02\n",
    "\n",
    "weight(100, 100) -- mean: -0.000185 | std: 1.249495e-02 | grad:data scale ratio: 7.499947e-02\n",
    "\n",
    "weight(100, 100) -- mean: +0.000056 | std: 1.192545e-02 | grad:data scale ratio: 7.160348e-02\n",
    "\n",
    "weight(100, 100) -- mean: +0.000086 | std: 1.038254e-02 | grad:data scale ratio: 6.230477e-02\n",
    "\n",
    "weight(100, 27) -- mean: +0.000000 | std: 1.833592e-02 | grad:data scale ratio: 1.115449e-01\n",
    "![weights](../images/Weight_No_BN_5_3_Gain.png)\n",
    "- BN, gain=5/3, the grad:data scale ratio is a lot better.\n",
    "\n",
    "weight(27, 10) -- mean: +0.000000 | std: 1.801080e-02 | grad:data scale ratio: 1.807870e-02\n",
    "\n",
    "weight(30, 100) -- mean: -0.000062 | std: 1.456459e-02 | grad:data scale ratio: 4.795692e-02\n",
    "\n",
    "weight(100, 100) -- mean: +0.000048 | std: 1.363576e-02 | grad:data scale ratio: 8.307827e-02\n",
    "\n",
    "weight(100, 100) -- mean: -0.000063 | std: 1.125054e-02 | grad:data scale ratio: 6.777788e-02\n",
    "\n",
    "weight(100, 100) -- mean: +0.000010 | std: 9.727866e-03 | grad:data scale ratio: 5.810474e-02\n",
    "\n",
    "weight(100, 100) -- mean: -0.000090 | std: 8.363669e-03 | grad:data scale ratio: 5.019680e-02\n",
    "\n",
    "weight(100, 27) -- mean: +0.000012 | std: 1.448200e-02 | grad:data scale ratio: 8.823942e-02\n",
    "![weights](../images/Weight_BN_5_3_Gain.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb0648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ratio(ratio):\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    legends = []\n",
    "    for i, param in enumerate(param_list):\n",
    "        if param.ndim > 1:\n",
    "            plt.plot([ratio[j][i] for j in range(len(ratio))])\n",
    "            legends.append(\"weight(%d, %d), index %d\" % (param.shape[0], param.shape[1], i))\n",
    "    plt.legend(legends)\n",
    "    plt.title(\"Ratio\")\n",
    "\n",
    "visualize_ratio(update_ratio)\n",
    "plt.plot([0, len(update_ratio)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dfa7e6",
   "metadata": {},
   "source": [
    "## Linear layer update:data ratio\n",
    "- No BN, gain=5/3: The last layer is obviously an outlier, because it has multiplied by 0.1 to make the output less confident.\n",
    "![update:data ratio](../images/ratio_No_BN_5_3_Gain.png)\n",
    "- No BN, gain=5/3, but without the last layer's multiplication, the update:data ratio is more balanced.\n",
    "![update:data ratio](../images/ratio_No_BN_5_3_Gain_confident.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8bcc76",
   "metadata": {},
   "source": [
    "# Train & Eval the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3fc3b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = 3\n",
    "embedding_size = 10\n",
    "embed = get_embedding(embedding_size)\n",
    "layers = get_MLP(chunk, embedding_size, [100, 100, 100, 100, 100], BN=True)\n",
    "param_list = [embed]\n",
    "param_list.extend(get_params(layers))\n",
    "weight_init(layers, gain=5/3)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 40000\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cf328b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x135387db910>]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_list, update_ratio = train_MLP(Xt, Yt, embed, layers, batch_size, epochs, lr)\n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "053175d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layers:\n",
    "    if isinstance(layer, BN1d):\n",
    "        layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "98bb3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_MLP(X, Y, embed, layers):\n",
    "    with torch.no_grad():\n",
    "        xx = go_through(X, embed, layers)\n",
    "        loss = F.cross_entropy(xx, Y)\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0947f18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2475.998779296875\n"
     ]
    }
   ],
   "source": [
    "dev_MLP(Xdev, Ydev, embed, layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741910b1",
   "metadata": {},
   "source": [
    "# Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1673468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\90534\\AppData\\Local\\Temp\\ipykernel_48260\\4283806034.py:19: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  vari = torch.var(x, dim=0, keepdim=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[148], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m         result \u001b[38;5;241m=\u001b[39m [itos[char] \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m result]\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(result))\n\u001b[1;32m---> 17\u001b[0m \u001b[43minfer_names\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[148], line 9\u001b[0m, in \u001b[0;36minfer_names\u001b[1;34m(num, embed, layers)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits)\n\u001b[0;32m      8\u001b[0m probs \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mexp() \u001b[38;5;241m/\u001b[39m logits\u001b[38;5;241m.\u001b[39mexp()\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m next_token \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m result\u001b[38;5;241m.\u001b[39mappend(next_token\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     11\u001b[0m xx \u001b[38;5;241m=\u001b[39m xx[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m+\u001b[39m [next_token]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "def infer_names(num, embed, layers): # Cant infer in parallel\n",
    "    for i in range(num):\n",
    "        result = []\n",
    "        xx = [0] * chunk\n",
    "        while True:\n",
    "            logits = go_through(torch.tensor(xx).unsqueeze(0), embed, layers)\n",
    "            probs = logits.exp() / logits.exp().sum(dim=1)\n",
    "            next_token = torch.multinomial(probs[0], num_samples=1)\n",
    "            result.append(next_token.item())\n",
    "            xx = xx[1:] + [next_token]\n",
    "            if next_token == 0:\n",
    "                break\n",
    "        result = [itos[char] for char in result]\n",
    "        print(''.join(result))\n",
    "    \n",
    "infer_names(2, embed, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3226c915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
