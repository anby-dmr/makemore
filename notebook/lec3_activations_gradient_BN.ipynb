{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafadec6",
   "metadata": {},
   "source": [
    "# TODO: Improvements based on lec2\n",
    "## Key Points\n",
    "1. **Initialization matters**: wants to logits to be close to 0 at the very beginning to avoid large losses.\n",
    "2. **Saturation of the tanh function**: break the gradient flow. plot tanh intermediate values to see how it saturates.\n",
    "   1. Gradient always deceases as we go back through tanh.\n",
    "   2. Dead neurons: tanh (or sigmoid or relu) satrurates and cause the gradient to be 0 for all instances.\n",
    "   3. Initialization: motivation is to keep the distribution's variance of the input and output of each layer the same. If no activation, can multiply by sqrt of (1 / num_of_inputs).\n",
    "   4. Kaiming's initialization: for relu, multiply by sqrt of (2 / number of inputs), where 2 is called \"gain\", used to compensate for the fact that relu is not symmetric around 0. \"gain\" has different values for different activation functions (see pytorch docs aboud weight initialization).\n",
    "   5. \"Gain\" is important: if no activation, then using gain=1 is enough to keep the variance the same, but with squashing activation functions like tanh, the variance will be smaller and smaller, so we need to multiply by a larger value to keep the variance the same.\n",
    "   6. Modern techniques like Adam, BN, residual connections, make initialization less important, but still useful.\n",
    "3. **BN motivation**: make the outputs before the activation function subject to a Gaussian (if too small, tanh has no effect; if too large, tanh saturates)\n",
    "   1. It's a direct idea: just normalize it to have mean 0 and variance 1.\n",
    "   2. Then scale and shift using learnable parameters.\n",
    "   3. BN's side effect: the activation value of an instance is dependent on the other instances in the batch (which is randmomly sampled), thus cause jittering in the training process. But this turns out to be a good thing, as it acts as a regularizer/data augmentation.\n",
    "   4. BN in inference: use the mean and variance of the entire training set or maintain a running mean and variance during training.\n",
    "   5. Bias is no more needed in the linear layer, as BN has its own bias (mathematically, the bias term will be substrated out by the mean of BN, so it never learns).\n",
    "   6. Momentum in BN: in running mean and variance, use momentum to update the mean and variance. (small batch size can cause large variance in estimation, so use a smaller momentum value to smooth it out)\n",
    "## Implementation detail\n",
    "1. Torchify the code of Linear, BatchNorm1d and Tanh, which should have `__call__()` and `parameters()` methods.\n",
    "2. Write: build MLP, enable the grad tracking, initialize the weights, training process.\n",
    "3. No BN, run for only 1 epoch, visualize (can try some other \"gain\" values to see the effect):\n",
    "   1. Value distribution of activation.\n",
    "   2. Grad distribution of activation.\n",
    "   3. Weight distribution.\n",
    "   4. Updata:data ratio over time (to see the speed of training of each layer).\n",
    "4. Add BN and check the robustness.\n",
    "5. Other details:\n",
    "   1. The position of the BN.\n",
    "   2. Make the last layer less confident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f5d96",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f3df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"../names.txt\", \"r\").read().splitlines()\n",
    "chars = sorted(set('.'.join(words)))\n",
    "stoi = {char:i for i, char in enumerate(chars)}\n",
    "itos = {i:char for i, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67331ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182578, 3])\n",
      "torch.Size([22777, 3])\n"
     ]
    }
   ],
   "source": [
    "ratio1 = 0.8\n",
    "ratio2 = 0.9\n",
    "chunk = 3\n",
    "\n",
    "def create_data(ws, chunk=3):\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for word in ws:\n",
    "        Xt = [0] * chunk # Cool\n",
    "        for ch in word + '.': # Dont forget to add ending token.\n",
    "            X.append(Xt)\n",
    "            Y.append(stoi[ch])   \n",
    "            Xt = Xt[1:] + [stoi[ch]]\n",
    "\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "random.shuffle(words) # No obvious affect (original words already shuffled)\n",
    "n1 = int(ratio1 * len(words))\n",
    "n2 = int(ratio2 * len(words))\n",
    "Xt, Yt = create_data(words[:n1], chunk=chunk)\n",
    "Xdev, Ydev = create_data(words[n1:n2], chunk=chunk)\n",
    "Xte, Yte = create_data(words[n2:], chunk=chunk)\n",
    "print(Xt.shape)\n",
    "print(Xdev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d1d0e",
   "metadata": {},
   "source": [
    "# Construct the component classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45e58180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50])\n",
      "torch.Size([10, 50])\n"
     ]
    }
   ],
   "source": [
    "class Linear:\n",
    "    def __init__(self, n_in, n_out, bias=True):\n",
    "        self.W = torch.randn((n_in, n_out)) * (n_in)**(-0.5)\n",
    "        self.b = torch.zeros(n_out) if bias else None # zeros\n",
    "        self.out = None # For visualization\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.W\n",
    "        if self.b is not None:\n",
    "            self.out += self.b\n",
    "        return self.out   \n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b] if self.b is not None else [self.W]\n",
    "\n",
    "test_linear = Linear(10, 50)\n",
    "x_test = torch.randn((32, 10))\n",
    "print(test_linear(x_test).shape)\n",
    "print(test_linear.parameters()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01366099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3652,  1.3443, -0.7855,  1.2520],\n",
      "        [ 1.1592, -1.4120, -0.0362, -0.6560],\n",
      "        [ 0.0312, -0.0277,  1.0200, -0.8850],\n",
      "        [-0.5356,  0.3590,  0.9782, -0.6227],\n",
      "        [ 0.7104, -0.2636, -1.1766,  0.9117]])\n",
      "tensor([[-1.3652,  1.3443, -0.7855,  1.2520],\n",
      "        [ 1.1592, -1.4120, -0.0362, -0.6560],\n",
      "        [ 0.0312, -0.0277,  1.0200, -0.8850],\n",
      "        [-0.5356,  0.3590,  0.9782, -0.6227],\n",
      "        [ 0.7104, -0.2636, -1.1766,  0.9117]])\n"
     ]
    }
   ],
   "source": [
    "class BN1d:\n",
    "    def __init__(self, num_feat, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.gamma = torch.ones(num_feat) # ones\n",
    "        self.beta = torch.zeros(num_feat) # zeros\n",
    "\n",
    "        self.training = True\n",
    "        self.rmean = torch.ones(num_feat) \n",
    "        self.rvar = torch.zeros(num_feat)\n",
    "\n",
    "        self.out = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            meani = torch.mean(x, dim=0, keepdim=True) \n",
    "            vari = torch.var(x, dim=0, keepdim=True)\n",
    "        else:\n",
    "            meani = self.rmean\n",
    "            vari = self.rvar\n",
    "        \n",
    "        if not self.training:\n",
    "            self.rmean = (1- self.momentum) * self.rmean + self.momentum * meani \n",
    "            self.rvar = (1 - self.momentum) * self.rvar + self.momentum * vari\n",
    "        \n",
    "        self.out = self.gamma * (x - meani) / torch.sqrt(vari + self.eps) + self.beta\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "BN_test = BN1d(4)\n",
    "x_test = torch.randn((5, 4))\n",
    "print(BN_test(x_test))\n",
    "print((x_test - x_test.mean(0, keepdim=True))/(torch.sqrt(x_test.var(0, keepdim=True) + 1e-5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56ce12c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182578, 3, 27])\n",
      "torch.Size([182578, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "X_ = F.one_hot(Xt, num_classes=27).float()\n",
    "print(X_.shape)\n",
    "W = torch.randn(27, 3)\n",
    "print((X_ @ W).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7221d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 3\n",
    "\n",
    "def get_embedding(embedding_size):\n",
    "    return torch.randn(27, embedding_size, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3cb125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MLP(chunk, embedding_size, num_neurons):\n",
    "    Ws = []\n",
    "    bs = []\n",
    "\n",
    "    # Input layer\n",
    "    W = torch.randn(chunk * embedding_size, num_neurons[0], requires_grad=True)\n",
    "    b = torch.randn(num_neurons[0])\n",
    "    Ws.append(W)\n",
    "    bs.append(b)\n",
    "\n",
    "    # Hidden layer\n",
    "    if len(num_neurons) > 1:\n",
    "        for n1, n2 in zip(num_neurons, num_neurons[1:]):\n",
    "            W = torch.randn(n1, n2, requires_grad=True)\n",
    "            b = torch.randn(n2, requires_grad=True)\n",
    "            Ws.append(W)\n",
    "            bs.append(b)\n",
    "\n",
    "    # Output layer\n",
    "    W = torch.randn(num_neurons[-1], 27, requires_grad=True)\n",
    "    b = torch.randn(27)\n",
    "    Ws.append(W)\n",
    "    bs.append(b)\n",
    "\n",
    "    return Ws, bs\n",
    "\n",
    "W, b = get_MLP(chunk, embedding_size, [100])\n",
    "print(W[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d225fdb0",
   "metadata": {},
   "source": [
    "# Train & val scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3889ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_through(xx, embed, Ws, bs):\n",
    "    # Through embedding\n",
    "    xx = F.one_hot(xx, num_classes=27).float()\n",
    "    xx = (xx @ embed).view(xx.shape[0], -1)\n",
    "    xx = F.tanh(xx) # Easy to forget\n",
    "\n",
    "    # Through MLP\n",
    "    num_layers = len(Ws)\n",
    "    for i, (W, b) in enumerate(zip(Ws, bs)):\n",
    "        xx = xx @ W + b\n",
    "        if i < num_layers - 1: # No activation after the last layer\n",
    "            xx = F.tanh(xx)\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f663cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLP(X, Y, embed, Ws, bs, batch_size, epochs, lr):\n",
    "    loss_list = []\n",
    "    for epoch in range(epochs):\n",
    "        # Zero out the grad (easy to forget)\n",
    "        embed.grad = None\n",
    "        for W, b in zip(Ws, bs):\n",
    "            W.grad = None\n",
    "            b.grad = None\n",
    "        \n",
    "        # Select batch\n",
    "        indices = torch.randint(low=0, high=X.shape[0], size=(batch_size,))\n",
    "        xx = X[indices]\n",
    "        yy = Y[indices]\n",
    "\n",
    "        xx = go_through(xx, embed, Ws, bs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(xx, yy)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Update params (Use tensor.data, because leaf tensor cannot do in-place operations)\n",
    "        embed.data += -lr * embed.grad\n",
    "        for W, b in zip(Ws, bs):\n",
    "            W.data += -lr * W.grad\n",
    "            if b.requires_grad == True:\n",
    "                b.data += -lr * b.grad\n",
    "            \n",
    "    return loss_list\n",
    "\n",
    "# embed = get_embedding(embedding_size)\n",
    "# embed.requires_grad = True\n",
    "# Ws, bs = get_MLP(chunk, embedding_size, [100])\n",
    "# loss_list = train_MLP(Xt, Yt, embed, Ws, bs, 64, 10000, 0.1)\n",
    "# plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a356959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_MLP(X, Y, embed, Ws, bs):\n",
    "    xx = go_through(X, embed, Ws, bs)\n",
    "    loss = F.cross_entropy(xx, Y)\n",
    "    print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8bcc76",
   "metadata": {},
   "source": [
    "# Train & Eval the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = 3\n",
    "embedding_size = 32\n",
    "num_neurons = [128, 256, 128]\n",
    "\n",
    "embed = get_embedding(embedding_size)\n",
    "Ws, bs = get_MLP(chunk, embedding_size, num_neurons)\n",
    "\n",
    "for W, b in zip(Ws, bs):\n",
    "    print(W.shape)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100000\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d34fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = train_MLP(Xt, Yt, embed, Ws, bs, batch_size, 50000, 0.01)\n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0947f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_MLP(Xdev, Ydev, embed, Ws, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741910b1",
   "metadata": {},
   "source": [
    "# Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c1673468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jip.\n",
      "luncekealie.\n"
     ]
    }
   ],
   "source": [
    "def infer_names(num, embed, Ws, bs): # Cant infer in parallel\n",
    "    for i in range(num):\n",
    "        result = []\n",
    "        xx = [0] * chunk\n",
    "        while True:\n",
    "            logits = go_through(torch.tensor(xx).unsqueeze(0), embed, Ws, bs)\n",
    "            probs = logits.exp() / logits.exp().sum(dim=1)\n",
    "            next_token = torch.multinomial(probs[0], num_samples=1)\n",
    "            result.append(next_token.item())\n",
    "            xx = xx[1:] + [next_token]\n",
    "            if next_token == 0:\n",
    "                break\n",
    "        result = [itos[char] for char in result]\n",
    "        print(''.join(result))\n",
    "    \n",
    "infer_names(2, embed, Ws, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3226c915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
