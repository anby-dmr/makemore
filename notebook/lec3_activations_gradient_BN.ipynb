{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafadec6",
   "metadata": {},
   "source": [
    "# TODO: Improvements based of lec2\n",
    "1. Initialization matters: wants to logits to be close to 0 at the very beginning to avoid large losses.\n",
    "2. Saturation of the tanh function: break the gradient flow. plot tanh intermediate values to see how it saturates.\n",
    "   1. Gradient always deceases as we go back through tanh.\n",
    "   2. Dead neurons: tanh (or sigmoid or relu) satrurates and cause the gradient to be 0 for all instances.\n",
    "   3. Initialization: motivation is to keep the distribution's variance of the input and output of each layer the same. If no activation, can multiply by sqrt of (1 / num_of_inputs).\n",
    "   4. Kaiming's initialization: for relu, multiply by sqrt of (2 / number of inputs), where 2 is called \"gain\", used to compensate for the fact that relu is not symmetric around 0. \"gain\" has different values for different activation functions (see pytorch docs aboud weight initialization).\n",
    "   5. Modern techniques like Adam, BN, residual connections, make initialization less important, but still useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f5d96",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f3df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"../names.txt\", \"r\").read().splitlines()\n",
    "chars = sorted(set('.'.join(words)))\n",
    "stoi = {char:i for i, char in enumerate(chars)}\n",
    "itos = {i:char for i, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67331ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio1 = 0.8\n",
    "ratio2 = 0.9\n",
    "chunk = 3\n",
    "\n",
    "def create_data(ws, chunk=3):\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for word in ws:\n",
    "        Xt = [0] * chunk # Cool\n",
    "        for ch in word + '.': # Dont forget to add ending token.\n",
    "            X.append(Xt)\n",
    "            Y.append(stoi[ch])   \n",
    "            Xt = Xt[1:] + [stoi[ch]]\n",
    "\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "random.shuffle(words) # No obvious affect (original words already shuffled)\n",
    "n1 = int(ratio1 * len(words))\n",
    "n2 = int(ratio2 * len(words))\n",
    "Xt, Yt = create_data(words[:n1], chunk=chunk)\n",
    "Xdev, Ydev = create_data(words[n1:n2], chunk=chunk)\n",
    "Xte, Yte = create_data(words[n2:], chunk=chunk)\n",
    "print(Xt.shape)\n",
    "print(len(Xdev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d1d0e",
   "metadata": {},
   "source": [
    "# Construct the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce12c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = F.one_hot(Xt, num_classes=27).float()\n",
    "print(X_.shape)\n",
    "W = torch.randn(27, 3)\n",
    "print((X_ @ W).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7221d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 3\n",
    "\n",
    "def get_embedding(embedding_size):\n",
    "    return torch.randn(27, embedding_size, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3cb125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MLP(chunk, embedding_size, num_neurons):\n",
    "    Ws = []\n",
    "    bs = []\n",
    "\n",
    "    # Input layer\n",
    "    W = torch.randn(chunk * embedding_size, num_neurons[0], requires_grad=True)\n",
    "    b = torch.randn(num_neurons[0])\n",
    "    Ws.append(W)\n",
    "    bs.append(b)\n",
    "\n",
    "    # Hidden layer\n",
    "    if len(num_neurons) > 1:\n",
    "        for n1, n2 in zip(num_neurons, num_neurons[1:]):\n",
    "            W = torch.randn(n1, n2, requires_grad=True)\n",
    "            b = torch.randn(n2, requires_grad=True)\n",
    "            Ws.append(W)\n",
    "            bs.append(b)\n",
    "\n",
    "    # Output layer\n",
    "    W = torch.randn(num_neurons[-1], 27, requires_grad=True)\n",
    "    b = torch.randn(27)\n",
    "    Ws.append(W)\n",
    "    bs.append(b)\n",
    "\n",
    "    return Ws, bs\n",
    "\n",
    "W, b = get_MLP(chunk, embedding_size, [100])\n",
    "print(W[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d225fdb0",
   "metadata": {},
   "source": [
    "# Train & val scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3889ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_through(xx, embed, Ws, bs):\n",
    "    # Through embedding\n",
    "    xx = F.one_hot(xx, num_classes=27).float()\n",
    "    xx = (xx @ embed).view(xx.shape[0], -1)\n",
    "    xx = F.tanh(xx) # Easy to forget\n",
    "\n",
    "    # Through MLP\n",
    "    num_layers = len(Ws)\n",
    "    for i, (W, b) in enumerate(zip(Ws, bs)):\n",
    "        xx = xx @ W + b\n",
    "        if i < num_layers - 1: # No activation after the last layer\n",
    "            xx = F.tanh(xx)\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f663cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLP(X, Y, embed, Ws, bs, batch_size, epochs, lr):\n",
    "    loss_list = []\n",
    "    for epoch in range(epochs):\n",
    "        # Zero out the grad (easy to forget)\n",
    "        embed.grad = None\n",
    "        for W, b in zip(Ws, bs):\n",
    "            W.grad = None\n",
    "            b.grad = None\n",
    "        \n",
    "        # Select batch\n",
    "        indices = torch.randint(low=0, high=X.shape[0], size=(batch_size,))\n",
    "        xx = X[indices]\n",
    "        yy = Y[indices]\n",
    "\n",
    "        xx = go_through(xx, embed, Ws, bs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(xx, yy)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Update params (Use tensor.data, because leaf tensor cannot do in-place operations)\n",
    "        embed.data += -lr * embed.grad\n",
    "        for W, b in zip(Ws, bs):\n",
    "            W.data += -lr * W.grad\n",
    "            if b.requires_grad == True:\n",
    "                b.data += -lr * b.grad\n",
    "            \n",
    "    return loss_list\n",
    "\n",
    "# embed = get_embedding(embedding_size)\n",
    "# embed.requires_grad = True\n",
    "# Ws, bs = get_MLP(chunk, embedding_size, [100])\n",
    "# loss_list = train_MLP(Xt, Yt, embed, Ws, bs, 64, 10000, 0.1)\n",
    "# plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a356959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_MLP(X, Y, embed, Ws, bs):\n",
    "    xx = go_through(X, embed, Ws, bs)\n",
    "    loss = F.cross_entropy(xx, Y)\n",
    "    print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8bcc76",
   "metadata": {},
   "source": [
    "# Train & Eval the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = 3\n",
    "embedding_size = 32\n",
    "num_neurons = [128, 256, 128]\n",
    "\n",
    "embed = get_embedding(embedding_size)\n",
    "Ws, bs = get_MLP(chunk, embedding_size, num_neurons)\n",
    "\n",
    "for W, b in zip(Ws, bs):\n",
    "    print(W.shape)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100000\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d34fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = train_MLP(Xt, Yt, embed, Ws, bs, batch_size, 50000, 0.01)\n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0947f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_MLP(Xdev, Ydev, embed, Ws, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741910b1",
   "metadata": {},
   "source": [
    "# Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c1673468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jip.\n",
      "luncekealie.\n"
     ]
    }
   ],
   "source": [
    "def infer_names(num, embed, Ws, bs): # Cant infer in parallel\n",
    "    for i in range(num):\n",
    "        result = []\n",
    "        xx = [0] * chunk\n",
    "        while True:\n",
    "            logits = go_through(torch.tensor(xx).unsqueeze(0), embed, Ws, bs)\n",
    "            probs = logits.exp() / logits.exp().sum(dim=1)\n",
    "            next_token = torch.multinomial(probs[0], num_samples=1)\n",
    "            result.append(next_token.item())\n",
    "            xx = xx[1:] + [next_token]\n",
    "            if next_token == 0:\n",
    "                break\n",
    "        result = [itos[char] for char in result]\n",
    "        print(''.join(result))\n",
    "    \n",
    "infer_names(2, embed, Ws, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3226c915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
