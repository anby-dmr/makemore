{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "1. Add start and end tokens to the text. (later only use one special token, because of the redundancy in the bigrams' probability matrix)\n",
    "2. Create a simple tokenizer that transforms the char.\n",
    "3. Transform the data into training data set and use one-hot encoding for the characters. (Note that one-hot is exatly the same as seleting the index of the bigram probability matrix)\n",
    "4. Create a weight matrix of 27 x 27 as a simple one layer NN.\n",
    "5. Use softmax to transform the logits into probability and use log-likelihood as a loss function to train the model.\n",
    "6. Use model smoothing to avoid infinite log-likelihood (regularize the w to zero is equivalent to adding a positive value to the counts of the bigram).\n",
    "7. Update the weight matrix with the gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../names.txt').read().splitlines()\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_line = '.' + '.'.join(words) + '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_set = set(words_line)\n",
    "print(len(words_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(sorted(words_set))\n",
    "stoi = {char : i for i, char in enumerate(chars)}\n",
    "itos = {i : char for i, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for ch1, ch2 in zip(words_line, words_line[1:]):\n",
    "    id1 = stoi[ch1]\n",
    "    id2 = stoi[ch2]\n",
    "    xs.append(id1)\n",
    "    ys.append(id2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss0: 3.7686\n",
      "loss1: 3.3788\n",
      "loss2: 3.1611\n",
      "loss3: 3.0272\n",
      "loss4: 2.9345\n",
      "loss5: 2.8672\n",
      "loss6: 2.8167\n",
      "loss7: 2.7771\n",
      "loss8: 2.7453\n",
      "loss9: 2.7188\n",
      "loss10: 2.6965\n",
      "loss11: 2.6774\n",
      "loss12: 2.6608\n",
      "loss13: 2.6464\n",
      "loss14: 2.6337\n",
      "loss15: 2.6225\n",
      "loss16: 2.6125\n",
      "loss17: 2.6037\n",
      "loss18: 2.5958\n",
      "loss19: 2.5887\n",
      "loss20: 2.5823\n",
      "loss21: 2.5764\n",
      "loss22: 2.5711\n",
      "loss23: 2.5663\n",
      "loss24: 2.5618\n",
      "loss25: 2.5577\n",
      "loss26: 2.5539\n",
      "loss27: 2.5504\n",
      "loss28: 2.5472\n",
      "loss29: 2.5442\n",
      "loss30: 2.5414\n",
      "loss31: 2.5387\n",
      "loss32: 2.5363\n",
      "loss33: 2.5340\n",
      "loss34: 2.5318\n",
      "loss35: 2.5298\n",
      "loss36: 2.5279\n",
      "loss37: 2.5261\n",
      "loss38: 2.5244\n",
      "loss39: 2.5228\n",
      "loss40: 2.5213\n",
      "loss41: 2.5198\n",
      "loss42: 2.5185\n",
      "loss43: 2.5172\n",
      "loss44: 2.5160\n",
      "loss45: 2.5148\n",
      "loss46: 2.5137\n",
      "loss47: 2.5127\n",
      "loss48: 2.5117\n",
      "loss49: 2.5108\n",
      "loss50: 2.5099\n",
      "loss51: 2.5090\n",
      "loss52: 2.5082\n",
      "loss53: 2.5074\n",
      "loss54: 2.5066\n",
      "loss55: 2.5059\n",
      "loss56: 2.5052\n",
      "loss57: 2.5045\n",
      "loss58: 2.5039\n",
      "loss59: 2.5033\n",
      "loss60: 2.5027\n",
      "loss61: 2.5021\n",
      "loss62: 2.5016\n",
      "loss63: 2.5011\n",
      "loss64: 2.5006\n",
      "loss65: 2.5001\n",
      "loss66: 2.4996\n",
      "loss67: 2.4992\n",
      "loss68: 2.4987\n",
      "loss69: 2.4983\n",
      "loss70: 2.4979\n",
      "loss71: 2.4975\n",
      "loss72: 2.4971\n",
      "loss73: 2.4967\n",
      "loss74: 2.4964\n",
      "loss75: 2.4960\n",
      "loss76: 2.4957\n",
      "loss77: 2.4954\n",
      "loss78: 2.4950\n",
      "loss79: 2.4947\n",
      "loss80: 2.4944\n",
      "loss81: 2.4941\n",
      "loss82: 2.4939\n",
      "loss83: 2.4936\n",
      "loss84: 2.4933\n",
      "loss85: 2.4931\n",
      "loss86: 2.4928\n",
      "loss87: 2.4926\n",
      "loss88: 2.4923\n",
      "loss89: 2.4921\n",
      "loss90: 2.4919\n",
      "loss91: 2.4917\n",
      "loss92: 2.4915\n",
      "loss93: 2.4913\n",
      "loss94: 2.4911\n",
      "loss95: 2.4909\n",
      "loss96: 2.4907\n",
      "loss97: 2.4905\n",
      "loss98: 2.4903\n",
      "loss99: 2.4901\n",
      "loss100: 2.4900\n",
      "loss101: 2.4898\n",
      "loss102: 2.4896\n",
      "loss103: 2.4895\n",
      "loss104: 2.4893\n",
      "loss105: 2.4892\n",
      "loss106: 2.4890\n",
      "loss107: 2.4889\n",
      "loss108: 2.4887\n",
      "loss109: 2.4886\n",
      "loss110: 2.4885\n",
      "loss111: 2.4883\n",
      "loss112: 2.4882\n",
      "loss113: 2.4881\n",
      "loss114: 2.4880\n",
      "loss115: 2.4878\n",
      "loss116: 2.4877\n",
      "loss117: 2.4876\n",
      "loss118: 2.4875\n",
      "loss119: 2.4874\n",
      "loss120: 2.4873\n",
      "loss121: 2.4872\n",
      "loss122: 2.4871\n",
      "loss123: 2.4870\n",
      "loss124: 2.4869\n",
      "loss125: 2.4868\n",
      "loss126: 2.4867\n",
      "loss127: 2.4866\n",
      "loss128: 2.4865\n",
      "loss129: 2.4864\n",
      "loss130: 2.4863\n",
      "loss131: 2.4862\n",
      "loss132: 2.4862\n",
      "loss133: 2.4861\n",
      "loss134: 2.4860\n",
      "loss135: 2.4859\n",
      "loss136: 2.4858\n",
      "loss137: 2.4858\n",
      "loss138: 2.4857\n",
      "loss139: 2.4856\n",
      "loss140: 2.4856\n",
      "loss141: 2.4855\n",
      "loss142: 2.4854\n",
      "loss143: 2.4853\n",
      "loss144: 2.4853\n",
      "loss145: 2.4852\n",
      "loss146: 2.4852\n",
      "loss147: 2.4851\n",
      "loss148: 2.4850\n",
      "loss149: 2.4850\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "lr = 50\n",
    "epochs = 150\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # forward\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    probs = logits.exp() / torch.sum(logits.exp(), dim=1, keepdim=True) # NOTE: keepdim is critical\n",
    "    loss = -probs[torch.arange(len(ys)), ys].log().mean() + 0.01 * (W ** 2).mean() # NOTE: regularization\n",
    "\n",
    "    # backward\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -lr * W.grad\n",
    "\n",
    "    print(f\"loss{epoch}: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".junide\n",
      ".janasah\n",
      ".p\n",
      ".cfay\n",
      ".a\n"
     ]
    }
   ],
   "source": [
    "num_samples = 5\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for i in range(num_samples):\n",
    "    ix = stoi['.'] # NOTE: starts with '.'\n",
    "    out = []\n",
    "    while True:\n",
    "        out.append(itos[ix])\n",
    "        logits = W[ix]\n",
    "        probs = logits.exp() / torch.sum(logits.exp())\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        if ix == stoi['.']:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
